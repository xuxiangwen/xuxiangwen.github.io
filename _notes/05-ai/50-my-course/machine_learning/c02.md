## 层（Layer）

深度学习中，有很多的计算层，这些层各有特性和作用，并且相互连接。下文中详细描述

### Batch Normalization

批量归⼀化（Batch Normalization）是神经网络中的一种Layer，它能让较深的神经网络的训练变得更加容易。

在浅层神经网络中，一般都会对输入数据进行归一化（Normalization）或标准化（standardization）处理，这样能够可以使得数据的各个特征分布相近，从而更容易训练出有效模型。而在深层神经网络中，任意一个中间层的神经元可以看成是下一层的输入，如果对这些层进行这些处理，应该也可以优化模型的训练效果。这就是Batch Normalization的来源。

对于深层网络，训练中模型参数的更新依然很容易造成输出层输出的剧烈变化，这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。而Batch Normalizatio利用小批量上的均值和标准差，不断调整神经.络中间输出，从而使整个神经.络在各层的中间输出的数值更稳定。

通常，我们会把Batch Normalization层放在全连接层和激活函数层之间。

![img](images/1192699-20180405213955224-1791925244.png)

将单独标准化每个标量特征，从而代替在层输入输出对特征进行共同白化，使其具有零均值和单位方差。对于具有$d$维输入$x = (x_1, x_2, \ldots, x_d)$的层，将标准化每一维：
$$
\begin{align}
& 
\mu_{\cal B} =  \frac 1 m \sum_{i=1}^m x_i
\\ & 
\sigma_\cal B^2 =  \frac 1 m  \sum_{i=1}^m (x_i-\mu_\cal B)^2
\\ & 
\hat x_i = \frac{x_i - \mu_\cal B} {\sqrt {\sigma_\cal B^2 + \epsilon } }
\end{align}
$$
注意简单标准化层的每一个输入可能会改变层可以表示什么。例如，标准化sigmoid的输入会将它们约束到非线性的线性状态。为了解决这个问题，我们要确保*插入到网络中的变换可以表示恒等变换*。为了实现这个，对于每一个激活值$x_k$，我们引入成对的参数$\gamma，\beta$，它们会归一化和移动标准化值：
$$
y_k = \gamma \hat x_k + \beta
$$
这些参数与原始的模型参数一起学习，并恢复网络的表示能力。实际上，通过设置$\gamma = \sqrt {\sigma_\cal B^2 + \epsilon} $和$\beta = \mu_{\cal B} $，我们可以重新获得原始的激活值。

采用链式法则， 可以得到以下梯度：
$$
\begin {align} 
&\frac {\partial \ell}{\partial \hat x_i} = \frac {\partial \ell} {\partial y_i} \cdot \gamma
\\ &
\frac {\partial \ell}{\partial \sigma_\cal B^2} = \sum_{i=1}^m \frac {\partial \ell}{\partial \hat x_i}\cdot(x_i-\mu_\cal B)\cdot \frac {-1}{2}(\sigma_\cal B^2+\epsilon)^{-3/2}
\\ &
\frac {\partial \ell}{\partial \mu_\cal B} = \sum_{i=1}^m \frac {\partial \ell}{\partial \hat x_i}\cdot \frac {-1} {\sqrt {\sigma_\cal B^2 + \epsilon}}
\\ &
\frac {\partial \ell}{\partial x_i} = \sum_{i=1}^m \frac {\partial \ell}{\partial \hat x_i} \cdot \frac {-1} {\sqrt {\sigma_\cal B^2 + \epsilon}} + \frac {\partial \ell}{\partial \sigma_\cal B^2} \cdot \frac {2(x_i - \mu_\cal B)} {m} + \frac {\partial \ell} {\partial \mu_\cal B} \cdot \frac {1} {m}
\\ &
\frac {\partial \ell}{\partial \gamma} = \sum_{i=1}^m \frac {\partial \ell}{\partial y_i} \cdot \hat x_i 
\\ &
\frac {\partial \ell}{\partial \beta} = \sum_{i=1}^m \frac {\partial \ell}{\partial y_i} \end{align}
$$


## Reference

- [动手学深度学习](https://zh.d2l.ai/d2l-zh.pdf)
- [深入理解Batch Normalization批标准化](https://www.cnblogs.com/guoyaohua/p/8724433.html)
- [Batch Normalization论文翻译——中英文对照](http://noahsnail.com/2017/09/04/2017-09-04-Batch%20Normalization%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/)

- [Batch Normalization学习笔记及其实现](https://zhuanlan.zhihu.com/p/26138673)