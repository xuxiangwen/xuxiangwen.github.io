{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0       1       2\n",
      "Healt:  0.30000 0.08400 0.00588\n",
      "Fever:  0.04000 0.02700 0.01512\n",
      "(0.01512, ['Healthy', 'Healthy', 'Fever'])\n"
     ]
    }
   ],
   "source": [
    "states = ('Healthy', 'Fever')\n",
    " \n",
    "observations = ('normal', 'cold', 'dizzy')\n",
    " \n",
    "start_probability = {'Healthy': 0.6, 'Fever': 0.4}\n",
    " \n",
    "transition_probability = {\n",
    "   'Healthy' : {'Healthy': 0.7, 'Fever': 0.3},\n",
    "   'Fever' : {'Healthy': 0.4, 'Fever': 0.6},\n",
    "   }\n",
    " \n",
    "emission_probability = {\n",
    "   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
    "   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6},\n",
    "   }\n",
    "# Helps visualize the steps of Viterbi.\n",
    "def print_dptable(V):\n",
    "    print \"    \",\n",
    "    for i in range(len(V)): print \"%7d\" % i,\n",
    "    print\n",
    "\n",
    "    for y in V[0].keys():\n",
    "        print \"%.5s: \" % y,\n",
    "        for t in range(len(V)):\n",
    "            print \"%.7s\" % (\"%f\" % V[t][y]),\n",
    "        print\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    # Initialize base cases (t == 0)\n",
    "    for y in states:\n",
    "        V[0][y] = start_p[y] * emit_p[y][obs[0]]\n",
    "        path[y] = [y]\n",
    "\n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1,len(obs)):\n",
    "        V.append({})\n",
    "        newpath = {}\n",
    "\n",
    "        for y in states:\n",
    "            (prob, state) = max([(V[t-1][y0] * trans_p[y0][y] * emit_p[y][obs[t]], y0) for y0 in states])\n",
    "            V[t][y] = prob\n",
    "            newpath[y] = path[state] + [y]\n",
    "\n",
    "        # Don't need to remember the old paths\n",
    "        path = newpath\n",
    "\n",
    "    print_dptable(V)\n",
    "    (prob, state) = max([(V[len(obs) - 1][y], y) for y in states])\n",
    "    return (prob, path[state])\n",
    "\n",
    "def example():\n",
    "    return viterbi(observations,\n",
    "                   states,\n",
    "                   start_probability,\n",
    "                   transition_probability,\n",
    "                   emission_probability)\n",
    "print example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.6\n",
      "0.22.0\n",
      "1.14.5\n",
      "0.19.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers as L\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(keras.__version__)\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(sklearn.__version__)\n",
    "\n",
    "# utf-8s设定 中文\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "#进行配置，每个GPU使用60%上限现存\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # 使用编号为1，2号的GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3 # 每个GPU现存上届控制在30%以内\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session )\n",
    "\n",
    "\n",
    "BATCH_SIZE=64 #64速度大大加强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "ANSI_X3.4-1968\n",
      "(None, None)\n",
      "None\n",
      "UTF-8\n"
     ]
    }
   ],
   "source": [
    "import locale, sys\n",
    "print(sys.getdefaultencoding())    #系统默认编码\n",
    "print(sys.getfilesystemencoding()) #文件系统编码\n",
    "print(locale.getdefaultlocale())   #系统当前编码\n",
    "print(sys.stdin.encoding)          #终端输入编码\n",
    "print(sys.stdout.encoding)         #终端输出编码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 加载数据和分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102477\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process(inpath, outpath):\n",
    "    with open(inpath, 'r') as fin, open(outpath, 'w') as fout:\n",
    "        for line in fin:\n",
    "            lineno, sen1, sen2 = line.strip().split('\\t')\n",
    "            words1= [ w for w in jieba.cut(sen1) if w.strip() ]\n",
    "            words2= [ w for w in jieba.cut(sen2) if w.strip() ]\n",
    "            union = words1 + words2\n",
    "            same_num = 0\n",
    "            for w in union:\n",
    "                if w in words1 and w in words2:\n",
    "                    same_num += 1\n",
    "            if same_num * 2 >= len(union):\n",
    "                fout.write(lineno + '\\t1\\n')\n",
    "            else:\n",
    "                fout.write(lineno + '\\t0\\n')\n",
    "\n",
    "\n",
    "def load_data(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        i = 0 \n",
    "        for line in f:\n",
    "            lineno, sent1, sent2, tag = line.strip().split('\\t')\n",
    "            data.append((lineno, sent1, sent2, tag))\n",
    "    return data      \n",
    "\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    return [w for w in jieba.cut(sentence) if w.strip() ]\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    return [w for w in sentence.decode('utf-8') if w.strip() ]\n",
    "\n",
    "\n",
    "def segmentation(data, split=split_sentence, userwords_filepath='words.txt', stupwords_filepath='stopwords.txt'):\n",
    "    jieba.load_userdict(userwords_filepath) \n",
    "    new_data = []\n",
    "    for  (lineno, sen1, sen2, tag) in data:\n",
    "        words1 = split(sen1)\n",
    "        words2 = split(sen2)\n",
    "        new_data.append((lineno, words1, words2, tag))\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# 创建停用词list  \n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]  \n",
    "    return stopwords  \n",
    "\n",
    "\n",
    "# 对句子进行分词  \n",
    "def remove_stopwords(data, ):  \n",
    "    sentence_seged = jieba.cut(sentence.strip())  \n",
    "    stopwords = stopwordslist('./test/stopwords.txt')  # 这里加载停用词的路径  \n",
    "    outstr = ''  \n",
    "    for word in sentence_seged:  \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t':  \n",
    "                outstr += word  \n",
    "                outstr += \" \"  \n",
    "    return outstr  \n",
    "\n",
    "\n",
    "def print_unicode(text):\n",
    "    print(repr(text).decode('unicode_escape'))\n",
    "    \n",
    "def print_utf8(text):\n",
    "    print(repr(text).decode('string_escape'))    \n",
    "\n",
    "\n",
    "data_raw = load_data('./data/atec_nlp_sim_train.csv') + load_data('./data/atec_nlp_sim_train_add.csv')\n",
    "print(len(data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('怎么更改花呗手机号码', '我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号')\n",
      "u'\\u600e'\n",
      "u'\\u4e48'\n",
      "u'\\u66f4'\n",
      "u'\\u6539'\n",
      "u'\\u82b1'\n",
      "u'\\u5457'\n",
      "u'\\u624b'\n",
      "u'\\u673a'\n",
      "u'\\u53f7'\n",
      "u'\\u7801'\n",
      "('也开不了花呗，就这样了？完事了', '真的嘛？就是花呗付款')\n",
      "u'\\u4e5f'\n",
      "u'\\u5f00'\n",
      "u'\\u4e0d'\n",
      "u'\\u4e86'\n",
      "u'\\u82b1'\n",
      "u'\\u5457'\n",
      "u'\\uff0c'\n",
      "u'\\u5c31'\n",
      "u'\\u8fd9'\n",
      "u'\\u6837'\n",
      "u'\\u4e86'\n",
      "u'\\uff1f'\n",
      "u'\\u5b8c'\n",
      "u'\\u4e8b'\n",
      "u'\\u4e86'\n"
     ]
    }
   ],
   "source": [
    "for  (lineno, sen1, sen2, tag) in data_raw[0:2]:\n",
    "    print_utf8((sen1, sen2))\n",
    "    for word in sen1.decode('utf-8'):\n",
    "        print_utf8(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = segmentation(data_raw)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ï»¿1', [u'怎', u'么', u'更', u'改', u'花', u'呗', u'手', u'机', u'号', u'码'], [u'我', u'的', u'花', u'呗', u'是', u'以', u'前', u'的', u'手', u'机', u'号', u'码', u'，', u'怎', u'么', u'更', u'改', u'成', u'现', u'在', u'的', u'支', u'付', u'宝', u'的', u'号', u'码', u'手', u'机', u'号'], '1')\n",
      "('4', '如何得知关闭借呗', '想永久关闭借呗', '0')\n",
      "[u'如', u'何', u'得', u'知', u'关', u'闭', u'借', u'呗']\n"
     ]
    }
   ],
   "source": [
    "#去除字符乱码\n",
    "print_unicode(data[0])   \n",
    "data[0]=('1', data[0][1], data[0][2], data[0][3])\n",
    "\n",
    "print_utf8(data_raw[3])\n",
    "print_unicode(data[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2171, 102477)\n",
      "[(u'呗', 211063), (u'花', 151025), (u'么', 83328), (u'还', 80050), (u'借', 69825), (u'我', 67036), (u'款', 62302), (u'的', 61108), (u'了', 56689), (u'用', 52685), (u'*', 50301), (u'不', 49940), (u'，', 46908), (u'怎', 43828), (u'吗', 42240), (u'什', 36860), (u'是', 34797), (u'期', 34456), (u'额', 33816), (u'能', 33219), (u'可', 33022), (u'以', 32472), (u'有', 30674), (u'付', 29243), (u'为', 29144), (u'蚂', 28459), (u'蚁', 28453), (u'分', 24341), (u'度', 22906), (u'开', 21435), (u'没', 21279), (u'支', 20091), (u'钱', 19894), (u'一', 19394), (u'通', 19357), (u'个', 19138), (u'在', 16865), (u'宝', 16033), (u'提', 14865), (u'到', 14833), (u'时', 14146), (u'号', 13690), (u'月', 13378), (u'多', 12928), (u'账', 12853), (u'收', 12378), (u'要', 11408), (u'费', 11183), (u'退', 10992), (u'后', 10709), (u'前', 10689), (u'使', 10529), (u'卡', 10075), (u'会', 9729), (u'手', 9311), (u'信', 9119), (u'里', 8620), (u'码', 8459), (u'现', 8355), (u'扣', 8209), (u'示', 8051), (u'天', 7917), (u'少', 7899), (u'这', 7772), (u'商', 7726), (u'行', 7519), (u'关', 7112), (u'上', 7083), (u'逾', 6830), (u'动', 6670), (u'机', 6588), (u'显', 6483), (u'已', 6449), (u'银', 6410), (u'如', 6410), (u'单', 6391), (u'自', 6230), (u'请', 6026), (u'余', 5753), (u'息', 5613), (u'候', 5586), (u'次', 5562), (u'何', 5549), (u'买', 5486), (u'消', 5467), (u'下', 5367), (u'样', 5359), (u'申', 5216), (u'办', 5187), (u'想', 4951), (u'最', 4912), (u'清', 4853), (u'就', 4824), (u'闭', 4823), (u'和', 4662), (u'经', 4659), (u'才', 4656), (u'日', 4622), (u'换', 4586), (u'哪', 4472)]\n",
      "('three-occur word: ', 131)\n",
      "[u'涉', u'錢', u'赞', u'凡', u'覆', u'洞', u'胀', u'ｐ', u'渝', u'精']\n",
      "('two-occur word: ', 178)\n",
      "[u'潜', u'欲', u'吸', u'巾', u'剛', u'梆', u'肖', u'恁', u'幕', u'库']\n",
      "('one-occur word: ', 481)\n",
      "[u'則', u'伊', u'抗', u'脖', u'贞', u'科', u'孒', u'衰', u'泳', u'闹']\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter()\n",
    "for (_, sent1, sent2, _) in data:   \n",
    "    word_counts.update(sent1)\n",
    "    word_counts.update(sent2)\n",
    "print(len(word_counts), len(data)) \n",
    "print_unicode(word_counts.most_common(100))\n",
    "\n",
    "low_freq_words = [word for word, count in word_counts.items() if count==3] \n",
    "print('three-occur word: ',len(low_freq_words))  #8960  total:13419\n",
    "print_unicode(low_freq_words[0:10])\n",
    "\n",
    "low_freq_words = [word for word, count in word_counts.items() if count==2] \n",
    "print('two-occur word: ',len(low_freq_words))  #1690  total:13419\n",
    "print_unicode(low_freq_words[0:10])\n",
    "\n",
    "low_freq_words = [word for word, count in word_counts.items() if count==1] \n",
    "print('one-occur word: ', len(low_freq_words))  #6423  total:13419\n",
    "print_unicode(low_freq_words[0:10])\n",
    "\n",
    "one_freq_words = low_freq_words\n",
    "\n",
    "#分词的一些问题：  ('我用', 2051)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面结果可以看出，低频词特别多，其中只出现一次的词高达6423，几乎占了所有词汇的一半以上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据分成train, validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102477, 102477)\n",
      "(71733, 71733)\n",
      "(30744, 30744)\n",
      "([u'我', u'的', u'来', u'分', u'期', u'评', u'估', u'和', u'支', u'付', u'宝', u'评', u'估', u'两', u'者', u'是', u'一', u'样', u'的', u'评', u'估', u'么'], [u'我', u'用', u'支', u'付', u'宝', u'买', u'东', u'西', u'也', u'蛮', u'多', u'的'])\n"
     ]
    }
   ],
   "source": [
    "X_all = [(sent1, sent2)  for (_, sent1, sent2, _) in data] \n",
    "y_all = np.array([tag  for (_, _, _, tag) in data]).astype(int)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "print(len(X_all), len(y_all))\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_val), len(y_val))\n",
    "print_unicode(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max length =', 112)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feaa7ff10d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sents_all = [sent for (sent, _) in X_all] + [sent for (_, sent) in X_all]    \n",
    "MAX_LENGTH = max(map(len, sents_all))\n",
    "print(\"max length =\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.xticks(np.linspace(0,48,13,endpoint=True))\n",
    "plt.hist(list(map(len,sents_all)),bins=25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图可以看出，大部分的句子长度在8个长度左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one-occur word: ', 467)\n",
      "[u'則', u'抗', u'脖', u'潜', u'孒', u'衰', u'泳', u'闹', u'驼', u'戗']\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter()\n",
    "for (sent1, sent2) in X_train:   \n",
    "    word_counts.update(sent1)\n",
    "    word_counts.update(sent2)\n",
    "    \n",
    "low_freq_words = [word for word, count in word_counts.items() if count==1] \n",
    "print('one-occur word: ', len(low_freq_words))  #6423  total:13419\n",
    "print_unicode(low_freq_words[0:10])\n",
    "\n",
    "one_freq_words = low_freq_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102477,)\n",
      "[[    0 83792]\n",
      " [    1 18685]]\n"
     ]
    }
   ],
   "source": [
    "print(y_all.shape)\n",
    "y_bin = np.bincount(y_all)\n",
    "ii = np.nonzero(y_bin)[0]\n",
    "print(np.vstack((ii,y_bin[ii])).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果来看数据的倾斜非常厉害。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 矩阵化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "PAD = \"#PAD#\"\n",
    "UNK = \"#UNK#\"\n",
    "START = \"#START#\"\n",
    "END = \"#END#\"\n",
    "\n",
    "\n",
    "def generate_vocabulary(X_train, min_occur_count=5):    \n",
    "    words = [ word for (sent, _)  in X_train for word in sent  ] + [ word for (_, sent)  in X_train for word in sent  ] \n",
    "    vocab = Counter(words)\n",
    "    vocab = [PAD, UNK, START, END] + [word for word, count in vocab.items() if count>=min_occur_count ] \n",
    "    return {token: index for index, token in enumerate(sorted(vocab))}\n",
    "\n",
    "def tokens_to_indices(X, vocab):    \n",
    "    matrix_1 = [[vocab.get(word, vocab[UNK]) for word in sent] for (sent, _)  in X] \n",
    "    matrix_2 = [[vocab.get(word, vocab[UNK]) for word in sent] for (_, sent)  in X]     \n",
    "    return matrix_1, matrix_2\n",
    "\n",
    "def to_matrix(X_indexed, pad=0, max_len=None, dtype='int32'):\n",
    "    if max_len  is None:\n",
    "        max_len = max(map(len, X_indexed[0]) + map(len, X_indexed[1]))\n",
    "    else:\n",
    "        max_len = min(max_len, max(map(len, X_indexed[0]) + map(len, X_indexed[1])))    \n",
    "        \n",
    "    max_len = max_len or max(map(len, X_indexed[0]) + map(len, X_indexed[1]))\n",
    "    matrix1 = np.empty([len(X_indexed[0]),max_len],dtype)\n",
    "    matrix2 = np.empty([len(X_indexed[1]),max_len],dtype)\n",
    "    matrix1.fill(pad)\n",
    "    matrix2.fill(pad)\n",
    "    \n",
    "    for i in range(len(X_indexed[0])):\n",
    "        line_ix = X_indexed[0][i][:max_len]\n",
    "        matrix1[i,:len(line_ix)] = line_ix\n",
    "        \n",
    "    for i in range(len(X_indexed[1])):\n",
    "        line_ix = X_indexed[1][i][:max_len]\n",
    "        matrix2[i,:len(line_ix)] = line_ix    \n",
    "    return matrix1, matrix2    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = generate_vocabulary(X_train, min_occur_count=1)\n",
    "vocab_inverse = {idx: w for w, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n",
      "[u'怀', u'挂', u'耀', u'谈', u'随']\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print_unicode(list(vocab)[0:5])\n",
    "# print(vocab[PAD], vocab[UNK], vocab[START], vocab[END], vocab[u\"花呗\"], vocab[u\"借呗\"], vocab[u\"开通\"], vocab[u\"支付宝\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index化。\n",
    "X_train_indexed = tokens_to_indices(X_train, vocab)\n",
    "X_val_indexed = tokens_to_indices(X_val, vocab)\n",
    "X_sample_indexed = tokens_to_indices(X_train[0:3], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "[u'我', u'的', u'来', u'分', u'期', u'评', u'估', u'和', u'支', u'付', u'宝', u'评', u'估', u'两', u'者', u'是', u'一', u'样', u'的', u'评', u'估', u'么']\n",
      "[u'我', u'用', u'支', u'付', u'宝', u'买', u'东', u'西', u'也', u'蛮', u'多', u'的']\n",
      "[u'我', u'的', u'来', u'分', u'期', u'评', u'估', u'和', u'支', u'付', u'宝', u'评', u'估', u'两', u'者', u'是', u'一', u'样', u'的', u'评', u'估', u'么']\n",
      "[u'我', u'用', u'支', u'付', u'宝', u'买', u'东', u'西', u'也', u'蛮', u'多', u'的']\n",
      "-----------------------------------------\n",
      "[u'身', u'份', u'证', u'过', u'期', u'借', u'呗', u'是', u'不', u'是', u'额', u'度', u'没', u'办', u'法', u'用']\n",
      "[u'我', u'的', u'身', u'份', u'证', u'过', u'期', u'了', u'为', u'什', u'么', u'不', u'能', u'办', u'蚂', u'蚁', u'借', u'呗']\n",
      "[u'身', u'份', u'证', u'过', u'期', u'借', u'呗', u'是', u'不', u'是', u'额', u'度', u'没', u'办', u'法', u'用']\n",
      "[u'我', u'的', u'身', u'份', u'证', u'过', u'期', u'了', u'为', u'什', u'么', u'不', u'能', u'办', u'蚂', u'蚁', u'借', u'呗']\n",
      "-----------------------------------------\n",
      "[u'借', u'呗', u'提', u'前', u'还', u'款', u'后', u'没', u'额', u'度', u'什', u'么', u'原', u'因']\n",
      "[u'借', u'呗', u'提', u'前', u'还', u'款', u'没', u'有', u'额', u'度', u'了', u'，', u'都', u'好', u'几', u'个', u'月', u'都', u'出', u'不', u'来', u'了']\n",
      "[u'借', u'呗', u'提', u'前', u'还', u'款', u'后', u'没', u'额', u'度', u'什', u'么', u'原', u'因']\n",
      "[u'借', u'呗', u'提', u'前', u'还', u'款', u'没', u'有', u'额', u'度', u'了', u'，', u'都', u'好', u'几', u'个', u'月', u'都', u'出', u'不', u'来', u'了']\n",
      "=========================================\n",
      "[u'則', u'抗', u'脖', u'潜', u'孒', u'衰', u'泳', u'闹', u'驼', u'戗']\n",
      "=========================================\n",
      "-----------------------------------------\n",
      "[827, 1314, 1045, 309, 1026, 1672, 194, 465, 954, 172, 637, 1672, 194, 110, 1500, 997, 92, 1075, 1314, 1672, 194, 125]\n",
      "[827, 1291, 954, 172, 637, 139, 106, 1635, 135, 1604, 581, 1314]\n",
      "array([ 827, 1314, 1045,  309, 1026, 1672,  194,  465,  954,  172,  637,\n",
      "       1672,  194,  110, 1500,  997,   92, 1075, 1314, 1672,  194,  125],\n",
      "      dtype=int32)\n",
      "array([ 827, 1291,  954,  172,  637,  139,  106, 1635,  135, 1604,  581,\n",
      "       1314,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3],\n",
      "      dtype=int32)\n",
      "-----------------------------------------\n",
      "[1760, 182, 1671, 1786, 1026, 236, 454, 997, 100, 997, 1951, 735, 1147, 340, 1151, 1291]\n",
      "[827, 1314, 1760, 182, 1671, 1786, 1026, 142, 119, 163, 125, 100, 1529, 340, 1600, 1599, 236, 454]\n",
      "array([1760,  182, 1671, 1786, 1026,  236,  454,  997,  100,  997, 1951,\n",
      "        735, 1147,  340, 1151, 1291,    3,    3,    3,    3,    3,    3],\n",
      "      dtype=int32)\n",
      "array([ 827, 1314, 1760,  182, 1671, 1786, 1026,  142,  119,  163,  125,\n",
      "        100, 1529,  340, 1600, 1599,  236,  454,    3,    3,    3,    3],\n",
      "      dtype=int32)\n",
      "-----------------------------------------\n",
      "[236, 454, 930, 331, 1791, 1108, 434, 1147, 1951, 735, 163, 125, 395, 526]\n",
      "[236, 454, 930, 331, 1791, 1108, 1147, 1022, 1951, 735, 142, 2012, 1837, 600, 300, 112, 1021, 1837, 305, 100, 1045, 142]\n",
      "array([ 236,  454,  930,  331, 1791, 1108,  434, 1147, 1951,  735,  163,\n",
      "        125,  395,  526,    3,    3,    3,    3,    3,    3,    3,    3],\n",
      "      dtype=int32)\n",
      "array([ 236,  454,  930,  331, 1791, 1108, 1147, 1022, 1951,  735,  142,\n",
      "       2012, 1837,  600,  300,  112, 1021, 1837,  305,  100, 1045,  142],\n",
      "      dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(X_sample_indexed[0])):\n",
    "    print('-----------------------------------------')\n",
    "    sent1 = [vocab_inverse[index] for index in X_sample_indexed[0][i]]\n",
    "    sent2 = [vocab_inverse[index] for index in X_sample_indexed[1][i]]\n",
    "    print_unicode(sent1)\n",
    "    print_unicode(sent2)\n",
    "    print_unicode(X_train[i][0])\n",
    "    print_unicode(X_train[i][1])\n",
    "  \n",
    "    \n",
    "#检查一下，是否一些低频词被替换成#UNK#。\n",
    "def filter_by_word(X, word):\n",
    "    indices = [i for i,(sent1, sent2) in enumerate(X) if word in sent1 or word in sent2]\n",
    "    return indices\n",
    "\n",
    "def show_words(X_indexed, indice):\n",
    "    sent1 = [ vocab_inverse[i] for i in X_indexed[0][indice]] \n",
    "    sent2 = [ vocab_inverse[i] for i in X_indexed[1][indice]] \n",
    "    print_unicode((sent1, sent2))\n",
    "\n",
    "print('=========================================')  \n",
    "print_unicode(one_freq_words[0:10])  # [u'只得', u'有发', u'填完', u'过日子', u'二立', u'选泽', u'银子', u'吸', u'号通', u'禁言']\n",
    "# indice = filter_by_word(X_train, u'过日子')[0]\n",
    "# print_unicode(X_train[indice])\n",
    "# show_words(X_train_indexed, indice)\n",
    "# print(X_train_indexed[0][indice], X_train_indexed[1][indice])\n",
    "\n",
    "print('=========================================') \n",
    "\n",
    "\n",
    "X_train_seq = to_matrix(X_train_indexed, max_len=25, pad=vocab[PAD])\n",
    "X_val_seq = to_matrix(X_val_indexed, max_len=25, pad=vocab[PAD])\n",
    "X_sample_seq = to_matrix(X_sample_indexed, max_len=25, pad=vocab[PAD])\n",
    "\n",
    "\n",
    "for i in range(len(X_sample_indexed[0])):\n",
    "    print('-----------------------------------------')\n",
    "    sent1 = X_sample_indexed[0][i]\n",
    "    sent2 = X_sample_indexed[1][i]\n",
    "    sent11 = X_sample_seq[0][i]\n",
    "    sent21 = X_sample_seq[1][i]   \n",
    "    print_unicode(sent1)\n",
    "    print_unicode(sent2)\n",
    "    print_unicode(sent11)\n",
    "    print_unicode(sent21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_matrix_dssm(X_indexed, vocab_len, ngram=2, pad=0, max_len=None, dtype='int32'):\n",
    "#     if max_len  is None:\n",
    "#         max_len = max(map(len, X_indexed[0]) + map(len, X_indexed[1]))\n",
    "#     else:\n",
    "#         max_len = min(max_len, max(map(len, X_indexed[0]) + map(len, X_indexed[1])))    \n",
    "        \n",
    "#     max_len = max_len or max(map(len, X_indexed[0]) + map(len, X_indexed[1]))\n",
    "#     max_depth = vocab_len*ngram\n",
    "#     matrix1 = np.zeros([len(X_indexed[0]),max_len, max_depth],dtype)\n",
    "#     matrix2 = np.zeros([len(X_indexed[1]),max_len, max_depth],dtype)\n",
    "    \n",
    "#     for i in range(len(X_indexed[0])):\n",
    "#         line_ix = X_indexed[0][i][:max_len]\n",
    "#         one_gram_matrix = np.zeros([len(line_ix)+ngram-1, vocab_len])\n",
    "#         one_gram_matrix[0:len(line_ix),] = np.eye(vocab_len)[line_ix]\n",
    "#         for j in range(len(line_ix)):\n",
    "#             matrix1[i, j, :] = one_gram_matrix[j:(j+ngram),].flatten()\n",
    "        \n",
    "#     for i in range(len(X_indexed[1])):\n",
    "#         line_ix = X_indexed[1][i][:max_len]\n",
    "#         one_gram_matrix = np.zeros([len(line_ix)+ngram-1, vocab_len])\n",
    "#         one_gram_matrix[0:len(line_ix),] = np.eye(vocab_len)[line_ix]\n",
    "#         for j in range(len(line_ix)):\n",
    "#             matrix2[i, j, :] = one_gram_matrix[j:(j+ngram),].flatten()\n",
    "\n",
    "#     print(matrix1)\n",
    "#     print(matrix2)\n",
    "#     return (matrix1, matrix2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_dssm = to_matrix_dssm(X_train_indexed, vocab_len=len(vocab), ngram=2, max_len=25)\n",
    "# X_val_dssm =   to_matrix_dssm(X_val_indexed,   vocab_len=len(vocab), ngram=2, max_len=25)\n",
    "\n",
    "# X_sample_indexed_dssm= ([[1, 2, 3, 2], [0, 4]], [[2,3], [0, 3, 2]])\n",
    "# X_sample_dssm = to_matrix_dssm(X_sample_indexed_dssm, vocab_len=5, ngram=2, max_len=5)\n",
    "\n",
    "\n",
    "# # def to_matrix_dssm(X_indexed, vocab_len, ngram=2, pad=0, max_len=None, dtype='int32'):\n",
    "# for i in range(len(X_sample_indexed_dssm[0])):\n",
    "#     print('-----------------------------------------')\n",
    "#     sent1 = X_sample_indexed_dssm[0][i]\n",
    "#     sent2 = X_sample_indexed_dssm[1][i]\n",
    "#     sent11 = X_sample_dssm[0][i]\n",
    "#     sent21 =  X_sample_dssm[1][i]  \n",
    "#     print_unicode(sent1)\n",
    "#     print_unicode(sent2)\n",
    "#     print_unicode(sent11)\n",
    "#     print_unicode(sent21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 BOW(bag-of-words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize(X_train):\n",
    "    corpus = [\"\".join(sent) for (sent, _) in X_train] + [\"\".join(sent) for (_, sent) in X_train]\n",
    "#     vectorizer = TfidfVectorizer(ngram_range=(1,1), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "#     vectorizer = TfidfVectorizer(ngram_range=(1,2), analyzer='char')\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,1), analyzer='char')\n",
    "    vectorizer.fit(corpus) \n",
    "    return vectorizer\n",
    "\n",
    "def tfidf_to_matrix(X, vectorizer):\n",
    "    corpus_list1 = [\"\".join(sent) for (sent, _) in X]\n",
    "    corpus_list2 = [\"\".join(sent) for (_, sent) in X]\n",
    "    matrix_1 = vectorizer.transform(corpus_list1)\n",
    "    matrix_2 = vectorizer.transform(corpus_list2)\n",
    "    return matrix_1, matrix_2\n",
    "\n",
    "vectorizer= tfidf_vectorize(X_train)  \n",
    "X_train_tfidf = tfidf_to_matrix(X_train, vectorizer)\n",
    "X_val_tfidf = tfidf_to_matrix(X_val, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u'我', u'的', u'来', u'分', u'期', u'评', u'估', u'和', u'支', u'付', u'宝', u'评', u'估', u'两', u'者', u'是', u'一', u'样', u'的', u'评', u'估', u'么'], [u'我', u'用', u'支', u'付', u'宝', u'买', u'东', u'西', u'也', u'蛮', u'多', u'的'])\n",
      "([u'我', u'的', u'来', u'分', u'期', u'评', u'估', u'和', u'支', u'付', u'宝', u'评', u'估', u'两', u'者', u'是', u'一', u'样', u'的', u'评', u'估', u'么'], [u'我', u'用', u'支', u'付', u'宝', u'买', u'东', u'西', u'也', u'蛮', u'多', u'的'])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output: \n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):        \n",
    "    with open(filename, 'rb') as input:\n",
    "        obj = pickle.load(input)        \n",
    "    return obj\n",
    "        \n",
    "save_object(X_train, 'X_train.pkl')\n",
    "\n",
    "X_train_load  = load_object('X_train.pkl')\n",
    "print_unicode(X_train[0])\n",
    "print_unicode(X_train_load[0])\n",
    "\n",
    "# file_X_train = open('X_train.obj', 'w')\n",
    "# pickle.dump(X_train, file_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((71733, 2021), (71733, 2021))\n",
      "((30744, 2021), (30744, 2021))\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0].shape, X_train_tfidf[1].shape)\n",
    "print(X_val_tfidf[0].shape, X_val_tfidf[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------row 0------------------------\n",
      "sent1: 我的来分期评估和支付宝评估两者是一样的评估么 \n",
      "sent2: 我用支付宝买东西也蛮多的 \n",
      "y = 0\n",
      "tfidf1: 一=0.10378576, 两=0.16997967, 么=0.05810486, 付=0.09276879, 估=0.61879771, 分=0.09734784, 和=0.14428454, 宝=0.10969795, 我=0.06849599, 支=0.10355410, 是=0.08933426, 期=0.08704420, 来=0.15649276, 样=0.14027876, 的=0.14178021, 者=0.23020450, 评=0.61227283\n",
      "tfidf2: 东=0.30985178, 也=0.31565385, 买=0.26733672, 付=0.17686618, 多=0.21959855, 宝=0.20914208, 我=0.13058943, 支=0.19742866, 用=0.14059786, 的=0.13515388, 蛮=0.64686831, 西=0.31166555\n",
      "------------------------row 400------------------------\n",
      "sent1: 花呗有没有像信用卡那样最后还款日后三天还都不算逾期，也没费用 \n",
      "sent2: 我花呗逾期了，现在工资还没发，能不能宽限几天 \n",
      "y = 0\n",
      "tfidf1: 三=0.26629830, 不=0.10145845, 也=0.22628726, 信=0.16976104, 像=0.32835394, 卡=0.16656231, 后=0.32784250, 呗=0.04177731, 天=0.17927967, 日=0.20034204, 最=0.19483287, 有=0.24726129, 期=0.11896844, 样=0.19172724, 款=0.09496800, 没=0.27160251, 用=0.20158477, 算=0.22285566, 花=0.05551629, 费=0.16343738, 还=0.17804385, 逾=0.18204135, 那=0.22435108, 都=0.20242265, ，=0.11253271\n",
      "tfidf2: 不=0.11689450, 了=0.11580737, 几=0.24428352, 发=0.30204391, 呗=0.04813337, 在=0.16811447, 天=0.20655556, 宽=0.38942808, 工=0.36537846, 我=0.10786062, 期=0.13706849, 没=0.15646228, 现=0.20069310, 能=0.27459758, 花=0.06396263, 资=0.32538320, 还=0.10256586, 逾=0.20973741, 限=0.24762940, ，=0.25930724\n",
      "------------------------row 800------------------------\n",
      "sent1: 蚂蚁借呗借的钱都第二天了怎么还不到 \n",
      "sent2: 借呗怎么不能提现了 \n",
      "y = 0\n",
      "tfidf1: 不=0.16834506, 么=0.13176999, 了=0.16677943, 二=0.39008258, 借=0.29913256, 到=0.25191911, 呗=0.06931905, 天=0.29747001, 怎=0.17429533, 的=0.16076432, 第=0.44652822, 蚁=0.20395873, 蚂=0.20396567, 还=0.14770974, 都=0.33587004, 钱=0.23375257\n",
      "tfidf2: 不=0.29936314, 么=0.23432276, 了=0.29657903, 借=0.26596938, 呗=0.12326805, 怎=0.30994432, 提=0.44465298, 现=0.51396870, 能=0.35161788\n",
      "------------------------row 1200------------------------\n",
      "sent1: 我现在需要咨询花呗转移 \n",
      "sent2: 现在企业支付宝无法使用花呗 \n",
      "y = 0\n",
      "tfidf1: 呗=0.06000983, 咨=0.49015613, 在=0.20959513, 我=0.13447421, 现=0.25021222, 移=0.46991534, 花=0.07974480, 要=0.23202540, 询=0.37165384, 转=0.33256626, 需=0.31903583\n",
      "tfidf2: 业=0.41933539, 付=0.18301837, 企=0.52796876, 使=0.23807550, 呗=0.06030333, 在=0.21062024, 宝=0.21641697, 支=0.20429611, 无=0.34140857, 法=0.33360488, 现=0.25143599, 用=0.14548848, 花=0.08013483\n",
      "------------------------row 1600------------------------\n",
      "sent1: 我花呗什么时候提额的 \n",
      "sent2: 花呗怎么一直不提高额度 \n",
      "y = 0\n",
      "tfidf1: 么=0.21362667, 什=0.30243725, 候=0.51198063, 呗=0.11238065, 我=0.25183040, 提=0.40537990, 时=0.41187151, 的=0.26063253, 花=0.14933841, 额=0.31705040\n",
      "tfidf2: 一=0.30381774, 不=0.21730597, 么=0.17009353, 呗=0.08947957, 度=0.28490296, 怎=0.22498679, 提=0.32277103, 直=0.50305608, 花=0.11890603, 额=0.25244144, 高=0.50659043\n",
      "------------------------row 2000------------------------\n",
      "sent1: 我这个月明明已经还了最低还款，为什么还要扣钱 \n",
      "sent2: 花呗还款了为什么还叫我还款 \n",
      "y = 0\n",
      "tfidf1: 个=0.17558388, 为=0.14526305, 么=0.09455185, 了=0.11967296, 什=0.13385970, 低=0.24348678, 已=0.21951529, 我=0.11146095, 扣=0.21078425, 明=0.59594110, 最=0.23196791, 月=0.19123615, 款=0.11306885, 经=0.23451734, 要=0.19231771, 还=0.31796836, 这=0.21231857, 钱=0.16772968, ，=0.13398139\n",
      "tfidf2: 为=0.24055216, 么=0.15657562, 了=0.19817557, 什=0.22166849, 叫=0.59907213, 呗=0.08236832, 我=0.18457668, 款=0.37447864, 花=0.10945616, 还=0.52654805\n",
      "------------------------row 2400------------------------\n",
      "sent1: 蚂蚁借呗逾期后会自动扣款吗 \n",
      "sent2: 蚂蚁借呗是自动扣钱吗 \n",
      "y = 0\n",
      "tfidf1: 会=0.33295920, 借=0.17559955, 动=0.35744988, 后=0.31932810, 吗=0.20729069, 呗=0.08138461, 扣=0.34488503, 期=0.23175741, 款=0.18500316, 自=0.36282217, 蚁=0.23945947, 蚂=0.23946761, 逾=0.35462710\n",
      "tfidf2: 借=0.20908244, 动=0.42560752, 吗=0.24681635, 呗=0.09690283, 扣=0.41064685, 是=0.28320830, 自=0.43200420, 蚁=0.28511900, 蚂=0.28512870, 钱=0.32676854\n",
      "------------------------row 2800------------------------\n",
      "sent1: 为什么我的花呗帮定不了银行卡 \n",
      "sent2: 为什么我银行卡已经绑定了花呗然后回到花呗首页还是显示要绑定银行卡 \n",
      "y = 0\n",
      "tfidf1: 不=0.19605114, 为=0.23576029, 么=0.15345658, 了=0.19422785, 什=0.21725278, 卡=0.32185324, 呗=0.08072752, 定=0.39395465, 帮=0.45209412, 我=0.18089985, 的=0.18722278, 花=0.10727576, 行=0.34644325, 银=0.35847586\n",
      "tfidf2: 为=0.10712640, 么=0.06972867, 了=0.08825460, 什=0.09871683, 到=0.13330792, 卡=0.29249183, 后=0.14392700, 呗=0.07336306, 回=0.17928433, 定=0.35801571, 已=0.16188481, 我=0.08219853, 是=0.10720548, 显=0.16125335, 然=0.21601547, 示=0.15345175, 经=0.17294830, 绑=0.36744453, 花=0.09748941, 行=0.31483859, 要=0.14182755, 还=0.07816350, 银=0.32577351, 页=0.24726818, 首=0.26205447\n",
      "------------------------row 3200------------------------\n",
      "sent1: 去超市使用花呗最多可以付多钱 \n",
      "sent2: 花呗可以到超市有吗 \n",
      "y = 0\n",
      "tfidf1: 付=0.19173610, 以=0.17903460, 使=0.24941578, 去=0.33457927, 可=0.17844270, 呗=0.06317577, 多=0.47612235, 市=0.43536139, 最=0.29462685, 用=0.15241854, 花=0.08395190, 超=0.37748964, 钱=0.21303665\n",
      "tfidf2: 以=0.24814991, 到=0.31822638, 可=0.24732950, 吗=0.22303096, 呗=0.08756442, 市=0.60343021, 有=0.25912741, 花=0.11636106, 超=0.52321740\n",
      "------------------------row 3600------------------------\n",
      "sent1: ***积分怎么借不了，借呗 \n",
      "sent2: 借呗以授权为什么借不了 \n",
      "y = 0\n",
      "tfidf1: *=0.75540809, 不=0.16476861, 么=0.12897057, 了=0.16323624, 借=0.29277756, 分=0.21607498, 呗=0.06784638, 怎=0.17059246, 积=0.39927395, ，=0.18275322\n",
      "tfidf2: 不=0.20627362, 为=0.24805327, 么=0.16145810, 了=0.20435526, 什=0.22858075, 以=0.24070348, 借=0.36652788, 呗=0.08493680, 授=0.54728040, 权=0.52614719\n",
      "------------------------row 4000------------------------\n",
      "sent1: 南充可以用花呗加油吗 \n",
      "sent2: 花呗可以滴滴打车支付吗 \n",
      "y = 0\n",
      "tfidf1: 以=0.17553494, 充=0.34189602, 加=0.39740119, 南=0.62922424, 可=0.17495461, 吗=0.15776644, 呗=0.06194084, 油=0.45825819, 用=0.14943916, 花=0.08231086\n",
      "tfidf2: 付=0.17322986, 以=0.16175430, 可=0.16121953, 吗=0.14538074, 呗=0.05707808, 打=0.34211881, 支=0.19336959, 滴=0.79448711, 花=0.07584892, 车=0.31943990\n",
      "------------------------row 4400------------------------\n",
      "sent1: 使用借呗会影响以后贷款买房吗 \n",
      "sent2: 借呗会影响证信吗 \n",
      "y = 0\n",
      "tfidf1: 买=0.30245322, 以=0.18684328, 会=0.26973651, 使=0.26029416, 借=0.14225650, 后=0.25869370, 吗=0.16793008, 呗=0.06593121, 响=0.32308158, 影=0.32233989, 房=0.50546640, 款=0.14987454, 用=0.15906635, 贷=0.31913719\n",
      "tfidf2: 会=0.37604865, 信=0.37350198, 借=0.19832453, 吗=0.23411692, 呗=0.09191689, 响=0.45041879, 影=0.44938477, 证=0.46007376\n",
      "------------------------row 4800------------------------\n",
      "sent1: 查询蚂蚁花呗分多少期 \n",
      "sent2: 花呗分期分了***期 \n",
      "y = 1\n",
      "tfidf1: 分=0.27419566, 呗=0.08609596, 多=0.32442988, 少=0.36239982, 期=0.24517383, 查=0.44153595, 花=0.11440968, 蚁=0.25332176, 蚂=0.25333038, 询=0.53321091\n",
      "tfidf2: *=0.77663420, 了=0.16782300, 分=0.44429289, 呗=0.06975279, 期=0.39726736, 花=0.09269185\n",
      "------------------------row 5200------------------------\n",
      "sent1: 我本期还款最低还款花呗算逾期吗 \n",
      "sent2: 花呗可以每个月还最低还款嘛 \n",
      "y = 0\n",
      "tfidf1: 低=0.33412994, 吗=0.17385357, 呗=0.06825683, 我=0.15295467, 最=0.31832293, 期=0.38874735, 本=0.40919842, 款=0.31032228, 算=0.36410728, 花=0.09070393, 还=0.29089259, 逾=0.29742381\n",
      "tfidf2: 个=0.26780080, 以=0.21499075, 低=0.37136639, 可=0.21427998, 呗=0.07586358, 嘛=0.42451762, 最=0.35379780, 月=0.29167365, 款=0.17245277, 每=0.39696743, 花=0.10081225, 还=0.32331054\n",
      "------------------------row 5600------------------------\n",
      "sent1: 花呗里的钱在淘宝上怎么用 \n",
      "sent2: 我在淘宝买东西怎么使用花呗 \n",
      "y = 1\n",
      "tfidf1: 上=0.39608673, 么=0.17307547, 呗=0.09104825, 在=0.31800240, 宝=0.32675452, 怎=0.22893107, 淘=0.44576433, 用=0.21966400, 的=0.21115855, 花=0.12099059, 里=0.37962756, 钱=0.30702617\n",
      "tfidf2: 东=0.41255778, 么=0.14749801, 买=0.35595033, 使=0.30633429, 呗=0.07759294, 在=0.27100732, 宝=0.27846603, 怎=0.19509914, 我=0.17387567, 淘=0.37988831, 用=0.18720158, 花=0.10311034, 西=0.41497276\n"
     ]
    }
   ],
   "source": [
    "#查看tfidf后的结果\n",
    "def show_sentence(X, y, X_tfidf, words, items=range(10), y_pred=None, a_pred=None):\n",
    "    for i in items:\n",
    "        print('------------------------row %d------------------------' % (i))\n",
    "        print(\"sent1: {} \\nsent2: {} \".format(''.join(X[i][0]), ''.join(X[i][1])) )\n",
    "        if y_pred is None:\n",
    "            print(\"y = {}\".format(y[i]))\n",
    "        else:\n",
    "            print(\"y = {}, y_pred = {}, a_pred= {}\".format(y[i], y_pred[i], a_pred[i]))\n",
    "        sent1_tfidf, sent2_tfidf = [], []\n",
    "        for j in range(len(words)):\n",
    "            if X_tfidf[0][i,j] >= 1e-10: sent1_tfidf.append(words[j] + \"=\" + '{:.8f}'.format(X_tfidf[0][i,j]))\n",
    "            if X_tfidf[1][i,j] >= 1e-10: sent2_tfidf.append(words[j] + \"=\" + '{:.8f}'.format(X_tfidf[1][i,j]))\n",
    "        print('tfidf1: {}'.format(', '.join(sent1_tfidf)))\n",
    "        print('tfidf2: {}'.format(', '.join(sent2_tfidf)))\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "show_sentence(X_train, y_train,  X_train_tfidf, words, items=np.arange(0, 6000, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(y_index, batch_size):\n",
    "    indices = np.random.permutation(np.arange(len(y_index)))\n",
    "    return list(y_index[indices[0:batch_size]] )\n",
    "\n",
    "#尝试随机的得到label比例是1：1的batch数据， 不知道这样是覅偶可以有做帮助数据倾斜的问题。\n",
    "#看来这招还挺有用，val f1很快从0.25提高到0.36\n",
    "#random_sent 无论是true, false, 对结果影响不是很大。\n",
    "def generate_batches_tfidf(X, y, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True):  \n",
    "    while True:\n",
    "        y_index_1 = np.array([i for i,label in enumerate(y) if label==1])\n",
    "        y_index_0 = np.array([i for i,label in enumerate(y) if label==0])\n",
    "\n",
    "        y_index_1_batch_size = int(batch_size*positive_ratio)\n",
    "        y_index_0_batch_size = batch_size - y_index_1_batch_size\n",
    "        \n",
    "        for start in range(0,len(y)-1,batch_size):\n",
    "            index = np.random.permutation(np.arange(2)) if random_sent else np.arange(2)\n",
    "            batch_indices = get_indices(y_index_1, y_index_1_batch_size) + get_indices(y_index_0, y_index_0_batch_size)\n",
    "            yield [X[index[0]][batch_indices].toarray(), X[index[1]][batch_indices].toarray()], y[batch_indices]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是自己写的一个求f1_score, precison, recall, accuracy的类，应该可以用sklearn里面的相同功能替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        A = self.model.predict(X)\n",
    "        return self.predict_(A)\n",
    "\n",
    "    def predict_(self, A):\n",
    "        return A\n",
    "    \n",
    "\n",
    "class ClassificationPredictor(Predictor):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        Predictor.__init__(self, model)\n",
    "\n",
    "    def predict_(self, A):\n",
    "        if A.shape[1]== 1:\n",
    "            return np.int32(A > 0.5).flatten()\n",
    "        else: \n",
    "            return  np.argmax(A, axis=-1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return self.accuracy_(y_pred, y)\n",
    "\n",
    "    def accuracy_(self, y_pred, y):\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "    def evaluate(self, X, y, title=\"\"):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = self.accuracy_(y_pred, y)\n",
    "\n",
    "        metrics = []\n",
    "\n",
    "        tp = np.sum((y_pred==1) * (y==1))*1.0\n",
    "        fn = np.sum((y_pred==0) * (y==1))*1.0\n",
    "        fp = np.sum((y_pred==1) * (y==0))*1.0\n",
    "        tn = np.sum((y_pred==0) * (y==0))*1.0\n",
    "\n",
    "        recall = tp/(tp+fn) if tp>0 else 0\n",
    "        precision = tp/(tp+fp) if tp>0 else 0\n",
    "        specificity = tn/(tn+fp) if tn>0 else 0\n",
    "        f1 = 2*recall*precision/(recall+precision) if recall+precision>0 else 0\n",
    "\n",
    "        metrics.append([accuracy, recall, precision, specificity, f1, tp, fn, fp, tn])\n",
    "\n",
    "        metrics = pd.DataFrame(metrics, index=[title],\n",
    "                               columns=['accuracy', 'recall', 'precision', 'specificity', 'f1',  'tp', 'fn', 'fp', 'tn'])\n",
    "        return metrics\n",
    "\n",
    "    def print_metrics(self, train_X, train_y, dev_X=None, dev_y=None, test_X=None, test_y=None):\n",
    "        metrics = []\n",
    "        if train_X is not None:\n",
    "            metrics_ = self.evaluate(train_X, train_y)\n",
    "            metrics_.index=[\"train\"]\n",
    "            metrics.append(metrics_)\n",
    "        if dev_X is not None:\n",
    "            metrics_ = self.evaluate(dev_X, dev_y)\n",
    "            metrics_.index = [\"dev\"]\n",
    "            metrics.append(metrics_)\n",
    "        if test_X is not None:\n",
    "            metrics_ = self.evaluate(test_X, test_y)\n",
    "            metrics_.index = [\"test\"]\n",
    "            metrics.append(metrics_)\n",
    "        print(pd.concat(metrics))\n",
    "\n",
    "    def print_accuracy(self, X, y, title=\"train\"):\n",
    "        print(\"{} accuracy: {}\".format(title, self.accuracy(X, y)))\n",
    "\n",
    "    def print_accuracy_train_test(self, train_X, train_y, dev_X=None, dev_y=None, test_X=None, test_y=None):\n",
    "        if train_X is not None:\n",
    "            self.print_accuracy(train_X, train_y, \"train\")\n",
    "        if dev_X is not None:\n",
    "            self.print_accuracy(dev_X, dev_y, \"dev\")\n",
    "        if test_X is not None:\n",
    "            self.print_accuracy(test_X, test_y, \"test\")\n",
    "            \n",
    "def accuracy1(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precision1 = precision(y_true, y_pred)\n",
    "    recall1 = recall(y_true, y_pred)\n",
    "    return 2*((precision1*recall1)/(precision1+recall1+K.epsilon()))\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 BOW + NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "\n",
    "    input_shape = X_train_tfidf[0].shape[1]\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.Dense(512, input_shape=(input_shape,)))\n",
    "    model_sent1.add(L.Activation('relu')) \n",
    "    model_sent1.add(L.Dropout(0.1))\n",
    "    model_sent1.add(L.Dense(128))\n",
    "    model_sent1.add(L.Activation('relu')) \n",
    "    model_sent1.add(L.Dropout(0.1))\n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.Dense(512, input_shape=(input_shape,)))\n",
    "    model_sent2.add(L.Activation('relu')) \n",
    "    model_sent2.add(L.Dropout(0.1))\n",
    "    model_sent2.add(L.Dense(128))\n",
    "    model_sent2.add(L.Activation('relu')) \n",
    "    model_sent2.add(L.Dropout(0.1))\n",
    "\n",
    "    input1 = L.Input(shape=(input_shape, ))\n",
    "    input2 = L.Input(shape=(input_shape, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    X = L.concatenate([encoder1, encoder2])\n",
    "    X = L.Dense(64)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dropout(0.1)(X)\n",
    "    X = L.Dense(32)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dropout(0.1)(X)\n",
    "    predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_accuracy(model):\n",
    "    #predict tag probabilities of shape [batch,time,n_tags]\n",
    "    predicted_tag_probabilities = model.predict(X_val_tfidf, verbose=1)\n",
    "\n",
    "    #compute accurary excluding padding\n",
    "    numerator = np.sum(predicted_tags == y_val)\n",
    "    denominator = len(y_val)\n",
    "    return float(numerator)/denominator\n",
    "\n",
    "\n",
    "class EvaluateAccuracy(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        sys.stdout.flush()\n",
    "        print(\"\\nMeasuring validation accuracy...\")\n",
    "        acc = compute_test_accuracy(self.model)\n",
    "        print(\"\\nValidation accuracy: %.5f\\n\" % acc)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "(71733, 1955)\n",
      "((71733, 1955), (71733, 1955))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1955)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1955)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          1067136     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          1067136     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           2080        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,152,833\n",
      "Trainable params: 2,152,833\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 71733 samples, validate on 30744 samples\n",
      "Epoch 1/5\n",
      "71733/71733 [==============================] - 12s 171us/step - loss: 0.4729 - acc: 0.8163 - recall: 5.9480e-04 - precision: 2.1243e-04 - f1: 3.1305e-04 - val_loss: 0.4574 - val_acc: 0.8201 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "Epoch 2/5\n",
      "71733/71733 [==============================] - 12s 167us/step - loss: 0.4644 - acc: 0.8170 - recall: 0.0062 - precision: 0.0683 - f1: 0.0112 - val_loss: 0.4552 - val_acc: 0.8206 - val_recall: 0.0329 - val_precision: 0.2613 - val_f1: 0.0567\n",
      "Epoch 3/5\n",
      "71733/71733 [==============================] - 12s 167us/step - loss: 0.4584 - acc: 0.8186 - recall: 0.0375 - precision: 0.2891 - f1: 0.0643 - val_loss: 0.4506 - val_acc: 0.8214 - val_recall: 0.0269 - val_precision: 0.2319 - val_f1: 0.0471\n",
      "Epoch 4/5\n",
      "71733/71733 [==============================] - 12s 167us/step - loss: 0.4494 - acc: 0.8214 - recall: 0.0800 - precision: 0.4625 - f1: 0.1310 - val_loss: 0.4548 - val_acc: 0.8197 - val_recall: 0.0714 - val_precision: 0.3924 - val_f1: 0.1159\n",
      "Epoch 5/5\n",
      "71733/71733 [==============================] - 12s 167us/step - loss: 0.4401 - acc: 0.8253 - recall: 0.1208 - precision: 0.5506 - f1: 0.1883 - val_loss: 0.4511 - val_acc: 0.8205 - val_recall: 0.0707 - val_precision: 0.3828 - val_f1: 0.1142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe628e07210>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train_tfidf[1].shape)\n",
    "print(X_train_tfidf[0].shape, X_train_tfidf[1].shape)\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit([X_train_tfidf[0], X_train_tfidf[1]], y_train, validation_data=[[X_val_tfidf[0], X_val_tfidf[1]], y_val], epochs=5, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71733 samples, validate on 30744 samples\n",
      "Epoch 1/10\n",
      "71733/71733 [==============================] - 22s 300us/step - loss: 0.4334 - acc: 0.8296 - recall: 0.1380 - precision: 0.5964 - f1: 0.2129 - val_loss: 0.4632 - val_acc: 0.8165 - val_recall: 0.0630 - val_precision: 0.3530 - val_f1: 0.1019\n",
      "Epoch 2/10\n",
      "71733/71733 [==============================] - 22s 300us/step - loss: 0.4192 - acc: 0.8365 - recall: 0.1895 - precision: 0.6879 - f1: 0.2833 - val_loss: 0.4750 - val_acc: 0.8095 - val_recall: 0.0961 - val_precision: 0.3547 - val_f1: 0.1447\n",
      "Epoch 3/10\n",
      "71733/71733 [==============================] - 21s 300us/step - loss: 0.4044 - acc: 0.8441 - recall: 0.2385 - precision: 0.7198 - f1: 0.3431 - val_loss: 0.4770 - val_acc: 0.8104 - val_recall: 0.0862 - val_precision: 0.3533 - val_f1: 0.1322\n",
      "Epoch 4/10\n",
      "71733/71733 [==============================] - 22s 300us/step - loss: 0.3901 - acc: 0.8500 - recall: 0.2782 - precision: 0.7410 - f1: 0.3880 - val_loss: 0.4841 - val_acc: 0.8058 - val_recall: 0.1059 - val_precision: 0.3546 - val_f1: 0.1557\n",
      "Epoch 5/10\n",
      "71733/71733 [==============================] - 21s 299us/step - loss: 0.3770 - acc: 0.8565 - recall: 0.3201 - precision: 0.7560 - f1: 0.4329 - val_loss: 0.4975 - val_acc: 0.7966 - val_recall: 0.1365 - val_precision: 0.3288 - val_f1: 0.1850\n",
      "Epoch 6/10\n",
      "71733/71733 [==============================] - 21s 299us/step - loss: 0.3639 - acc: 0.8622 - recall: 0.3512 - precision: 0.7726 - f1: 0.4670 - val_loss: 0.4996 - val_acc: 0.7962 - val_recall: 0.1375 - val_precision: 0.3251 - val_f1: 0.1848\n",
      "Epoch 7/10\n",
      "71733/71733 [==============================] - 22s 300us/step - loss: 0.3489 - acc: 0.8681 - recall: 0.3918 - precision: 0.7840 - f1: 0.5055 - val_loss: 0.5271 - val_acc: 0.7974 - val_recall: 0.1296 - val_precision: 0.3246 - val_f1: 0.1769\n",
      "Epoch 8/10\n",
      "71733/71733 [==============================] - 22s 300us/step - loss: 0.3370 - acc: 0.8734 - recall: 0.4205 - precision: 0.7950 - f1: 0.5334 - val_loss: 0.5485 - val_acc: 0.7801 - val_recall: 0.1947 - val_precision: 0.3129 - val_f1: 0.2310\n",
      "Epoch 9/10\n",
      "71733/71733 [==============================] - 21s 300us/step - loss: 0.3248 - acc: 0.8794 - recall: 0.4495 - precision: 0.8120 - f1: 0.5618 - val_loss: 0.5447 - val_acc: 0.7853 - val_recall: 0.1697 - val_precision: 0.3151 - val_f1: 0.2112\n",
      "Epoch 10/10\n",
      "71733/71733 [==============================] - 22s 300us/step - loss: 0.3141 - acc: 0.8837 - recall: 0.4770 - precision: 0.8130 - f1: 0.5857 - val_loss: 0.5806 - val_acc: 0.7683 - val_recall: 0.2051 - val_precision: 0.2911 - val_f1: 0.2320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d2f3c5390>"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train_tfidf[0], X_train_tfidf[1]], y_train, validation_data=[[X_val_tfidf[0], X_val_tfidf[1]], y_val], epochs=5, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "(71733, 2021)\n",
      "((71733, 2021), (71733, 2021))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 2021)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2021)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          1100928     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          1100928     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           2080        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,220,417\n",
      "Trainable params: 2,220,417\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.6529 - acc: 0.6170 - recall: 0.6032 - precision: 0.6239 - f1: 0.6091 - val_loss: 0.6369 - val_acc: 0.6486 - val_recall: 0.5853 - val_precision: 0.2757 - val_f1: 0.3677\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.6108 - acc: 0.6727 - recall: 0.6584 - precision: 0.6812 - f1: 0.6663 - val_loss: 0.6122 - val_acc: 0.6829 - val_recall: 0.5724 - val_precision: 0.2992 - val_f1: 0.3858\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.5787 - acc: 0.7057 - recall: 0.7042 - precision: 0.7097 - f1: 0.7039 - val_loss: 0.6225 - val_acc: 0.6831 - val_recall: 0.5856 - val_precision: 0.3027 - val_f1: 0.3917\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.5433 - acc: 0.7366 - recall: 0.7396 - precision: 0.7385 - f1: 0.7364 - val_loss: 0.6424 - val_acc: 0.6705 - val_recall: 0.6242 - val_precision: 0.2994 - val_f1: 0.3974\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.5072 - acc: 0.7623 - recall: 0.7691 - precision: 0.7622 - f1: 0.7632 - val_loss: 0.6377 - val_acc: 0.6822 - val_recall: 0.5974 - val_precision: 0.3042 - val_f1: 0.3957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc5570af50>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            \n",
    "print(y_train.shape)\n",
    "print(X_train_tfidf[1].shape)\n",
    "print(X_train_tfidf[0].shape, X_train_tfidf[1].shape)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_tfidf(X_train_tfidf, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_tfidf[0], X_val_tfidf[1]], y_val]\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.4699 - acc: 0.7883 - recall: 0.8039 - precision: 0.7828 - f1: 0.7909 - val_loss: 0.5434 - val_acc: 0.7417 - val_recall: 0.5109 - val_precision: 0.3490 - val_f1: 0.4059\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.4336 - acc: 0.8111 - recall: 0.8305 - precision: 0.8032 - f1: 0.8143 - val_loss: 0.6132 - val_acc: 0.6997 - val_recall: 0.5878 - val_precision: 0.3179 - val_f1: 0.4049\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.4096 - acc: 0.8237 - recall: 0.8429 - precision: 0.8147 - f1: 0.8267 - val_loss: 0.6154 - val_acc: 0.7104 - val_recall: 0.5646 - val_precision: 0.3234 - val_f1: 0.4037\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.3807 - acc: 0.8388 - recall: 0.8586 - precision: 0.8294 - f1: 0.8418 - val_loss: 0.5920 - val_acc: 0.7274 - val_recall: 0.5302 - val_precision: 0.3358 - val_f1: 0.4027\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.3578 - acc: 0.8526 - recall: 0.8741 - precision: 0.8412 - f1: 0.8556 - val_loss: 0.6565 - val_acc: 0.7110 - val_recall: 0.5623 - val_precision: 0.3246 - val_f1: 0.4033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc4d0e9d10>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_batches_tfidf(X_train_tfidf, y_train), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_tfidf[0], X_val_tfidf[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accuracy   recall  precision  specificity        f1      tp      fn      fp       tn\n",
      "val  0.711001  0.55877   0.324069     0.744388  0.410222  3090.0  2440.0  6445.0  18769.0\n",
      "       accuracy   recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train  0.838066  0.90802   0.534428     0.822357  0.672844  11945.0  1210.0  10406.0  48172.0\n",
      "     accuracy    recall  precision  specificity        f1      tp      fn      fp       tn\n",
      "val   0.71048  0.549186   0.321546     0.745855  0.405609  3037.0  2493.0  6408.0  18806.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train  0.836365  0.907944    0.53153     0.820291  0.670522  11944.0  1211.0  10527.0  48051.0\n"
     ]
    }
   ],
   "source": [
    "# # Test score...\n",
    "# y_val_pred = model.predict([X_val_tfidf[0], X_val_tfidf[1]])\n",
    "# # val_class = np.int(val_predictions>=0.5)\n",
    "# print(val_predictions.shape)\n",
    "\n",
    "predictor = ClassificationPredictor(model)\n",
    "print(predictor.evaluate([X_val_tfidf[0], X_val_tfidf[1]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_tfidf[0], X_train_tfidf[1]], y_train, title='train'))\n",
    "\n",
    "print(predictor.evaluate([X_val_tfidf[1], X_val_tfidf[0]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_tfidf[1], X_train_tfidf[0]], y_train, title='train'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------row 0------------------------\n",
      "sent1: 我的花呗还款到底是怎么回事 \n",
      "sent2: 花呗还款***是什么意思 \n",
      "y = 0, y_pred = 0, a_pred= [0.28875333]\n",
      "tfidf1: 么=0.14807314, 事=0.44262431, 到=0.28308761, 呗=0.07789550, 回=0.38072136, 底=0.57279022, 怎=0.19585989, 我=0.17455366, 是=0.22765745, 款=0.17707171, 的=0.18065477, 花=0.10351239, 还=0.16598502\n",
      "tfidf2: 么=0.19450228, 什=0.27536231, 呗=0.10232005, 思=0.58232405, 意=0.57203474, 是=0.29904069, 款=0.23259351, 花=0.13596927, 还=0.21803052\n",
      "------------------------row 2200------------------------\n",
      "sent1: 我不差花呗钱怎么会扣钱了 \n",
      "sent2: 花呗都扣钱啦 \n",
      "y = 0, y_pred = 0, a_pred= [0.21557876]\n",
      "tfidf1: 不=0.18674019, 么=0.14616855, 了=0.18500349, 会=0.31458552, 呗=0.07689357, 差=0.59332336, 怎=0.19334065, 我=0.17230847, 扣=0.32585325, 花=0.10218097, 钱=0.51858960\n",
      "tfidf2: 呗=0.09663953, 啦=0.69354256, 扣=0.40953108, 花=0.12842064, 都=0.46824538, 钱=0.32588069\n",
      "------------------------row 4400------------------------\n",
      "sent1: 我还清了花呗还扣我的钱 \n",
      "sent2: 我换了花呗为什么有扣我的钱 \n",
      "y = 0, y_pred = 0, a_pred= [0.07963774]\n",
      "tfidf1: 了=0.23648157, 呗=0.09828956, 我=0.44050821, 扣=0.41652343, 清=0.46101474, 的=0.22795256, 花=0.13061330, 还=0.41888415, 钱=0.33144479\n",
      "tfidf2: 为=0.27303876, 么=0.17772118, 了=0.22493921, 什=0.25160484, 呗=0.09349217, 我=0.41900757, 扣=0.39619346, 换=0.44754520, 有=0.27666929, 的=0.21682649, 花=0.12423823, 钱=0.31526739\n",
      "------------------------row 6600------------------------\n",
      "sent1: 花呗分期！这个月分期了下个月还能分期么 \n",
      "sent2: 分期后的花呗每个月 \n",
      "y = 0, y_pred = 0, a_pred= [0.11164144]\n",
      "tfidf1: 下=0.25083091, 个=0.38427676, 么=0.10346644, 了=0.13095603, 分=0.52003747, 呗=0.05442965, 月=0.41853275, 期=0.46499487, 能=0.15525872, 花=0.07232952, 还=0.11598241, 这=0.23233650\n",
      "tfidf2: 个=0.35950521, 分=0.32434293, 后=0.39959632, 呗=0.10184194, 月=0.39155296, 期=0.29001334, 每=0.53290305, 的=0.23619121, 花=0.13533391\n",
      "------------------------row 8800------------------------\n",
      "sent1: 支付宝花呗不在理赔范围内 \n",
      "sent2: 花呗在保障范围内吗 \n",
      "y = 0, y_pred = 1, a_pred= [0.63092685]\n",
      "tfidf1: 不=0.11632591, 付=0.14537242, 内=0.36414064, 呗=0.04789924, 围=0.45036843, 在=0.16729674, 宝=0.17190111, 支=0.16227344, 理=0.32414798, 花=0.06365150, 范=0.45208499, 赔=0.47982866\n",
      "tfidf2: 保=0.38937002, 内=0.37446739, 吗=0.12546165, 呗=0.04925763, 围=0.46314054, 在=0.17204114, 花=0.06545661, 范=0.46490578, 障=0.47484625\n",
      "------------------------row 11000------------------------\n",
      "sent1: 蚂蚁借呗关联账户是什么意思 \n",
      "sent2: 蚂蚁借呗是怎么回事儿 \n",
      "y = 0, y_pred = 0, a_pred= [0.03356009]\n",
      "tfidf1: 么=0.12905909, 什=0.18271257, 借=0.14648926, 关=0.29323296, 呗=0.06789295, 思=0.38639246, 意=0.37956514, 户=0.34909528, 是=0.19842400, 联=0.48600336, 蚁=0.19976269, 蚂=0.19976949, 账=0.25939365\n",
      "tfidf2: 么=0.15134134, 事=0.45239372, 借=0.17178085, 儿=0.61741783, 呗=0.07961477, 回=0.38912447, 怎=0.20018282, 是=0.23268221, 蚁=0.23425203, 蚂=0.23425999\n",
      "------------------------row 13200------------------------\n",
      "sent1: 借呗不能还款 \n",
      "sent2: 借呗在哪里还款 \n",
      "y = 0, y_pred = 0, a_pred= [0.34566316]\n",
      "tfidf1: 不=0.44790877, 借=0.39794485, 呗=0.18443434, 款=0.41925536, 能=0.52609261, 还=0.39300522\n",
      "tfidf2: 借=0.26225818, 呗=0.12154803, 哪=0.57961506, 在=0.42452839, 款=0.27630248, 还=0.25900281, 里=0.50679704\n",
      "------------------------row 15400------------------------\n",
      "sent1: 我的花呗怎么还款 \n",
      "sent2: 我就是，我花呗我都进不去，我怎么换 \n",
      "y = 0, y_pred = 0, a_pred= [0.02500038]\n",
      "tfidf1: 么=0.33186593, 呗=0.17458170, 怎=0.43896701, 我=0.39121486, 款=0.39685840, 的=0.40488885, 花=0.23199506, 还=0.37201056\n",
      "tfidf2: 不=0.15701570, 么=0.12290208, 去=0.34240792, 呗=0.06465398, 就=0.30597858, 怎=0.16256552, 我=0.57952463, 换=0.30949735, 是=0.18895780, 花=0.08591625, 进=0.38197830, 都=0.31326651\n",
      "------------------------row 17600------------------------\n",
      "sent1: 已经实名认证为什么开通不了借呗 \n",
      "sent2: 为什么借呗会没有额度了 \n",
      "y = 0, y_pred = 0, a_pred= [0.48721293]\n",
      "tfidf1: 不=0.15431517, 为=0.18557091, 么=0.12078827, 了=0.15288002, 什=0.17100334, 借=0.13710142, 名=0.43016138, 呗=0.06354199, 实=0.40044303, 已=0.28042679, 开=0.20762386, 经=0.29959164, 认=0.38485685, 证=0.31804823, 通=0.21341677\n",
      "tfidf2: 为=0.31678961, 么=0.20619865, 了=0.26098274, 什=0.29192119, 会=0.44378293, 借=0.23404695, 呗=0.10847306, 度=0.34537824, 有=0.32100188, 没=0.35260237, 额=0.30602623\n",
      "------------------------row 19800------------------------\n",
      "sent1: 蚂蚁借呗的额度多少 \n",
      "sent2: 蚂蚁借呗上期借款是多少 \n",
      "y = 0, y_pred = 0, a_pred= [0.10473751]\n",
      "tfidf1: 借=0.24373816, 呗=0.11296461, 多=0.42567728, 少=0.47549681, 度=0.35967936, 的=0.26198686, 蚁=0.33237789, 蚂=0.33238920, 额=0.31869789\n",
      "tfidf2: 上=0.41554496, 借=0.41220230, 呗=0.09552110, 多=0.35994602, 少=0.40207262, 是=0.27917006, 期=0.27201361, 款=0.21713816, 蚁=0.28105352, 蚂=0.28106308\n",
      "------------------------row 22000------------------------\n",
      "sent1: 借呗借钱不能分十二期吗 \n",
      "sent2: 借呗不分期还款 \n",
      "y = 0, y_pred = 0, a_pred= [0.21335466]\n",
      "tfidf1: 不=0.20805153, 二=0.48208885, 借=0.36968704, 分=0.27283553, 十=0.50552878, 吗=0.21820295, 呗=0.08566889, 期=0.24395766, 能=0.24436756, 钱=0.28888629\n",
      "tfidf2: 不=0.38634535, 借=0.34324878, 分=0.50664725, 呗=0.15908451, 期=0.45302192, 款=0.36163024, 还=0.33898809\n",
      "------------------------row 24200------------------------\n",
      "sent1: 我的花呗逾期了 \n",
      "sent2: 我的蚂蚁花呗信用情况如何，有逾期的情况吗 \n",
      "y = 0, y_pred = 0, a_pred= [0.4358587]\n",
      "tfidf1: 了=0.35454369, 呗=0.14736008, 我=0.33021475, 期=0.41963449, 的=0.34175663, 花=0.19582127, 逾=0.64211006\n",
      "tfidf2: 何=0.20144269, 信=0.18001891, 况=0.61483802, 吗=0.11283869, 呗=0.04430171, 如=0.19517074, 情=0.56998196, 我=0.09927437, 有=0.13110107, 期=0.12615715, 用=0.10688280, 的=0.20548855, 花=0.05887088, 蚁=0.13034976, 蚂=0.13035420, 逾=0.19304126\n",
      "------------------------row 26400------------------------\n",
      "sent1: 店家店里显示支持花呗可是支付的时候显示店家不支持 \n",
      "sent2: 什么样的店家支持花呗 \n",
      "y = 0, y_pred = 0, a_pred= [0.12409858]\n",
      "tfidf1: 不=0.08392181, 付=0.10487705, 候=0.15743057, 可=0.09760574, 呗=0.03455629, 家=0.34353222, 店=0.58692342, 持=0.35941667, 支=0.35121020, 时=0.12664770, 是=0.10099424, 显=0.30382141, 的=0.08014274, 示=0.28912222, 花=0.04592055, 里=0.14408316\n",
      "tfidf2: 么=0.16339731, 什=0.23132614, 呗=0.08595694, 家=0.42725915, 店=0.48664702, 持=0.44701501, 支=0.29120562, 样=0.39447942, 的=0.19935083, 花=0.11422495\n",
      "------------------------row 28600------------------------\n",
      "sent1: 如何兑换花呗分期免息 \n",
      "sent2: 如何兑换蚂蚁花呗免息卷 \n",
      "y = 1, y_pred = 1, a_pred= [0.89383984]\n",
      "tfidf1: 何=0.33642402, 免=0.42192400, 兑=0.49787401, 分=0.23563176, 呗=0.07398710, 如=0.32594941, 息=0.33791081, 换=0.35417481, 期=0.21069166, 花=0.09831868\n",
      "tfidf2: 何=0.29313868, 免=0.36763797, 兑=0.43381602, 卷=0.49463725, 呗=0.06446770, 如=0.28401177, 息=0.29443418, 换=0.30860560, 花=0.08566870, 蚁=0.18968451, 蚂=0.18969097\n"
     ]
    }
   ],
   "source": [
    "show_sentence(X_val, y_val,  X_val_tfidf, words, items=np.arange(0, len(X_val), 2200), \n",
    "              y_pred=predictor.predict([X_val_tfidf[0], X_val_tfidf[1]]), a_pred=model.predict([X_val_tfidf[0], X_val_tfidf[1]])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "loaded_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "\n",
    "# predictor = ClassificationPredictor(loaded_model)\n",
    "# print(predictor.evaluate([X_val_seq[0], X_val_seq[1]], y_val, title='val'))\n",
    "# print(predictor.evaluate([X_train_seq[0], X_train_seq[1]], y_train, title='train'))\n",
    "# model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Word Embedding + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(y_index, batch_size):\n",
    "    indices = np.random.permutation(np.arange(len(y_index)))\n",
    "    return list(y_index[indices[0:batch_size]] )\n",
    "\n",
    "#尝试随机的得到label比例是1：1的batch数据， 不知道这样是覅偶可以有做帮助数据倾斜的问题。\n",
    "#看来这招还挺有用，val f1很快从0.25提高到0.36\n",
    "def generate_batches_seq(X, y, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True):  \n",
    "    while True:\n",
    "        y_index_1 = np.array([i for i,label in enumerate(y) if label==1])\n",
    "        y_index_0 = np.array([i for i,label in enumerate(y) if label==0])\n",
    "\n",
    "        y_index_1_batch_size = int(batch_size*positive_ratio)\n",
    "        y_index_0_batch_size = batch_size - y_index_1_batch_size\n",
    "    \n",
    "        \n",
    "        for start in range(0,len(y)-1,batch_size):\n",
    "            index = np.random.permutation(np.arange(2)) if random_sent else np.arange(2)\n",
    "            batch_indices = get_indices(y_index_1, y_index_1_batch_size) + get_indices(y_index_0, y_index_0_batch_size)\n",
    "            yield [X[index[0]][batch_indices], X[index[1]][batch_indices]], y[batch_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate, dot\n",
    "\n",
    "def get_model():\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent1.add(L.SimpleRNN(128,return_sequences=False))\n",
    "#     model_sent1.add(L.Dense(64))\n",
    "#     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent2.add(L.SimpleRNN(128,return_sequences=False))\n",
    "#     model_sent2.add(L.Dense(64))\n",
    "#     model_sent2.add(L.Activation('elu')) \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    X = L.concatenate([encoder1, encoder2])\n",
    "    X = L.Dense(64)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dense(32)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "\n",
    "    predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model_gru():\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent1.add(L.GRU(128, return_sequences=False))\n",
    "#     model_sent1.add(L.CuDNNGRU(128,return_sequences=False))\n",
    "#     model_sent1.add(L.Dense(64))\n",
    "#     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent2.add(L.GRU(128, return_sequences=False))\n",
    "#     model_sent2.add(L.CuDNNGRU(128,return_sequences=False))\n",
    "#     model_sent2.add(L.Dense(64))\n",
    "#     model_sent2.add(L.Activation('elu')) \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    X = L.concatenate([encoder1, encoder2])\n",
    "    X = L.Dense(64)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dense(32)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "\n",
    "    predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model_gru_cosine():\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), 64))\n",
    "    model_sent1.add(L.GRU(64, return_sequences=False))                    \n",
    "#     model_sent1.add(L.CuDNNGRU(64,return_sequences=False))\n",
    "#     model_sent1.add(L.Dense(64))\n",
    "#     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), 64))\n",
    "    model_sent2.add(L.GRU(64, return_sequences=False))               \n",
    "#     model_sent2.add(L.CuDNNGRU(64,return_sequences=False))\n",
    "#     model_sent2.add(L.Dense(64))\n",
    "#     model_sent2.add(L.Activation('elu')) \n",
    "    \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    predictions = dot([encoder1, encoder2], axes = 1, normalize = True) \n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model_lstm():\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent1.add(L.Bidirectional(L.CuDNNLSTM(128,return_sequences=False)))\n",
    "#     model_sent1.add(L.Dense(64))\n",
    "#     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent2.add(L.Bidirectional(L.CuDNNLSTM(128,return_sequences=False)))\n",
    "#     model_sent2.add(L.Dense(64))\n",
    "#     model_sent2.add(L.Activation('elu')) \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    X = L.concatenate([encoder1, encoder2])\n",
    "    X = L.Dense(128)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dense(32)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "\n",
    "    predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# def get_model_gru_cosine():\n",
    "#     K.clear_session()\n",
    "\n",
    "#     model_sent1 = Sequential()\n",
    "#     model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "#     model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "#     model_sent1.add(L.CuDNNGRU(128,return_sequences=False))\n",
    "#     model_sent1.add(L.Dense(64))\n",
    "#     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "#     model_sent2 = Sequential()\n",
    "#     model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "#     model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "#     model_sent2.add(L.CuDNNGRU(128,return_sequences=False))\n",
    "#     model_sent2.add(L.Dense(64))\n",
    "#     model_sent2.add(L.Activation('elu')) \n",
    "    \n",
    "\n",
    "#     input1 = L.Input(shape=(None, ))\n",
    "#     input2 = L.Input(shape=(None, ))\n",
    "\n",
    "#     encoder1 = model_sent1(input1)\n",
    "#     encoder2 = model_sent2(input2)\n",
    "\n",
    "#     predictions = dot([encoder1, encoder2], axes = 1, normalize = True) \n",
    "\n",
    "#     model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "#     model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "def get_model_lstm_cosine(embedding_size=64):\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), embedding_size))\n",
    "    model_sent1.add(L.CuDNNLSTM(embedding_size,return_sequences=False))\n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), embedding_size))\n",
    "    model_sent2.add(L.CuDNNLSTM(embedding_size,return_sequences=False))\n",
    " \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    predictions = dot([encoder1, encoder2], axes = 1, normalize = True) \n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "2025\n",
      "(71733, 25)\n",
      "((71733, 25), (71733, 25))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          292096      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          292096      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 602,753\n",
      "Trainable params: 602,753\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 32s 29ms/step - loss: 0.6922 - acc: 0.5351 - recall: 0.5147 - precision: 0.5421 - f1: 0.5073 - val_loss: 0.6885 - val_acc: 0.5470 - val_recall: 0.5125 - val_precision: 0.2013 - val_f1: 0.2845\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 31s 28ms/step - loss: 0.6929 - acc: 0.5243 - recall: 0.4977 - precision: 0.5286 - f1: 0.4906 - val_loss: 0.6698 - val_acc: 0.6465 - val_recall: 0.4559 - val_precision: 0.2431 - val_f1: 0.3101\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 32s 28ms/step - loss: 0.6927 - acc: 0.5188 - recall: 0.5905 - precision: 0.4983 - f1: 0.5114 - val_loss: 0.6869 - val_acc: 0.3824 - val_recall: 0.7781 - val_precision: 0.1947 - val_f1: 0.3077\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 31s 28ms/step - loss: 0.6894 - acc: 0.5393 - recall: 0.5525 - precision: 0.5408 - f1: 0.5227 - val_loss: 0.6935 - val_acc: 0.5286 - val_recall: 0.6025 - val_precision: 0.2126 - val_f1: 0.3092\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 31s 28ms/step - loss: 0.6857 - acc: 0.5582 - recall: 0.5100 - precision: 0.5717 - f1: 0.5243 - val_loss: 0.6683 - val_acc: 0.6361 - val_recall: 0.4703 - val_precision: 0.2393 - val_f1: 0.3101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea8e699890>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            \n",
    "print(y_train.shape)\n",
    "print(len(vocab))\n",
    "print(X_train_seq[1].shape)\n",
    "print(X_train_seq[0].shape, X_train_seq[1].shape)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1120/1120 [==============================] - 11s 10ms/step - loss: 0.4152 - acc: 0.8283 - recall: 0.8550 - precision: 0.8162 - f1: 0.8333 - val_loss: 0.6307 - val_acc: 0.7388 - val_recall: 0.6126 - val_precision: 0.3661 - val_f1: 0.4502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc475e8f10>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accuracy    recall  precision  specificity        f1      tp      fn      fp       tn\n",
      "val  0.721311  0.669982   0.354613     0.732569  0.463763  3705.0  1825.0  6743.0  18471.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train   0.78512  0.848651   0.454061     0.770853  0.591596  11164.0  1991.0  13423.0  45155.0\n",
      "     accuracy    recall  precision  specificity        f1      tp      fn      fp       tn\n",
      "val  0.717376  0.660036   0.348982     0.729952  0.456564  3650.0  1880.0  6809.0  18405.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train  0.783252  0.846978   0.451514     0.768941  0.589025  11142.0  2013.0  13535.0  45043.0\n"
     ]
    }
   ],
   "source": [
    "predictor = ClassificationPredictor(model)\n",
    "print(predictor.evaluate([X_val_seq[0], X_val_seq[1]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_seq[0], X_train_seq[1]], y_train, title='train'))\n",
    "\n",
    "print(predictor.evaluate([X_val_seq[1], X_val_seq[0]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_seq[1], X_train_seq[0]], y_train, title='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "2025\n",
      "(71733, 25)\n",
      "((71733, 25), (71733, 25))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          358272      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          358272      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 735,105\n",
      "Trainable params: 735,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 16s 14ms/step - loss: 0.6743 - acc: 0.5773 - recall: 0.5449 - precision: 0.5782 - f1: 0.5403 - val_loss: 0.6892 - val_acc: 0.6312 - val_recall: 0.5767 - val_precision: 0.2620 - val_f1: 0.3538\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 13s 12ms/step - loss: 0.6571 - acc: 0.6141 - recall: 0.5447 - precision: 0.6362 - f1: 0.5821 - val_loss: 0.6557 - val_acc: 0.6620 - val_recall: 0.5476 - val_precision: 0.2768 - val_f1: 0.3605\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 13s 12ms/step - loss: 0.6448 - acc: 0.6265 - recall: 0.5545 - precision: 0.6504 - f1: 0.5944 - val_loss: 0.6515 - val_acc: 0.6654 - val_recall: 0.5438 - val_precision: 0.2788 - val_f1: 0.3614\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 13s 12ms/step - loss: 0.6325 - acc: 0.6372 - recall: 0.6020 - precision: 0.6509 - f1: 0.6214 - val_loss: 0.6191 - val_acc: 0.6528 - val_recall: 0.6189 - val_precision: 0.2852 - val_f1: 0.3837\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 13s 12ms/step - loss: 0.6167 - acc: 0.6573 - recall: 0.6759 - precision: 0.6546 - f1: 0.6621 - val_loss: 0.6373 - val_acc: 0.6255 - val_recall: 0.6787 - val_precision: 0.2778 - val_f1: 0.3879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9fb5e34650>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            \n",
    "print(y_train.shape)\n",
    "print(len(vocab))\n",
    "print(X_train_seq[1].shape)\n",
    "print(X_train_seq[0].shape, X_train_seq[1].shape)\n",
    "\n",
    "model = get_model_gru()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "2025\n",
      "(71733, 25)\n",
      "((71733, 25), (71733, 25))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          203712      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          203712      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 407,424\n",
      "Trainable params: 407,424\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "1120/1120 [==============================] - 99s 88ms/step - loss: 0.7187 - acc: 0.5484 - recall: 0.5876 - precision: 0.5371 - f1: 0.5251 - val_loss: 0.6294 - val_acc: 0.7508 - val_recall: 0.2618 - val_precision: 0.2877 - val_f1: 0.2650\n",
      "Epoch 2/15\n",
      "1120/1120 [==============================] - 96s 86ms/step - loss: 0.6783 - acc: 0.5994 - recall: 0.5290 - precision: 0.6246 - f1: 0.5626 - val_loss: 0.6621 - val_acc: 0.6777 - val_recall: 0.5068 - val_precision: 0.2799 - val_f1: 0.3534\n",
      "Epoch 3/15\n",
      "1120/1120 [==============================] - 96s 86ms/step - loss: 0.6612 - acc: 0.6211 - recall: 0.5699 - precision: 0.6379 - f1: 0.5983 - val_loss: 0.6627 - val_acc: 0.6318 - val_recall: 0.5997 - val_precision: 0.2665 - val_f1: 0.3625\n",
      "Epoch 4/15\n",
      "1120/1120 [==============================] - 96s 86ms/step - loss: 0.6502 - acc: 0.6325 - recall: 0.5861 - precision: 0.6489 - f1: 0.6123 - val_loss: 0.6942 - val_acc: 0.5760 - val_recall: 0.7062 - val_precision: 0.2560 - val_f1: 0.3701\n",
      "Epoch 5/15\n",
      "1120/1120 [==============================] - 95s 85ms/step - loss: 0.6261 - acc: 0.6563 - recall: 0.6567 - precision: 0.6592 - f1: 0.6547 - val_loss: 0.6237 - val_acc: 0.6581 - val_recall: 0.6399 - val_precision: 0.2928 - val_f1: 0.3950\n",
      "Epoch 6/15\n",
      "1120/1120 [==============================] - 94s 84ms/step - loss: 0.6105 - acc: 0.6747 - recall: 0.7039 - precision: 0.6677 - f1: 0.6827 - val_loss: 0.5989 - val_acc: 0.6740 - val_recall: 0.6521 - val_precision: 0.3079 - val_f1: 0.4110\n",
      "Epoch 7/15\n",
      "1120/1120 [==============================] - 94s 84ms/step - loss: 0.5946 - acc: 0.6941 - recall: 0.7241 - precision: 0.6857 - f1: 0.7021 - val_loss: 0.6129 - val_acc: 0.6621 - val_recall: 0.6973 - val_precision: 0.3065 - val_f1: 0.4191\n",
      "Epoch 8/15\n",
      "1120/1120 [==============================] - 94s 84ms/step - loss: 0.5826 - acc: 0.7047 - recall: 0.7401 - precision: 0.6941 - f1: 0.7142 - val_loss: 0.6205 - val_acc: 0.6411 - val_recall: 0.7267 - val_precision: 0.2971 - val_f1: 0.4152\n",
      "Epoch 9/15\n",
      "1120/1120 [==============================] - 94s 84ms/step - loss: 0.5688 - acc: 0.7154 - recall: 0.7579 - precision: 0.7013 - f1: 0.7263 - val_loss: 0.6197 - val_acc: 0.6470 - val_recall: 0.7432 - val_precision: 0.3036 - val_f1: 0.4246\n",
      "Epoch 10/15\n",
      "1120/1120 [==============================] - 96s 86ms/step - loss: 0.5590 - acc: 0.7277 - recall: 0.7689 - precision: 0.7132 - f1: 0.7380 - val_loss: 0.5983 - val_acc: 0.6837 - val_recall: 0.6980 - val_precision: 0.3239 - val_f1: 0.4355\n",
      "Epoch 11/15\n",
      "1120/1120 [==============================] - 96s 86ms/step - loss: 0.5460 - acc: 0.7402 - recall: 0.7800 - precision: 0.7255 - f1: 0.7497 - val_loss: 0.5957 - val_acc: 0.6827 - val_recall: 0.7043 - val_precision: 0.3241 - val_f1: 0.4370\n",
      "Epoch 12/15\n",
      "1120/1120 [==============================] - 95s 85ms/step - loss: 0.5366 - acc: 0.7456 - recall: 0.7840 - precision: 0.7313 - f1: 0.7545 - val_loss: 0.5974 - val_acc: 0.6831 - val_recall: 0.7053 - val_precision: 0.3250 - val_f1: 0.4377\n",
      "Epoch 13/15\n",
      "1120/1120 [==============================] - 94s 84ms/step - loss: 0.5247 - acc: 0.7563 - recall: 0.7941 - precision: 0.7411 - f1: 0.7648 - val_loss: 0.5823 - val_acc: 0.7090 - val_recall: 0.6709 - val_precision: 0.3429 - val_f1: 0.4461\n",
      "Epoch 14/15\n",
      "1120/1120 [==============================] - 94s 84ms/step - loss: 0.5127 - acc: 0.7633 - recall: 0.7938 - precision: 0.7512 - f1: 0.7698 - val_loss: 0.5992 - val_acc: 0.6967 - val_recall: 0.6947 - val_precision: 0.3355 - val_f1: 0.4449\n",
      "Epoch 15/15\n",
      "1120/1120 [==============================] - 94s 84ms/step - loss: 0.5008 - acc: 0.7694 - recall: 0.7999 - precision: 0.7569 - f1: 0.7760 - val_loss: 0.5874 - val_acc: 0.7213 - val_recall: 0.6649 - val_precision: 0.3534 - val_f1: 0.4539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9eb842d310>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(len(vocab))\n",
    "print(X_train_seq[1].shape)\n",
    "print(X_train_seq[0].shape, X_train_seq[1].shape)\n",
    "\n",
    "model = get_model_gru_cosine()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=15,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 64)           154368      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 64)           154368      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 308,736\n",
      "Trainable params: 308,736\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.7046 - acc: 0.5086 - recall: 0.5631 - precision: 0.5073 - f1: 0.5291 - val_loss: 0.7334 - val_acc: 0.2847 - val_recall: 0.8983 - val_precision: 0.1879 - val_f1: 0.3076\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.7116 - acc: 0.5742 - recall: 0.5908 - precision: 0.5750 - f1: 0.5792 - val_loss: 0.6888 - val_acc: 0.5922 - val_recall: 0.6161 - val_precision: 0.2457 - val_f1: 0.3454\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.6631 - acc: 0.6047 - recall: 0.5544 - precision: 0.6183 - f1: 0.5815 - val_loss: 0.6784 - val_acc: 0.6176 - val_recall: 0.6093 - val_precision: 0.2598 - val_f1: 0.3583\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.6608 - acc: 0.6159 - recall: 0.5605 - precision: 0.6328 - f1: 0.5913 - val_loss: 0.6931 - val_acc: 0.5987 - val_recall: 0.6493 - val_precision: 0.2563 - val_f1: 0.3618\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.6495 - acc: 0.6256 - recall: 0.5802 - precision: 0.6406 - f1: 0.6059 - val_loss: 0.6694 - val_acc: 0.6358 - val_recall: 0.6191 - val_precision: 0.2730 - val_f1: 0.3726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea601ebb10>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_gru_cosine_dropout():\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), 64))\n",
    "    model_sent1.add(L.GRU(64,  dropout=0.3, recurrent_dropout=0.3, return_sequences=False))                    \n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), 64))\n",
    "    model_sent2.add(L.GRU(64,  dropout=0.3, recurrent_dropout=0.3, return_sequences=False))               \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    predictions = dot([encoder1, encoder2], axes = 1, normalize = True) \n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model_gru_cosine_dropout()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.6412 - acc: 0.6350 - recall: 0.6063 - precision: 0.6461 - f1: 0.6225 - val_loss: 0.6764 - val_acc: 0.6267 - val_recall: 0.6459 - val_precision: 0.2726 - val_f1: 0.3772\n",
      "Epoch 2/20\n",
      "1120/1120 [==============================] - 114s 102ms/step - loss: 0.6341 - acc: 0.6442 - recall: 0.6271 - precision: 0.6516 - f1: 0.6363 - val_loss: 0.6774 - val_acc: 0.5993 - val_recall: 0.7062 - val_precision: 0.2672 - val_f1: 0.3818\n",
      "Epoch 3/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.6293 - acc: 0.6537 - recall: 0.6588 - precision: 0.6549 - f1: 0.6542 - val_loss: 0.6727 - val_acc: 0.5979 - val_recall: 0.7228 - val_precision: 0.2693 - val_f1: 0.3866\n",
      "Epoch 4/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.6202 - acc: 0.6631 - recall: 0.6806 - precision: 0.6602 - f1: 0.6677 - val_loss: 0.6563 - val_acc: 0.6228 - val_recall: 0.7045 - val_precision: 0.2802 - val_f1: 0.3948\n",
      "Epoch 5/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.6176 - acc: 0.6701 - recall: 0.6981 - precision: 0.6633 - f1: 0.6779 - val_loss: 0.6393 - val_acc: 0.6331 - val_recall: 0.7058 - val_precision: 0.2869 - val_f1: 0.4018\n",
      "Epoch 6/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.6093 - acc: 0.6755 - recall: 0.7102 - precision: 0.6665 - f1: 0.6853 - val_loss: 0.6697 - val_acc: 0.6050 - val_recall: 0.7456 - val_precision: 0.2770 - val_f1: 0.3981\n",
      "Epoch 7/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.6053 - acc: 0.6851 - recall: 0.7215 - precision: 0.6749 - f1: 0.6953 - val_loss: 0.6516 - val_acc: 0.6209 - val_recall: 0.7404 - val_precision: 0.2856 - val_f1: 0.4060\n",
      "Epoch 8/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.6008 - acc: 0.6906 - recall: 0.7330 - precision: 0.6783 - f1: 0.7024 - val_loss: 0.6538 - val_acc: 0.6291 - val_recall: 0.7407 - val_precision: 0.2908 - val_f1: 0.4114\n",
      "Epoch 9/20\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.5951 - acc: 0.6939 - recall: 0.7340 - precision: 0.6818 - f1: 0.7048 - val_loss: 0.6526 - val_acc: 0.6314 - val_recall: 0.7535 - val_precision: 0.2942 - val_f1: 0.4170\n",
      "Epoch 10/20\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.5897 - acc: 0.7008 - recall: 0.7431 - precision: 0.6874 - f1: 0.7122 - val_loss: 0.6507 - val_acc: 0.6289 - val_recall: 0.7612 - val_precision: 0.2942 - val_f1: 0.4183\n",
      "Epoch 11/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5846 - acc: 0.7045 - recall: 0.7482 - precision: 0.6908 - f1: 0.7163 - val_loss: 0.6480 - val_acc: 0.6290 - val_recall: 0.7686 - val_precision: 0.2956 - val_f1: 0.4210\n",
      "Epoch 12/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5854 - acc: 0.7090 - recall: 0.7483 - precision: 0.6965 - f1: 0.7194 - val_loss: 0.6392 - val_acc: 0.6413 - val_recall: 0.7603 - val_precision: 0.3020 - val_f1: 0.4262\n",
      "Epoch 13/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5759 - acc: 0.7151 - recall: 0.7574 - precision: 0.7009 - f1: 0.7261 - val_loss: 0.6463 - val_acc: 0.6472 - val_recall: 0.7530 - val_precision: 0.3045 - val_f1: 0.4275\n",
      "Epoch 14/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5724 - acc: 0.7141 - recall: 0.7574 - precision: 0.7001 - f1: 0.7255 - val_loss: 0.6324 - val_acc: 0.6521 - val_recall: 0.7575 - val_precision: 0.3092 - val_f1: 0.4327\n",
      "Epoch 15/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5719 - acc: 0.7208 - recall: 0.7612 - precision: 0.7066 - f1: 0.7309 - val_loss: 0.6397 - val_acc: 0.6409 - val_recall: 0.7739 - val_precision: 0.3037 - val_f1: 0.4301\n",
      "Epoch 16/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5657 - acc: 0.7228 - recall: 0.7661 - precision: 0.7078 - f1: 0.7337 - val_loss: 0.6510 - val_acc: 0.6394 - val_recall: 0.7840 - val_precision: 0.3045 - val_f1: 0.4325\n",
      "Epoch 17/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5633 - acc: 0.7282 - recall: 0.7709 - precision: 0.7132 - f1: 0.7388 - val_loss: 0.6534 - val_acc: 0.6374 - val_recall: 0.7848 - val_precision: 0.3033 - val_f1: 0.4314\n",
      "Epoch 18/20\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.5594 - acc: 0.7287 - recall: 0.7679 - precision: 0.7150 - f1: 0.7385 - val_loss: 0.6368 - val_acc: 0.6527 - val_recall: 0.7706 - val_precision: 0.3114 - val_f1: 0.4374\n",
      "Epoch 19/20\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.5568 - acc: 0.7315 - recall: 0.7725 - precision: 0.7170 - f1: 0.7417 - val_loss: 0.6319 - val_acc: 0.6558 - val_recall: 0.7731 - val_precision: 0.3138 - val_f1: 0.4402\n",
      "Epoch 20/20\n",
      "1120/1120 [==============================] - 114s 102ms/step - loss: 0.5522 - acc: 0.7355 - recall: 0.7725 - precision: 0.7221 - f1: 0.7446 - val_loss: 0.6421 - val_acc: 0.6464 - val_recall: 0.7856 - val_precision: 0.3094 - val_f1: 0.4380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea549ef610>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=20,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5494 - acc: 0.7382 - recall: 0.7723 - precision: 0.7260 - f1: 0.7463 - val_loss: 0.6438 - val_acc: 0.6551 - val_recall: 0.7784 - val_precision: 0.3142 - val_f1: 0.4414\n",
      "Epoch 2/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5482 - acc: 0.7386 - recall: 0.7717 - precision: 0.7268 - f1: 0.7465 - val_loss: 0.6316 - val_acc: 0.6603 - val_recall: 0.7744 - val_precision: 0.3177 - val_f1: 0.4441\n",
      "Epoch 3/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5452 - acc: 0.7417 - recall: 0.7792 - precision: 0.7280 - f1: 0.7507 - val_loss: 0.6336 - val_acc: 0.6592 - val_recall: 0.7860 - val_precision: 0.3188 - val_f1: 0.4469\n",
      "Epoch 4/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5410 - acc: 0.7422 - recall: 0.7779 - precision: 0.7291 - f1: 0.7506 - val_loss: 0.6436 - val_acc: 0.6503 - val_recall: 0.7884 - val_precision: 0.3124 - val_f1: 0.4412\n",
      "Epoch 5/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5424 - acc: 0.7438 - recall: 0.7807 - precision: 0.7300 - f1: 0.7525 - val_loss: 0.6362 - val_acc: 0.6587 - val_recall: 0.7856 - val_precision: 0.3184 - val_f1: 0.4465\n",
      "Epoch 6/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5379 - acc: 0.7479 - recall: 0.7818 - precision: 0.7353 - f1: 0.7557 - val_loss: 0.6378 - val_acc: 0.6602 - val_recall: 0.7830 - val_precision: 0.3193 - val_f1: 0.4468\n",
      "Epoch 7/20\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.5416 - acc: 0.7466 - recall: 0.7794 - precision: 0.7344 - f1: 0.7542 - val_loss: 0.6278 - val_acc: 0.6658 - val_recall: 0.7828 - val_precision: 0.3237 - val_f1: 0.4510\n",
      "Epoch 8/20\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.5324 - acc: 0.7486 - recall: 0.7852 - precision: 0.7346 - f1: 0.7571 - val_loss: 0.6233 - val_acc: 0.6726 - val_recall: 0.7759 - val_precision: 0.3277 - val_f1: 0.4537\n",
      "Epoch 9/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5294 - acc: 0.7511 - recall: 0.7879 - precision: 0.7371 - f1: 0.7597 - val_loss: 0.6300 - val_acc: 0.6687 - val_recall: 0.7846 - val_precision: 0.3261 - val_f1: 0.4540\n",
      "Epoch 10/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5282 - acc: 0.7531 - recall: 0.7830 - precision: 0.7421 - f1: 0.7599 - val_loss: 0.6269 - val_acc: 0.6765 - val_recall: 0.7713 - val_precision: 0.3299 - val_f1: 0.4554\n",
      "Epoch 11/20\n",
      "1120/1120 [==============================] - 113s 101ms/step - loss: 0.5269 - acc: 0.7541 - recall: 0.7833 - precision: 0.7433 - f1: 0.7607 - val_loss: 0.6300 - val_acc: 0.6656 - val_recall: 0.7870 - val_precision: 0.3240 - val_f1: 0.4524\n",
      "Epoch 12/20\n",
      "1119/1120 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.7543 - recall: 0.7873 - precision: 0.7422 - f1: 0.7620"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE,     \n",
    "    epochs=20,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accuracy    recall  precision  specificity        f1      tp     fn      fp       tn\n",
      "val  0.715262  0.753879   0.364365     0.706648  0.491283  2818.0  920.0  4916.0  11842.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train   0.77263  0.895297   0.439374     0.745279  0.589463  13382.0  1565.0  17075.0  49959.0\n",
      "     accuracy    recall  precision  specificity       f1      tp     fn      fp       tn\n",
      "val  0.717359  0.752006   0.366159     0.709631  0.49251  2811.0  927.0  4866.0  11892.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train  0.772325  0.897371   0.439137     0.744443  0.589699  13413.0  1534.0  17131.0  49903.0\n"
     ]
    }
   ],
   "source": [
    "predictor = ClassificationPredictor(model)\n",
    "print(predictor.evaluate([X_val_seq[0], X_val_seq[1]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_seq[0], X_train_seq[1]], y_train, title='train'))\n",
    "\n",
    "print(predictor.evaluate([X_val_seq[1], X_val_seq[0]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_seq[1], X_train_seq[0]], y_train, title='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_gru_cnn(kernel_size=3):\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), 64))\n",
    "    model_sent1.add(L.Convolution1D(64, kernel_size=kernel_size, padding = \"same\", activation = \"elu\"))\n",
    "    model_sent1.add(L.GRU(64,  dropout=0.1, recurrent_dropout=0.1, return_sequences=False))        \n",
    "   \n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), 64))\n",
    "    model_sent2.add(L.Convolution1D(64, kernel_size=kernel_size, padding = \"same\", activation = \"elu\"))\n",
    "    model_sent2.add(L.GRU(64,  dropout=0.1, recurrent_dropout=0.1, return_sequences=False))        \n",
    "  \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    predictions = dot([encoder1, encoder2], axes = 1, normalize = True) \n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "2025\n",
      "(71733, 25)\n",
      "((71733, 25), (71733, 25))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 64)           166720      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 64)           166720      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 333,440\n",
      "Trainable params: 333,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 116s 103ms/step - loss: 0.7140 - acc: 0.5425 - recall: 0.5826 - precision: 0.5398 - f1: 0.5425 - val_loss: 0.6809 - val_acc: 0.6049 - val_recall: 0.5662 - val_precision: 0.2433 - val_f1: 0.3343\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.6637 - acc: 0.6071 - recall: 0.5559 - precision: 0.6223 - f1: 0.5828 - val_loss: 0.6526 - val_acc: 0.6561 - val_recall: 0.5495 - val_precision: 0.2734 - val_f1: 0.3577\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.6467 - acc: 0.6297 - recall: 0.5871 - precision: 0.6442 - f1: 0.6110 - val_loss: 0.6657 - val_acc: 0.6031 - val_recall: 0.6696 - val_precision: 0.2630 - val_f1: 0.3718\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.6349 - acc: 0.6486 - recall: 0.6501 - precision: 0.6504 - f1: 0.6472 - val_loss: 0.6314 - val_acc: 0.6466 - val_recall: 0.6443 - val_precision: 0.2847 - val_f1: 0.3887\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 115s 103ms/step - loss: 0.6179 - acc: 0.6658 - recall: 0.6885 - precision: 0.6610 - f1: 0.6719 - val_loss: 0.6467 - val_acc: 0.6132 - val_recall: 0.7123 - val_precision: 0.2758 - val_f1: 0.3916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea623f2f50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE1  = 64\n",
    "print(y_train.shape)\n",
    "print(len(vocab))\n",
    "print(X_train_seq[1].shape)\n",
    "print(X_train_seq[0].shape, X_train_seq[1].shape)\n",
    "\n",
    "model = get_model_gru_cnn(kernel_size=3)\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, batch_size=BATCH_SIZE1, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE1, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 116s 104ms/step - loss: 0.4112 - acc: 0.8257 - recall: 0.8585 - precision: 0.8129 - f1: 0.8334 - val_loss: 0.6643 - val_acc: 0.7090 - val_recall: 0.6801 - val_precision: 0.3494 - val_f1: 0.4536\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 117s 104ms/step - loss: 0.4096 - acc: 0.8274 - recall: 0.8592 - precision: 0.8154 - f1: 0.8351 - val_loss: 0.6676 - val_acc: 0.7166 - val_recall: 0.6687 - val_precision: 0.3567 - val_f1: 0.4572\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 116s 103ms/step - loss: 0.4101 - acc: 0.8265 - recall: 0.8600 - precision: 0.8133 - f1: 0.8344 - val_loss: 0.6637 - val_acc: 0.7163 - val_recall: 0.6580 - val_precision: 0.3535 - val_f1: 0.4516\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 114s 102ms/step - loss: 0.4070 - acc: 0.8276 - recall: 0.8600 - precision: 0.8150 - f1: 0.8352 - val_loss: 0.6632 - val_acc: 0.7180 - val_recall: 0.6635 - val_precision: 0.3557 - val_f1: 0.4548\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 115s 102ms/step - loss: 0.4051 - acc: 0.8284 - recall: 0.8611 - precision: 0.8159 - f1: 0.8362 - val_loss: 0.6715 - val_acc: 0.7130 - val_recall: 0.6637 - val_precision: 0.3508 - val_f1: 0.4509\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 115s 102ms/step - loss: 0.4029 - acc: 0.8300 - recall: 0.8642 - precision: 0.8170 - f1: 0.8383 - val_loss: 0.6660 - val_acc: 0.7173 - val_recall: 0.6566 - val_precision: 0.3549 - val_f1: 0.4523\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 115s 102ms/step - loss: 0.4010 - acc: 0.8314 - recall: 0.8621 - precision: 0.8201 - f1: 0.8388 - val_loss: 0.6717 - val_acc: 0.7141 - val_recall: 0.6655 - val_precision: 0.3515 - val_f1: 0.4524\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 115s 102ms/step - loss: 0.4008 - acc: 0.8309 - recall: 0.8629 - precision: 0.8188 - f1: 0.8387 - val_loss: 0.6766 - val_acc: 0.7144 - val_recall: 0.6583 - val_precision: 0.3521 - val_f1: 0.4509\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 115s 102ms/step - loss: 0.4006 - acc: 0.8308 - recall: 0.8647 - precision: 0.8176 - f1: 0.8388 - val_loss: 0.6740 - val_acc: 0.7149 - val_recall: 0.6605 - val_precision: 0.3525 - val_f1: 0.4513\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 116s 104ms/step - loss: 0.3975 - acc: 0.8329 - recall: 0.8655 - precision: 0.8202 - f1: 0.8405 - val_loss: 0.6756 - val_acc: 0.7282 - val_recall: 0.6442 - val_precision: 0.3645 - val_f1: 0.4567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea8e9ffa50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=10,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_lstm():\n",
    "#     K.clear_session()\n",
    "\n",
    "#     model_sent1 = Sequential(a)\n",
    "#     model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "#     model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "#     model_sent1.add(L.CuDNNLSTM(128,return_sequences=False))\n",
    "# #     model_sent1.add(L.Bidirectional(L.CuDNNLSTM(128,return_sequences=False)))\n",
    "# #     model_sent1.add(L.Dense(64))\n",
    "# #     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "#     model_sent2 = Sequential()\n",
    "#     model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "#     model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "#     model_sent2.add(L.CuDNNLSTM(128,return_sequences=False))\n",
    "# #     model_sent2.add(L.Bidirectional(L.CuDNNLSTM(128,return_sequences=False)))\n",
    "# #     model_sent2.add(L.Dense(64))\n",
    "# #     model_sent2.add(L.Activation('elu')) \n",
    "\n",
    "#     input1 = L.Input(shape=(None, ))\n",
    "#     input2 = L.Input(shape=(None, ))\n",
    "\n",
    "#     encoder1 = model_sent1(input1)\n",
    "#     encoder2 = model_sent2(input2)\n",
    "\n",
    "#     X = L.concatenate([encoder1, encoder2])\n",
    "    \n",
    "#     X = L.Conv2D(filters=16, kernel_size=5, padding=\"same\")(X)\n",
    "#     X = L.Flatten()(X)\n",
    "#     X = L.Dense(128)(X)\n",
    "#     X = L.Activation('elu')(X)\n",
    "#     X = L.Dense(32)(X)\n",
    "#     X = L.Activation('elu')(X)\n",
    "\n",
    "#     predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "#     model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "#     model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "2025\n",
      "(71733, 25)\n",
      "((71733, 25), (71733, 25))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          391296      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          391296      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 782,592\n",
      "Trainable params: 782,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 13s 12ms/step - loss: 0.6820 - acc: 0.5740 - recall: 0.5052 - precision: 0.5921 - f1: 0.5175 - val_loss: 0.5637 - val_acc: 0.7982 - val_recall: 0.1868 - val_precision: 0.3699 - val_f1: 0.2372\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.6610 - acc: 0.6117 - recall: 0.5317 - precision: 0.6400 - f1: 0.5726 - val_loss: 0.6535 - val_acc: 0.6615 - val_recall: 0.5340 - val_precision: 0.2734 - val_f1: 0.3546\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.6504 - acc: 0.6227 - recall: 0.5453 - precision: 0.6501 - f1: 0.5868 - val_loss: 0.6142 - val_acc: 0.7104 - val_recall: 0.4749 - val_precision: 0.3034 - val_f1: 0.3621\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.6418 - acc: 0.6328 - recall: 0.5848 - precision: 0.6511 - f1: 0.6119 - val_loss: 0.6556 - val_acc: 0.6396 - val_recall: 0.6207 - val_precision: 0.2762 - val_f1: 0.3759\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.6289 - acc: 0.6499 - recall: 0.6293 - precision: 0.6595 - f1: 0.6408 - val_loss: 0.6675 - val_acc: 0.6157 - val_recall: 0.6789 - val_precision: 0.2721 - val_f1: 0.3822\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.6176 - acc: 0.6619 - recall: 0.6624 - precision: 0.6643 - f1: 0.6604 - val_loss: 0.6119 - val_acc: 0.6714 - val_recall: 0.6256 - val_precision: 0.3004 - val_f1: 0.3988\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.6050 - acc: 0.6737 - recall: 0.6908 - precision: 0.6712 - f1: 0.6782 - val_loss: 0.6420 - val_acc: 0.6334 - val_recall: 0.6865 - val_precision: 0.2843 - val_f1: 0.3957\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.5955 - acc: 0.6925 - recall: 0.7199 - precision: 0.6853 - f1: 0.6997 - val_loss: 0.6244 - val_acc: 0.6536 - val_recall: 0.6725 - val_precision: 0.2956 - val_f1: 0.4040\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.5879 - acc: 0.7041 - recall: 0.7374 - precision: 0.6948 - f1: 0.7130 - val_loss: 0.6007 - val_acc: 0.6797 - val_recall: 0.6509 - val_precision: 0.3126 - val_f1: 0.4150\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.5698 - acc: 0.7150 - recall: 0.7532 - precision: 0.7023 - f1: 0.7248 - val_loss: 0.6111 - val_acc: 0.6734 - val_recall: 0.6648 - val_precision: 0.3102 - val_f1: 0.4156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc4e74bf90>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(len(vocab))\n",
    "print(X_train_seq[1].shape)\n",
    "print(X_train_seq[0].shape, X_train_seq[1].shape)\n",
    "\n",
    "model = get_model_lstm_cosine()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=10,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.5594 - acc: 0.7220 - recall: 0.7588 - precision: 0.7099 - f1: 0.7313 - val_loss: 0.6135 - val_acc: 0.6841 - val_recall: 0.6634 - val_precision: 0.3179 - val_f1: 0.4223\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.5590 - acc: 0.7325 - recall: 0.7783 - precision: 0.7160 - f1: 0.7437 - val_loss: 0.6182 - val_acc: 0.6862 - val_recall: 0.6818 - val_precision: 0.3240 - val_f1: 0.4316\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.5431 - acc: 0.7439 - recall: 0.7896 - precision: 0.7266 - f1: 0.7547 - val_loss: 0.6495 - val_acc: 0.6660 - val_recall: 0.7032 - val_precision: 0.3114 - val_f1: 0.4243\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.5339 - acc: 0.7496 - recall: 0.7911 - precision: 0.7336 - f1: 0.7592 - val_loss: 0.6422 - val_acc: 0.6973 - val_recall: 0.6574 - val_precision: 0.3297 - val_f1: 0.4308\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.5188 - acc: 0.7573 - recall: 0.7973 - precision: 0.7415 - f1: 0.7663 - val_loss: 0.6345 - val_acc: 0.7003 - val_recall: 0.6463 - val_precision: 0.3312 - val_f1: 0.4299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc4d0d9150>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,    \n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accuracy    recall  precision  specificity        f1      tp      fn      fp       tn\n",
      "val   0.67431  0.663291   0.310348     0.676727  0.422849  3668.0  1862.0  8151.0  17063.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train  0.716295  0.773622   0.369401     0.703421  0.500037  10177.0  2978.0  17373.0  41205.0\n",
      "     accuracy    recall  precision  specificity        f1      tp      fn      fp       tn\n",
      "val  0.673757  0.661483    0.30958      0.67645  0.421769  3658.0  1872.0  8158.0  17056.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train  0.714288  0.771342   0.367193     0.701475  0.497536  10147.0  3008.0  17487.0  41091.0\n"
     ]
    }
   ],
   "source": [
    "predictor = ClassificationPredictor(model)\n",
    "print(predictor.evaluate([X_val_seq[0], X_val_seq[1]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_seq[0], X_train_seq[1]], y_train, title='train'))\n",
    "\n",
    "print(predictor.evaluate([X_val_seq[1], X_val_seq[0]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_seq[1], X_train_seq[0]], y_train, title='train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 DSSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "\n",
    "\n",
    "#     model_sent1 = Sequential()\n",
    "#     model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "#     model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "#     model_sent1.add(L.SimpleRNN(128,return_sequences=False))\n",
    "# #     model_sent1.add(L.Dense(64))\n",
    "# #     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "#     model_sent2 = Sequential()\n",
    "#     model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "#     model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "#     model_sent2.add(L.SimpleRNN(128,return_sequences=False))\n",
    "# #     model_sent2.add(L.Dense(64))\n",
    "# #     model_sent2.add(L.Activation('elu')) \n",
    "\n",
    "#     input1 = L.Input(shape=(None, ))\n",
    "#     input2 = L.Input(shape=(None, ))\n",
    "\n",
    "#     encoder1 = model_sent1(input1)\n",
    "#     encoder2 = model_sent2(input2)\n",
    "\n",
    "#     X = L.concatenate([encoder1, encoder2])\n",
    "#     X = L.Dense(64)(X)\n",
    "#     X = L.Activation('elu')(X)\n",
    "#     X = L.Dense(32)(X)\n",
    "#     X = L.Activation('elu')(X)\n",
    "\n",
    "#     predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "#     model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "#     model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路\n",
    "- 分词效果分析\n",
    "- 错误分类分析\n",
    "    - 同音词分析\n",
    "- Data Augmentation   \n",
    "    - 去除一个或若干个词。\n",
    "- 外部训练好的word embedding向量。 \n",
    "- 把所有汉字变成拼音，这样可以有晓解决谐音字的问题。  07-07\n",
    "\n",
    "## 总结\n",
    "\n",
    "- BOW\n",
    "    - fit： 经过15 epochs，f1在0.25左右  \n",
    "    val_loss: 0.5505 - val_acc: 0.7726 - val_recall: 0.2186 - val_precision: 0.3074 - val_f1: 0.2459\n",
    "    - fit_generator + generate_batches_tfidf：\n",
    "        - 由于数据倾斜比较厉害，所以在获得batch的时候，正例:反例=1:1，这种情况下性能快速提高。 f1提高到0.37。  下面是5 epochs 的结果。如果再增加epochs，则过拟合更加严重。     \n",
    "    \n",
    "            val_loss: 0.6349 - val_acc: 0.6664 - val_recall: 0.5774 - val_precision: 0.2874 - val_f1: 0.3761      \n",
    "      \n",
    "        - 另外一个体会是，以前会用某一个阈值（一般0.5，如果数据）来计算y_pred, 但现在发现可能用generate_batches_tfidf里，设定positive的比例，更加简单，效果也更好。尝试了positive_ratio=[0.6,0.5.0.4,0.3,0.2], 发现当positive_ratio=[0.6,0.5,0.4]的时候,性能接近。当positive_ratio=0.3，f1马上下降到0.3，当positive_ratio=0.2，性能非常差。这也进一步证明数据倾斜的问题。    \n",
    "    \n",
    "        - 随机化两个句子的顺序。\n",
    "        \n",
    "            当调用 generate_batches_tfidf(X_train_tfidf, y_train, positive_ratio=0.5, random_sent=False) 产生traiing数据， 结果如下：  \n",
    "\n",
    "![ali_nlp_random_sent_false.ipynb](../../../image/ali_nlp_random_sent_false.png)\n",
    "\n",
    "            而调用 generate_batches_tfidf(X_train_tfidf, y_train, positive_ratio=0.5, random_sent=True) 产生traiing数据， 结果如下\n",
    "![ali_nlp_random_sent_true.ipynb](../../../image/ali_nlp_random_sent_true.png)\n",
    "\n",
    "            比较这两个结果，可以很明显发现，random_sent=True能够提高val f1_大概0.02左右，而且能够非常明显降低过拟合。\n",
    "            \n",
    "- Word Embedding    \n",
    "    - simple RNN\n",
    "    - lstm \n",
    "    - gru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Word Embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
