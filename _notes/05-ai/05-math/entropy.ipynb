{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 信息熵是什么\n",
    "categories: mathematics\n",
    "date: 2020-06-16\n",
    "---\n",
    "\n",
    "熵 (entropy) 这一词最初来源于热力学。1948年，香农将热力学中的熵引入信息论，所以也被称为香农熵 (Shannon entropy)，信息熵 (information entropy)。既然是信息熵，首先来看信息。\n",
    "\n",
    "## 信息量\n",
    "\n",
    "信息量用于度量信息的不确定性，信息量和事件发生的概率有关，事件发生的概率越小，其信息量越大。某个事件$x$发生的信信息量为：\n",
    "\n",
    "$$\n",
    "I(x) = −\\log(p(x))\n",
    "$$\n",
    "\n",
    "以下三个事件中, 信息量逐渐递减。\n",
    "\n",
    "1. 中国足球队获得了世界冠军：概率太小了，信息量很大。\n",
    "2. 荷兰足球队获得了世界冠军\n",
    "3. 中国乒乓球队获得了世界冠军\n",
    "\n",
    "当$x, y$是相互独立的事件， 可以得出：\n",
    "\n",
    "$$\n",
    "I(x, y)  = I(x) + I(y)\n",
    "$$\n",
    "\n",
    "## 信息熵 (information entropy)\n",
    "\n",
    "![img](images/200px-Binary_entropy_plot.svg.png)\n",
    "\n",
    "再来看熵的公式：\n",
    "\n",
    "$$\n",
    "H(X) =  - \\sum_{i=1}^n {p({x_i})} \\log (p({x_i}))~~~~(i = 1,2, \\ldots ,n)\n",
    "$$\n",
    "\n",
    "其中$X$ 表示的是随机变量，随机变量的取值为 $({x_1},{x_2}, \\ldots ,{x_n})$，$p({x_i}) $表示事件发生的概率，且有$\\sum {p({x_i})}  = 1$ 。熵可以理解为信息的不确定程度，是随机变量不确定性的度量.  熵越大，随机变量不确定性越大，系统越混乱（无序）。同样看之前的三个事件，事件2熵最大。\n",
    "\n",
    "### 信息量和熵\n",
    "\n",
    "信息量是描述某一个事件发生的不确定性，而熵是描述整个系统的不确定性。比如，对于一个六面的骰子，每一个面相当于一个事件，而骰子的所有面构成了一个系统。从公式上，也可以看出**熵是对系统中所有事件发生的信息量的加权平均(数学期望)**。下面看看实际的例子, 假设我们从某婚介网站拿到以下数据. \n",
    "\n",
    "![img](images/v2-ccf99fcbded18b80b8d9d8553be1eec6_hd.png)\n",
    "\n",
    "设随机变量$X=\\{帅, 不帅\\}$,  $Y=\\{嫁, 不嫁\\}$ \n",
    "\n",
    "- $H(X) = -\\frac 1 3\\log\\frac 1 3-\\frac 2 3\\log \\frac 2 3 = 0.918 $\n",
    "- $H(Y) = -\\frac 1 2\\log\\frac 1 2-\\frac 1 2\\log \\frac 1 2 = 1$\n",
    "\n",
    "相关代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "P(X) = [0.3333333333333333, 0.6666666666666666]\n",
      "H(X) = 0.9182958340544896\n",
      "--------------------------------------------------\n",
      "P(X) = [0.5, 0.5]\n",
      "H(X) = 1.0\n",
      "--------------------------------------------------\n",
      "P(X) =\n",
      " [0.12662  0.090728 0.081755 0.074776 0.0668   0.069791 0.062812 0.060818\n",
      " 0.059821 0.042871 0.03988  0.027916 0.027916 0.023928 0.023928 0.021934\n",
      " 0.01994  0.01994  0.018943 0.014955 0.00997  0.001994 0.007976 0.000997\n",
      " 0.001994 0.000997]\n",
      "H(X) = 4.183962086962477\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "def entropy(X, base=None):    \n",
    "    h = sum([-x*log(x) for x in X]) if base is None else sum([-x*log(x, base) for x in X])\n",
    "    return h\n",
    "\n",
    "print('-'*50)\n",
    "X = [1/3, 2/3]\n",
    "print(\"P(X) =\", X)\n",
    "print(\"H(X) =\", entropy(X, base=2))    \n",
    "\n",
    "print('-'*50)\n",
    "X = [1/2, 1/2]\n",
    "print(\"P(X) =\", X)\n",
    "print(\"H(X) =\", entropy(X, base=2))    \n",
    "\n",
    "print('-'*50)\n",
    "X = [12.7, 9.1, 8.2, 7.5, 6.7, 7.0, 6.3, 6.1, 6.0, 4.3, 4.0, \n",
    "             2.8, 2.8, 2.4, 2.4, 2.2, 2.0, 2.0, 1.9, 1.5, 1.0, 0.2,\n",
    "             0.8, 0.1, 0.2, 0.1]\n",
    "X = [x/sum(X) for x in X ]   \n",
    "print(\"P(X) =\\n\", np.around(X, 6))\n",
    "print(\"H(X) =\",  entropy(X, base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200616160507533](images/image-20200616160507533.png)\n",
    "\n",
    "上面的结果，可以看到，不确定体现在：\n",
    "\n",
    "- 概率是否均匀分布\n",
    "- 不确定项的数据量\n",
    "\n",
    "## 联合熵（joint entropy）\n",
    "\n",
    "对于服从联合分布为$p(x,y)$的一对离散随机变量$(X,Y)$ ,其**联合熵**定义为：\n",
    "\n",
    "$$\n",
    "H(X,Y) =  - \\sum_{x \\in X,y \\in Y} {p(x,y)} \\ln (p(x,y))\n",
    "$$\n",
    "\n",
    "当$X, Y$是相互独立的系统，即$p(x, y) = p(x)p(y)$， 可以得出：\n",
    "\n",
    "$$\n",
    "H(X,Y) =  H(X) + H(Y)\n",
    "$$\n",
    "\n",
    " 当$X, Y$非相互独立， 可以得出：\n",
    "\n",
    "$$\n",
    "H(X,Y) < H(X) + H(Y）\n",
    "$$\n",
    "\n",
    "基本的含义是，既然$X, Y$既然相互有关系，系统的不确定性降低了。比方：足球比赛如果有假球发生，结果内定了,  悬念没了，大家都不愿看了。相关代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "P(XY) =\n",
      " [[0.41666667 0.25      ]\n",
      " [0.08333333 0.25      ]]\n",
      "H(X, Y) = 1.8250112108241772\n",
      "--------------------------------------------------\n",
      "P(X) = [0.66666667 0.33333333]\n",
      "P(Y) = [0.5 0.5]\n",
      "H(X) = 0.9182958340544896\n",
      "H(Y) = 1.0\n",
      "H(X) + H(Y) = 1.9182958340544896\n",
      "--------------------------------------------------\n",
      "P(XY) =\n",
      " [[0.33333333 0.33333333]\n",
      " [0.16666667 0.16666667]]\n",
      "H(X, Y) = 1.9182958340544893\n"
     ]
    }
   ],
   "source": [
    "def joint_entropy(XY, base=None):\n",
    "    xy = [p for xY in XY for p in xY ]\n",
    "    return entropy(xy, base)\n",
    "\n",
    "print('-'*50)    \n",
    "XY = np.array([[5/12,3/12], [1/12,3/12]])\n",
    "print(\"P(XY) =\\n\", XY)\n",
    "print(\"H(X, Y) =\", joint_entropy(XY, base=2)) \n",
    "\n",
    "print('-'*50)  \n",
    "X = np.sum(np.array(XY), axis=1)\n",
    "Y = np.sum(np.array(XY), axis=0)\n",
    "\n",
    "print(\"P(X) =\", X)\n",
    "print(\"P(Y) =\", Y)\n",
    "print(\"H(X) =\",  entropy(X, base=2))\n",
    "print(\"H(Y) =\",  entropy(Y, base=2))\n",
    "\n",
    "# 当X，Y并不独立的时候，联合熵（即整体X，Y的熵）小于X，Y的熵之和。\n",
    "print(\"H(X) + H(Y) =\",  entropy(Y, base=2) + entropy(X, base=2))\n",
    "\n",
    "print('-'*50)  \n",
    "# 当X，Y独立的时候，联合熵等于X，Y的熵之和\n",
    "XY = X.reshape((2, 1)) @ Y.reshape((1, 2))\n",
    "print(\"P(XY) =\\n\", XY) \n",
    "print(\"H(X, Y) =\", joint_entropy(XY, base=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200616160651988](images/image-20200616160651988.png)\n",
    "\n",
    "## 条件熵 (conditional entropy)\n",
    "\n",
    "若$(X,Y)\\sim p(x,y)$，条件熵定义为：\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "H(Y|X) &= \\sum_{x \\in X} {p(x)H(Y|X = x)} \\\\ \n",
    "       &=  - \\sum_{x \\in X} {p(x)} \\sum_{y \\in Y} {p(y|x)\\log p(y|x)} \\\\ \n",
    "       &=  - \\sum_{x \\in X} {\\sum_{y \\in Y} {p(x,y)\\log p(y|x)} }  \\end{array}\n",
    "$$\n",
    "\n",
    "其物理意义：$X$给定条件下, $Y$ 的条件概率分布的熵对 X 的数学期望。\n",
    "\n",
    "而且\n",
    "\n",
    "$$\n",
    "H(Y|X) <= H(Y)\n",
    "$$\n",
    "\n",
    "其含义是， 当在一个系统中，知道了$X$的信息，则整个系统的熵是变小的。\n",
    "\n",
    "再看具体的例子：\n",
    "\n",
    "![img](images/v2-ccf99fcbded18b80b8d9d8553be1eec6_hd.png)\n",
    "\n",
    "设随机变量$Y=\\{嫁, 不嫁\\}$, 可以算得：\n",
    "\n",
    "$H(Y) = -\\frac 1 2\\log\\frac 1 2-\\frac 1 2\\log \\frac 1 2 = 1$\n",
    "\n",
    "再设随机变量$X=\\{帅, 不帅\\}$, 如果我们知道了一个人的长相是帅还是不帅, 再来计算$Y$的熵, 这就是条件熵.\n",
    "\n",
    "![img](images/v2-83f2f4b00981806c74e330b2d6f91db5_hd.png)\n",
    "\n",
    "下面看一下计算过程:\n",
    "\n",
    "- $X = 不帅$  \n",
    "\n",
    "  $H(Y|X = 不帅) = - \\frac 3 4 \\log \\frac 3 4- \\frac 1 4 \\log \\frac 1 4 = 0.811\\\\ \n",
    "  p(X = 不帅) = 4/12 = 1/3$\n",
    "\n",
    "- $X = 帅$\n",
    "\n",
    "  $H(Y|X = 帅) = - \\frac 3 8 \\log \\frac 3 8- \\frac 5 8 \\log \\frac 5 8 = 0.954\\\\ \n",
    "  p(X = 帅) = 8/12 = 2/3$\n",
    "\n",
    "再来计算条件熵:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "H(Y|X) &= \\sum_{x \\in X} {p(x)H(Y|X = x)} \\\\ \n",
    "       &= p(X = 不帅)H(Y|X = 不帅) + p(X = 帅)H(Y|X = 帅)  \\\\ \n",
    " &= \\frac 1 3 *  0.811+ \\frac 2 3 *  0.954   \\\\ \n",
    " &= 0.907 <1= H(Y)\n",
    " \\end{array}\n",
    "$$\n",
    "\n",
    "0.907小于1, 也就是当知道了一些信息后, 整个系统不确定性降低了, 也就是熵降低了。相关代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(Y_X, base=None):\n",
    "    h = sum([ Y_x['x'] * entropy(Y_x['Y'], base) for Y_x in Y_X])\n",
    "    return h\n",
    "            \n",
    "Y_X = [{'x':1/3, 'Y':[3/4, 1/4]},\n",
    "       {'x':2/3, 'Y':[3/8, 5/8]}]\n",
    "print(\"P(Y|X) =\", Y_X)\n",
    "\n",
    "Y = np.sum(np.array([ Y_x['x'] * np.array(Y_x['Y']) for Y_x in Y_X]), axis=0)\n",
    "print(\"P(Y) =\", Y)\n",
    "print(\"H(Y|X) =\", conditional_entropy(Y_X, 2))print(\"H(Y) =\", entropy(Y, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200616160806442](images/image-20200616160806442.png)\n",
    "\n",
    "### 联合熵和条件熵的关系\n",
    "\n",
    "$$\n",
    "H(X,Y) = H(X) + H(Y|X) =  H(Y) + H(X|Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "H(X,Y) &=  - \\sum_{x \\in X} {\\sum_{y \\in Y} {p(x,y)\\log p(x,y)} } \\\\\n",
    " &=  - \\sum_{x \\in X} {\\sum_{y \\in Y} {p(x,y)\\log p(x)p(y|x)} } \\\\\n",
    "&=  - \\sum_{x \\in X} {\\sum_{y \\in Y} {p(x,y)\\log p(x)} }  - \\sum_{x \\in X} {\\sum_{y \\in Y} {p(x,y)\\log p(y|x)} } \\\\\n",
    "&=  - \\sum_{x \\in X} {p(x)\\log p(x)}  - \\sum_{x \\in X} {\\sum_{y \\in Y} {p(x,y)\\log p(y|x)} } \\\\\n",
    "&= H(X) + H(Y|X)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "上面公式可以理解为：描述$ X$ 和 $Y$ 所需的信息是描述 X 自己所需的信息, 加上给定 $X$ 的条件下具体化 $Y $所需的额外信息。可以通俗的理解, 当系统不可知的变量越多, 整个系统越混乱 也就是熵越大.\n",
    "\n",
    "如果做一点儿变化: \n",
    "\n",
    "$$\n",
    "H(Y|X) = H(X,Y) - H(X)\n",
    "$$\n",
    "\n",
    "条件熵可以表示为$ (X, Y)$ 发生所包含的熵，减去 $X$ 单独发生包含的熵，即在 $X $发生的前提下， $Y $发生 “新” 带来的熵 。\n",
    "\n",
    "## 信息增益(Information gain)\n",
    "\n",
    "$$\n",
    "IG(Y,X) = H(Y) - H(Y|X) = H(X) - H(X|Y) = IG(X,Y) \\\\\n",
    "IG(Y,X) = IG(X,Y) = H(X) + H(Y) - H(X, Y)\n",
    "$$\n",
    "\n",
    ">  上面公式可以用$H(X,Y) = H(X) + H(Y\\vert X) =  H(Y) + H(X \\vert Y)$推出. \n",
    "\n",
    "信息增益是对称的, 也就是两个事件相互的信息增益是相同的，所以信息增益也叫相互信息(Mutual Information).\n",
    "\n",
    "上面公式可以得出：\n",
    "\n",
    "- 当$X$和$Y$相互独立，信息增益为0，即$H(Y) = H(Y\\vert X) $， 也就是说X的信息，没啥作用，不能减少熵。\n",
    "- 当$X$和$Y$相同时，$H(Y\\vert X)=0$，也就是说，都告诉答案了，没信息了，熵为0。\n",
    "\n",
    "**应用**\n",
    "\n",
    "- 在决策树算法中, 经常采用信息增益来进行特征选择. \n",
    "- 在文本挖掘中，经常采用信息增益来发现Syntagmatic关系的词汇。\n",
    "\n",
    "## KL散度，相对熵 (relative entropy)\n",
    "\n",
    "假设$p(x),q(x)$ 是随机变量$X$中取不同值时的两个概率分布，那么 $p$的$q$的相对熵是： \n",
    "\n",
    "$$\n",
    "D_{KL}\\left( {p||q} \\right) = \\sum_x {p\\left( x \\right)\\log \\frac{{p\\left( x \\right)}}{{q\\left( x \\right)}}}  = {E_{p\\left( x \\right)}}\\log \\frac{{p\\left( x \\right)}}{{q\\left( x \\right)}}\n",
    "$$\n",
    "\n",
    "相对熵又称互熵，鉴别信息，KL 散度（Kullback–Leibler divergence， KLD），Kullback 熵。它是两个随机分布之间距离的度量.  当两个分布相同的时候，KL散度为0，越是不同，KL散度越大。相关代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(P,Q):\n",
    "    sum = P*(np.log2(P/Q))#计算KL散度\n",
    "    all_value= [x for x in sum if str(x) != 'nan' and str(x)!= 'inf']#除去inf值\n",
    "    return np.sum(all_value)\n",
    "\n",
    "\n",
    "#P和Q是两个概率分布，np.array格式\n",
    "print('-'*50)\n",
    "P = np.array([0.65, 0.35])\n",
    "Q = np.array([0.667,0.333])\n",
    "print(\"P =\", P)\n",
    "print(\"Q =\", Q)\n",
    "print(\"D(P||Q)\", KL(P,Q))\n",
    "\n",
    "print('-'*50)\n",
    "P = np.array([0.65, 0.35])\n",
    "Q = np.array([0.9,0.1])\n",
    "print(\"P =\", P)\n",
    "print(\"Q =\", Q)\n",
    "print(\"D(P||Q)\", KL(P,Q))\n",
    "\n",
    "print('-'*50)\n",
    "P = np.array([0.65, 0.35])\n",
    "Q = np.array([0.17,0.83])\n",
    "print(\"P =\", P)\n",
    "print(\"Q =\", Q)\n",
    "print(\"D(P||Q)\", KL(P,Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200616160943279](images/image-20200616160943279.png)\n",
    "\n",
    "### 信息增益和KL散度\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "IG(X,Y) &=H(X)-H(X|Y)\\\\\n",
    "&=-\\sum_xp(x)logp(x)+\\sum_y\\sum_xp(x,y)log\\frac{p(x,y)}{p(y)}\\\\\n",
    "&=-\\sum_x\\sum_yp(x,y)logp(x)+\\sum_y\\sum_xp(x,y)log\\frac{p(x,y)}{p(y)}\\\\\n",
    "&=\\sum_y\\sum_xp(x,y)log\\frac{p(x,y)}{p(y)p(x)} \\\\\n",
    "&=D_{KL}(p(x,y)||p(x)p(y))\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "从上面推导可以看到，信息增益也是一种KL散度，如果$p(x), p(y)$相互独立，分布$p(x,y)$和分布$p(x)p(y)$相同，则信息增益为0。从这个意义上说，信息增益时描述分布是否相互独立的度量。下面是两个应用。\n",
    "\n",
    "- [浅谈KL散度（相对熵）在用户画像中的应用](https://www.cnblogs.com/charlotte77/p/5392052.html?from=timeline&isappinstalled=0): 主要使用散度来计算消费群体对不同商品的喜好\n",
    "\n",
    "- [KL 散度（从动力系统到推荐系统）](https://chuansongme.com/n/2759305)\n",
    "\n",
    "  \n",
    "\n",
    "## 交叉熵（cross entropy）\n",
    "\n",
    "交叉熵本质上可以看成,用一个猜测的分布的编码方式去编码其真实的分布,得到的平均编码长度或者信息量。其中$p(x)$是真实的分布，而$q(x)$是猜测的分布. 在机器学习中，经常采用交叉熵来作为损失函数。而且交叉熵的公式和最大似然估计（MLE）推导出来的公式相同，不得不说，信息论这里和概率论在这里融合了。\n",
    "\n",
    "$$\n",
    "H(p, q) = - \\sum_x p(x)\\log\\left({q(x)}\\right)\n",
    "$$\n",
    "\n",
    "### 交叉熵和散度的关系\n",
    "\n",
    "下面推导中，由于$H(p)$是常量, 所以可以看到交叉熵和散度的是等价。\n",
    "$$\n",
    "\\begin{align*}\n",
    "D\\left( {p||q} \\right) &= \\sum_x {p\\left( x \\right)\\log \\frac{{p\\left( x \\right)}}{{q\\left( x \\right)}}} \\\\\n",
    "&=  \\sum_x p\\left( x \\right) \\log \\left(p\\left( x \\right)\\right) - \\sum_x p\\left( x \\right) \\log \\left(q\\left( x \\right)\\right)  \\\\\n",
    "&=  H(p, q) - H(p)\n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "## 进阶\n",
    "\n",
    "仔细体会一下几句话。\n",
    "\n",
    "- 熵的意义是对p分布进行编码所需的最小字节数。\n",
    "\n",
    "- KL散度的意义是“额外所需的编码长度”如果我们用q的编码来表示p。\n",
    "\n",
    "- 交叉熵指的是当你用q作为密码本来表示p时所需要的“平均的编码长度”。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
