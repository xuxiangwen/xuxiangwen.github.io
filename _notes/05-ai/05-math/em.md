熵 (entropy) 这一词最初来源于热力学。1948年，香农将热力学中的熵引入信息论，所以也被称为香农熵 (Shannon entropy)，信息熵 (information entropy)。既然是信息熵，首先来看信息。

## 信息量

信息量用于度量信息的不确定性，信息量和事件发生的概率有关，事件发生的概率越小，其信息量越大。某个事件$x$发生的信信息量为：

$$
I(x) = −\log(p(x))
$$

以下三个事件中, 信息量逐渐递减。

1. 中国足球队获得了世界冠军：概率太小了，信息量很大。
2. 荷兰足球队获得了世界冠军
3. 中国乒乓球队获得了世界冠军

当$x, y$是相互独立的事件， 可以得出：

$$
I(x, y)  = I(x) + I(y)
$$

## 信息熵 (information entropy)

![img](images/200px-Binary_entropy_plot.svg.png)

再来看熵的公式：

$$
H(X) =  - \sum_{i=1}^n {p({x_i})} \log (p({x_i}))~~~~(i = 1,2, \ldots ,n)
$$

其中$X$ 表示的是随机变量，随机变量的取值为 $({x_1},{x_2}, \ldots ,{x_n})$，$p({x_i}) $表示事件发生的概率，且有$\sum {p({x_i})}  = 1$ 。熵可以理解为信息的不确定程度，是随机变量不确定性的度量.  熵越大，随机变量不确定性越大，系统越混乱（无序）。同样看之前的三个事件，事件2熵最大。

