核主成分分析（Kernel Principal Component Analysis）简称KPCA，它是对[主成分分析（PCA）](https://eipi10.cn/mathematics/2021/05/12/covariance_matrix/#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90)的扩展。KPCA假设，样本数据是高维到低维的投影，这种投影会带来信息的丢失，正如：一棵大树在正午的投影是一个椭圆，我们很难区分哪里是树干，哪里是树冠，也就是说，在低维空间无法区分很多信息，基于此，KPCA通过映射函数把数据映射到高维空间，然后再进行PCA操作，这样更有可能获取关键的主成分。本文将对KPCA进行通俗易懂的详细推导，并实际比较KPCA和PCA，帮助大家更加深刻的理解它们。

## 从低维到高维

KPCA通过映射函数把数据映射到高维空间，这带来两个问题：

- 映射函数如何选取呢？
- 高维空间要多少维才合适呢？

很明显，对于第一个问题没有确定答案，因为数学上的相关函数太多了。那我们先看看第二个问题，由于只能看到低维数据，我们很难想象高维数据的特征，换句话说，高维数据有无限的可能，正如上面的例子，一个二维椭圆投影，在三维空间，可以是一棵树，也可以是一个人，而在四维空间，可能性就更多了。由此，如果映射后的高维空间维数非常的多，或许更加能够反应这种可能性，也就是说，我们希望高维空间的维度高一些。

然而，选用维度高的空间，又带来计算问题。这是当维度很高的时候，PCA的计算量变的及其的大，下面是计算步骤：

1. 计算协方差矩阵。

   设$\mathbf X = \begin{bmatrix} x_1-\mu & x_2-\mu & \cdots & x_n-\mu \end{bmatrix}$ ，即中心化的矩阵，协方差矩阵可以简化为：
   $$
   \Sigma = \frac 1 {n-1} \mathbf X \cdot \mathbf X^{\mathbf T}
   $$

   其中$\mathbf X$是$$d\times n$$矩阵，$d$表示随机变量个数，$$表示n样本个数。

2. 





## 核方法d

**核方法** 是一类把低维空间的非线性可分问题，转化为高维空间的线性可分问题的方法。核方法不仅仅用于SVM，还可以用于其他数据为非线性可分的算法。核方法的理论基础是C



## 核技巧 

核技巧（Kernal Trick）是一种利用核函数直接计算 ![[公式]](https://www.zhihu.com/equation?tex=%5Clangle+%5Cphi%28x%29%2C%5Cphi%28z%29+%5Crangle%E2%80%8B) ，从而加速核方法计算的技巧。





## 实际应用

是重要的降维方法之一，PCA的本质是选取特征值最大的几个对应特征向量，对数据进行降维，然而这样也带来几个问题：

- PCA容易收到离群点的影响。如果有若干离群点，将会对特征矩阵有极大的影响
- PCA隐含假定数据是正态分布，如果数据不是正态分布，作用大打折扣。
- 特征矩阵本质上对数据进行旋转，所以PCA只能处理数据的线性关系。
- PCA忽略的特征值，可能也包含很多有用信息。



### 离群点的例子

非正态分布。

## 参考

- [Kernel Principal Component Analysis](https://shengtao96.github.io/2017/06/09/Kernel-Principal-Component-Analysis/)

