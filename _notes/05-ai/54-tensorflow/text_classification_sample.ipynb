{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to trains a sentiment analysis model to classify movie reviews as *positive* or *negative*, based on the text of the review.   In our POC, there are 68 classes ofL1 and  318 classes of L2, so it is much complex than the notebook , but we can run POC with the same workflow as the notebook. \n",
    "\n",
    "This notebook uses the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews.\n",
    "\n",
    "IMDb lets users rate movies on a scale from 1 to 10, but the dataset only has positive and negative reviews. \n",
    "\n",
    "- negative review : score≤4\n",
    "- positive review : score≥7\n",
    "\n",
    "## Text Classification Workflow\n",
    "\n",
    "Here is a standard workflow of text classification workflow. \n",
    "\n",
    "![Topic classification](images/text_classfication_workflow.png)\n",
    "\n",
    "In order to simplify the steps, they can be splitted to two stages.\n",
    "\n",
    "- Data Preparation \n",
    "  - Gather  Data: collect enough and good quality data.\n",
    "  - Explore Data: \n",
    "  - Prepare Data:\n",
    "- Model Construction\n",
    "  - Build, Train & Evaluate Model\n",
    "  - Tune Hyper-Parameters\n",
    "  - Deploy Model\n",
    "\n",
    "## Gather Data\n",
    "\n",
    "Before starting, let's import the packages that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "# restrict the memory of GPU to 1GB\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data \n",
    "\n",
    "Let's download and extract IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url, untar=True)\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217095446441](images/image-20201217095446441.png)\n",
    "\n",
    "Explore the directory structure.\n",
    "\n",
    "![image-20201104115404253](images/image-20201104115404253.png)\n",
    "\n",
    "There are 2 folders which have the data.\n",
    "\n",
    "- train: train data\n",
    "- test: test data\n",
    "\n",
    "### View data\n",
    "\n",
    "Let's count how many positive and negative comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(base_dir):\n",
    "    pos_dir = os.path.join(base_dir, 'pos')\n",
    "    pos_files = os.listdir(pos_dir)\n",
    "    pos_files = [os.path.join(pos_dir, file_name) for file_name in pos_files]\n",
    "    \n",
    "    neg_dir = os.path.join(base_dir, 'neg')\n",
    "    neg_files = os.listdir(neg_dir)\n",
    "    neg_files = [os.path.join(neg_dir, file_name) for file_name in neg_files]    \n",
    "\n",
    "    return pos_files, neg_files\n",
    "\n",
    "train_pos_files, train_neg_files = get_files(os.path.join(dataset_dir, 'train'))\n",
    "test_pos_files, test_files = get_files(os.path.join(dataset_dir, 'test'))\n",
    "    \n",
    "print('train dataset: {} positive reviews, {} negative reviews'.format(len(train_pos_files), len(train_neg_files)))\n",
    "print('test dataset: {} positive reviews, {} negative reviews'.format(len(test_pos_files), len(test_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217115036617](images/image-20201217115036617.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in train_pos_files[0:3]:    \n",
    "    print('-'*30 + file_path + '-'*30 )\n",
    "    with open(file_path) as f:\n",
    "        comment = f.read() \n",
    "        print(comment if len(comment)<=800 else comment[0:800]+' ...')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217111649607](images/image-20201217111649607.png)\n",
    "\n",
    "Below are some negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in train_neg_files[0:3]:    \n",
    "    print('-'*30 + file_path + '-'*30 )\n",
    "    with open(file_path) as f:\n",
    "        comment = f.read() \n",
    "        print(comment if len(comment)<=800 else comment[0:800]+' ...') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217100213645](images/image-20201217100213645.png)\n",
    "\n",
    "Below are some positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in pos_files[0:3]:    \n",
    "    \n",
    "    print('-'*30 + file_path + '-'*30 )\n",
    "    with open(file_path) as f:\n",
    "        comment = f.read() \n",
    "        print(comment if len(comment)<=800 else comment[0:800]+' ...') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "\n",
    "Let's explore the data further. Understanding the characteristics of your data beforehand will enable you to build a better model. \n",
    "\n",
    "### Basic Information\n",
    "\n",
    "First, we load the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(imdb_data_path, seed=123):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))\n",
    "\n",
    "(train_texts, train_labels), (test_texts, test_labels) = load_imdb_sentiment_analysis_dataset(dataset_dir)\n",
    "print('train_texts.length={}, train_labels.shape={}'.format(len(train_texts), train_labels.shape))\n",
    "print('test_texts.length={}, test_labels.shape={}'.format(len(test_texts), test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217122349480](images/image-20201217122349480.png)\n",
    "\n",
    "### Classes Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(train_labels, test_labels, classes):\n",
    "    '''class distribution'''\n",
    "    def plot_dist(labels, title, color='blue', width = 0.7):        \n",
    "        bin_count = np.bincount(labels)       \n",
    "        bin_percentage = bin_count/len(labels)\n",
    "        rects = plt.bar(classes, bin_count, width, color=color)\n",
    "        plt.title(title)\n",
    "        plt.ylim(0,max(bin_count)*1.1) \n",
    "        \n",
    "        for i, r in enumerate(rects):\n",
    "            plt.annotate('{:0.1f}%'.format(int(bin_percentage[i]*100)),\n",
    "                        xy=(r.get_x() + r.get_width() / 2, r.get_height()),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')        \n",
    "\n",
    "    classes_count = len(classes)            \n",
    "    height = min(16, classes_count*4)\n",
    "    plt.figure(figsize=(height, 4))\n",
    "    plt.subplot(121)\n",
    "    plot_dist(train_labels, 'Train', color='teal')\n",
    "    plt.subplot(122)\n",
    "    plot_dist(test_labels, 'Test', color='coral')\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3) \n",
    "    plt.show()    \n",
    "\n",
    "classes=['negative', 'positive']    \n",
    "plot_distribution(train_labels, test_labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217123826241](images/image-20201217123826241.png)\n",
    "\n",
    "### Sample Length Distribution\n",
    "\n",
    "Check if the sample length in train dataset is similar as that in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length_distribution(train_text_lengths, test_text_lengths):\n",
    "    \n",
    "    def plot_length_dist(lengths, title):\n",
    "        median = np.median(lengths)\n",
    "        plt.hist(lengths, 50)\n",
    "        plt.axvline(x=median, color='coral', linestyle='dashed', linewidth=2)\n",
    "        plt.text(median+30, 100, median, color='coral')\n",
    "        plt.xlabel('Length of a sample')\n",
    "        plt.ylabel('Number of samples')\n",
    "        plt.title('{}: Sample Length Distribution'.format(title))          \n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(121)\n",
    "    plot_length_dist(train_text_lengths, 'Train')\n",
    "    plt.subplot(122)\n",
    "    plot_length_dist(test_text_lengths, 'Test')\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3) \n",
    "    plt.show()    \n",
    "\n",
    "    \n",
    "train_text_lengths = [len(s.split()) for s in train_texts]\n",
    "test_text_lengths = [len(s.split()) for s in test_texts]\n",
    "plot_length_distribution(train_text_lengths, test_text_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217131542940](images/image-20201217131542940.png)\n",
    "\n",
    "### Frequency Distribution of Words\n",
    "\n",
    "Check if the frequency distribution in train dataset is similar as that in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
    "                                          ngram_range=(1, 2),\n",
    "                                          num_ngrams=50,\n",
    "                                          stop_words=None,\n",
    "                                          title=''\n",
    "                                         ):\n",
    "    \"\"\"Plots the frequency distribution of n-grams.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
    "            Min and mplt are the lower and upper bound values for the range.\n",
    "        num_ngrams: int, number of n-grams to plot.\n",
    "            Top `num_ngrams` frequent n-grams will be plotted.\n",
    "    \"\"\"\n",
    "    # Create args required for vectorizing.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1, 1),\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "            'stop_words': stop_words\n",
    "    }\n",
    "    vectorizer = text.CountVectorizer(**kwargs)\n",
    "\n",
    "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
    "    # idxices). This also converts every text to an array the length of\n",
    "    # vocabulary, where every element idxicates the count of the n-gram\n",
    "    # corresponding at that idxex in vocabulary.\n",
    "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "    # This is the list of all n-grams in the index order from the vocabulary.\n",
    "    all_ngrams = list(vectorizer.get_feature_names())\n",
    "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
    "    # ngrams = all_ngrams[:num_ngrams]\n",
    "\n",
    "    # Add up the counts per n-gram ie. column-wise\n",
    "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
    "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
    "        zip(all_counts, all_ngrams), reverse=True)])\n",
    "    ngrams = list(all_ngrams)[:num_ngrams]\n",
    "    counts = list(all_counts)[:num_ngrams]\n",
    "\n",
    "    idx = np.arange(num_ngrams)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('{} Frequency distribution of n-grams'.format(title))\n",
    "    plt.xticks(idx, ngrams, rotation=45)\n",
    "\n",
    "\n",
    "def plot_frequency_distribution(train_texts, test_texts) :  \n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union([\"br\"])\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    plt.subplot(211)\n",
    "    plot_frequency_distribution_of_ngrams(train_texts, title='Train', stop_words=stop_words)  \n",
    "    plt.subplot(212)\n",
    "    plot_frequency_distribution_of_ngrams(test_texts, title='Test', stop_words=stop_words)   \n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n",
    "plot_frequency_distribution(train_texts, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217134151280](images/image-20201217134151280.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()\n",
    "for s in train_texts:\n",
    "    word_counter.update(s.split())\n",
    "for s in test_texts:\n",
    "    word_counter.update(s.split())   \n",
    "\n",
    "print('Number of Vocabulary: {}'.format(len(word_counter)))\n",
    "\n",
    "for word in list(word_counter):\n",
    "    if word_counter[word]<100: word_counter.pop(word)\n",
    "        \n",
    "print('Number of Vocabulary: {}'.format(len(word_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217142728260](images/image-20201217142728260.png)\n",
    "\n",
    "## Prepare Data\n",
    "\n",
    "Different models need to different data. So we need to choose a model first. \n",
    "\n",
    "### Choose a Model\n",
    "\n",
    "Below is a flowchart from Google as a starting point to construct the first model. We can apply it in our POC. \n",
    "\n",
    "![image-20201217135347743](images/image-20201217135347743.png)\n",
    "\n",
    "1. Calculate the number of samples/number of words per sample ratio.\n",
    "2. If this ratio is less than 1500, tokenize the text as n-grams and use a simple multi-layer perceptron (MLP) model to classify them (left branch in the flowchart below):\n",
    "  a. Split the samples into word n-grams; convert the n-grams into vectors.\n",
    "  b. Score the importance of the vectors and then select the top 20K using the scores.\n",
    "    c. Build an MLP model.\n",
    "3. If the ratio is greater than 1500, tokenize the text as sequences and use a sepCNN model to classify them (right branch in the flowchart below):\n",
    "     a. Split the samples into words; select the top 20K words based on their frequency.\n",
    "    b. Convert the samples into word sequence vectors.\n",
    "    c. If the original number of samples/number of words per sample ratio is less   than 15K, using a fine-tuned pre-trained embedding with the sepCNN  model will likely provide the best results.\n",
    "4. Measure the model performance with different hyperparameter values to find\n",
    "   the best model configuration for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_median = np.median(train_text_lengths)\n",
    "train_number = len(train_texts)\n",
    "print (\"[the number of samples]/[number of words per sample] = {:0.1f}\".format(train_number/length_median))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217144321430](images/image-20201217144321430.png)\n",
    "\n",
    "Though the ratio is leas than 1500, we still use RNN as the following model. Because RNN and its extended models (such as Transformer, BERT) show much powerful performance than MLP for 5 years.  \n",
    "\n",
    "In real project, we will build MLP model first and then also build RNN model, and they are both the baselines of the performance.\n",
    "\n",
    "### Load Data\n",
    "\n",
    "First,  clean unused folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "print(os.listdir(train_dir))\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "print(os.listdir(train_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217152311389](images/image-20201217152311389.png)\n",
    "\n",
    "Next, create train dataset, validation dataset and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    os.path.join(dataset_dir, 'train'), \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)\n",
    "\n",
    "print('-'*100)\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    os.path.join(dataset_dir, 'train'), \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='validation', \n",
    "    seed=seed)\n",
    "\n",
    "print('-'*100)\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    os.path.join(dataset_dir, 'test'), \n",
    "    batch_size=batch_size)\n",
    "\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print('-'*50, label_batch.numpy()[i], '-'*50)\n",
    "        print(text_batch.numpy()[i])\n",
    "        \n",
    "print('-'*100)\n",
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217152912385](images/image-20201217152912385.png)\n",
    "\n",
    "### Token to Vector\n",
    "\n",
    "The following function will remove the  punctuations or HTML elements to simplify the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a `TextVectorization` layer. we will use this layer to standardize, tokenize, and vectorize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "print(\"7 ---> \",vectorize_layer.get_vocabulary()[7])\n",
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217154849445](images/image-20201217154849445.png)\n",
    "\n",
    "Next, we will convert the dataset from token to vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# cache data to improve the performance\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, let's check the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_batch, label_batch = next(iter(train_ds))\n",
    "first_vector, first_label = vector_batch[0], label_batch[0]\n",
    "\n",
    "print('Label：{}'.format(raw_train_ds.class_names[first_label]))\n",
    "print('-'*50, 'Vector', '-'*50)\n",
    "print(first_vector.numpy())\n",
    "print('-'*50, 'orginal review', '-'*50)\n",
    "print(' '.join([vectorize_layer.get_vocabulary()[v] for v in first_vector]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217155826393](images/image-20201217155826393.png)\n",
    "\n",
    "## Build, Train & Evaluate Model\n",
    "\n",
    "We will apply Embedding and GRU in the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn(embedding_dim, dropput=0):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Embedding(input_dim=max_features+1, output_dim=embedding_dim, mask_zero=True),\n",
    "        layers.SimpleRNN(units=32),\n",
    "        layers.Dense(units=1)])\n",
    "    model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer='adam',\n",
    "                  metrics=tf.metrics.BinaryAccuracy(threshold=0.0))    \n",
    "    return model \n",
    "\n",
    "model = get_rnn(embedding_dim=32)\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217163929440](images/image-20201217163929440.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.keras.utils.plot_model(model, '{}.png'.format(model.name), show_shapes=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217163943713](images/image-20201217163943713.png)\n",
    "\n",
    "Next, let's start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217164441244](images/image-20201217164441244.png)\n",
    "\n",
    "Next, let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''显示训练的loss和accuracy的走势图'''\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    epochs = range(1, len(history.history['binary_accuracy'])+1)\n",
    "    plt.subplot(121)\n",
    "    plt.plot(epochs, history.history['binary_accuracy'])\n",
    "    plt.plot(epochs, history.history['val_binary_accuracy'])\n",
    "    plt.title('Accuracy vs. epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(epochs)\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(epochs, history.history['loss'])\n",
    "    plt.plot(epochs, history.history['val_loss'])\n",
    "    plt.title('Loss vs. epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(epochs)\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "    plt.show() \n",
    "    \n",
    "def evaluate(test_ds):\n",
    "    loss, accuracy = model.evaluate(test_ds, verbose=False)\n",
    "    print(\"Loss: {:0.5f}, Accuracy: {:0.1f}%\".format(loss, accuracy*100))\n",
    "\n",
    "plot_history(history)    \n",
    "evaluate(test_ds)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217164722084](images/image-20201217164722084.png)\n",
    "\n",
    "The diagram shows that the model is a serious over-fitting.   But the Accuracy is 85%. It is not bad and It can be the baseline. Next we can consider to optimize the model or choose better model.  There are many models coming in the recent years. We can try some of them and choose the better one.\n",
    "\n",
    "For IMBD dataset, here is state of art. The best model can get 96.8 accuracy. It's really good result. Maybe, we can consider to implement it by ourselves.\n",
    "\n",
    "![image-20201217170631096](images/image-20201217170631096.png)\n",
    "\n",
    "## Tune Hyperparameters \n",
    "\n",
    "After we choose a model, we still can optimize it by choosing better hyperparameters.\n",
    "\n",
    "We had to choose a number of hyperparameters for defining and training the model. We relied on intuition, examples and best practice recommendations. Our first choice of hyperparameter values, however, may not yield the best results. \n",
    "\n",
    "- **Number of layers in the model**: The number of layers in a neural network is an indicator of its complexity. We must be careful in choosing this value. Too many layers will allow the model to learn too much information about the training data, causing overfitting. Too few layers can limit the model’s learning ability, causing underfitting. For text classification datasets, we experimented with one, two, and three-layer MLPs. Models with two layers performed well, and in some cases better than three-layer models. Similarly, we tried sepCNNs with four and six layers, and the four-layer models performed well.\n",
    "- **Number of units per layer**: The units in a layer must hold the information for the transformation that a layer performs. For the first layer, this is driven by the number of features. In subsequent layers, the number of units depends on the choice of expanding or contracting the representation from the previous layer. Try to minimize the information loss between layers. We tried unit values in the range `[8, 16, 32, 64]`, and 32/64 units worked well.\n",
    "- **Dropout rate**: Dropout layers are used in the model for [regularization](https://developers.google.com/machine-learning/glossary/?utm_source=DevSite&utm_campaign=Text-Class-Guide&utm_medium=referral&utm_content=glossary&utm_term=dropout-regularization#dropout_regularization). They define the fraction of input to drop as a precaution for overfitting. Recommended range: 0.2–0.5.\n",
    "- **Learning rate**: This is the rate at which the neural network weights change between iterations. A large learning rate may cause large swings in the weights, and we may never find their optimal values. A low learning rate is good, but the model will take more iterations to converge. It is a good idea to start low, say at 1e-4. If the training is very slow, increase this value. If your model is not learning, try decreasing learning rate.\n",
    "\n",
    "## Deploy Your Model\n",
    "\n",
    "If we get a good model, we will start to consider to deploy the model. We will consider a lot of things. \n",
    "\n",
    "- Which platform will we use?\n",
    "\n",
    "  Cloud(AWS or Azure) or own server. If the model uses the deep learning technology, maybe, we can consider to choose a server which has GPU. \n",
    "\n",
    "- Automate the prediction\n",
    "\n",
    "  We need to connect the prediction with other applications and make everything run automatically\n",
    "\n",
    "- Model Refreshment\n",
    "\n",
    "  Sometimes we need to re-train the model because of some causes, for example, data change, business logic change, new better model. We need to provide a mechanism to refresh the model smoothly. \n",
    "\n",
    "- Cyber Security \n",
    "\n",
    "  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T08:54:56.656677Z",
     "start_time": "2020-12-18T08:54:56.650982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894143, 0.7310586 ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.nn.softmax([2.0, 3.0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
