{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "首先引入需要的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T06:01:09.909451Z",
     "start_time": "2021-12-06T06:01:09.811014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ipyparams\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string  \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import traceback\n",
    "\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import stats\n",
    "from sklearn import feature_extraction, feature_selection\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model, models, layers, regularizers, preprocessing, datasets, metrics, losses, optimizers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorboard.plugins.hparams import api as hp \n",
    "\n",
    "base_path = os.path.abspath('/tf/eipi10/jian-xu3/qbz95')\n",
    "sys.path.append(base_path)\n",
    "\n",
    "import qbz95\n",
    "from qbz95 import tf as qtf\n",
    "from qbz95 import utils as qutils\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# 设置GPU内存自动扩增\n",
    "qtf.utils.set_gpu_memory_growth()\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# auto load the changes of referenced codes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ebablbe auto-completion\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:10:12.796087Z",
     "start_time": "2021-12-06T05:10:12.680413Z"
    }
   },
   "outputs": [],
   "source": [
    "# 当module有新的方法的时候，需要运行下面方法。\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:10:13.022930Z",
     "start_time": "2021-12-06T05:10:12.798954Z"
    }
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"config the strategy of reducing learning rate\"\"\"\n",
    "    lr_times = [(0, 1), (60, 1e-1), (90, 1e-2), (105, 1e-3), (120, 0.5e-3)]\n",
    "    \n",
    "    base_lr = 1e-3\n",
    "    new_lr = base_lr\n",
    "    for border_epoch, times in lr_times:\n",
    "        if epoch>=border_epoch: \n",
    "            new_lr = base_lr*times\n",
    "    if abs(lr - new_lr)>1e-7:\n",
    "        if new_lr > lr > 0.1*new_lr - 1e-7:\n",
    "            print('Epoch %05d: Still keep learning rate %s instead of %s' % \n",
    "                  (epoch + 1, round(lr, 7), round(new_lr, 7))) \n",
    "            return lr   \n",
    "        print('Epoch %05d: LearningRateScheduler reducing learning rate to %s from %s.' % \n",
    "              (epoch + 1, round(new_lr, 7), round(lr, 7)))\n",
    "    return new_lr\n",
    "\n",
    "\n",
    "\n",
    "output_path = os.path.abspath('./output')\n",
    "data_name = 'imdb'\n",
    "data_path = os.path.join(output_path, data_name)\n",
    "program_path = os.path.join(data_path, qbz95.utils.get_notebook_name().split('.')[0])\n",
    "word_vectors_path = '/tf/eipi10/xuxiangwen.github.io/_notes/05-ai/54-tensorflow/models/word_vectors'\n",
    "classes = ['Negative', 'Positive']\n",
    "\n",
    "params = {\n",
    "    'data_name': data_name,\n",
    "    'data_path': data_path,\n",
    "    'program_name': qbz95.utils.get_notebook_name(),\n",
    "    'program_path': program_path,\n",
    "    'classes': classes,\n",
    "    'word_vectors_path': word_vectors_path,\n",
    "    'sample_perecent': 1,    \n",
    "    'text_columns': 'snps_sa_comments',\n",
    "    'validation_percent': 0.0,  \n",
    "    'use_stop_words': True,    \n",
    "    'stop_words':stopwords.words('english'),\n",
    "    'batch_size': 32,    \n",
    "    'max_features': 20000,\n",
    "    'sequence_length': 500,    \n",
    "    'epochs': 3,    \n",
    "    'learning_rate':0.001,\n",
    "    'clip_value':None,\n",
    "    'dropout':0.1,\n",
    "    'metrics':['accuracy'], \n",
    "    'loss': losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    'restore_best_checkpoint':True,\n",
    "    'use_savedmodel':True,\n",
    "    'use_bias_initializer':True,\n",
    "    'use_class_weight':False,\n",
    "    'class_weight': [1.0, 1.0],\n",
    "    'callbacks': {\n",
    "        'ModelCheckpoint': {\n",
    "            'enabled': True,\n",
    "            'monitor': 'val_accuracy',               \n",
    "        },\n",
    "        'EarlyStopping': {\n",
    "            'enabled': True,\n",
    "            'patience': 40,   \n",
    "            'monitor': 'val_accuracy',            \n",
    "        },\n",
    "        'ReduceLROnPlateau': {\n",
    "            'enabled': True,\n",
    "            'monitor': 'val_loss',\n",
    "            'patience': 15,\n",
    "            'factor': np.sqrt(0.1),            \n",
    "        },\n",
    "        'LearningRateScheduler': {\n",
    "            'enabled': True,\n",
    "            'schedule': lr_schedule,            \n",
    "            \n",
    "        }             \n",
    "    },\n",
    "    'model_params':{\n",
    "        'mlp':{'dropout':0.6, 'layer_count':1, 'units':256, 'epochs':15},\n",
    "        'rnn':{'dropout':0.4, 'embedding_dim':200, 'units':200, 'epochs':5},\n",
    "        'embedding':{'dropout':0.4, 'embedding_dim':200, 'epochs':15},\n",
    "        'sepcnn':{'dropout':0.4, 'epochs': 10, 'batch_size':128},\n",
    "        'tl':{'dropout':0.4, 'trainable':True, 'layer_count':1, 'unit':64, 'epochs': 10},\n",
    "        'pg':{'dropout':0.4, 'embedding_dim':300, 'units':80, 'epochs':20, 'learning_rate':0.0005},  \n",
    "    },\n",
    "    'embedding_paths':{\n",
    "        'cc_en_300':os.path.join(word_vectors_path, 'snps', 'cc.en.300.vec'),\n",
    "        'fasttext_crawl_300d_2M':os.path.join(word_vectors_path, 'fasttext-crawl-300d-2M.vec'),\n",
    "        'glove_twitter_27B_200d':os.path.join(word_vectors_path, 'glove.twitter.27B.200d.txt')\n",
    "    },\n",
    "    'keras_layper_paths':{\n",
    "    },\n",
    "    'model_resutls':{\n",
    "        'show_top_n':20,\n",
    "        'show_exclude_columns':qtf.classification.ModelResults.exclude_columns1\n",
    "    }\n",
    "}\n",
    "\n",
    "params = qtf.classification.Params(params)\n",
    "model_results=qtf.classification.ProgramModelResults(params.program_path)\n",
    "model_results.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据\n",
    "\n",
    "开始下载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:10:13.078431Z",
     "start_time": "2021-12-06T05:10:13.025320Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir =  os.path.join(os.path.expanduser('~'), '.keras/datasets/aclImdb') \n",
    "if not os.path.exists(dataset_dir):\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url, untar=True)\n",
    "    dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "    print(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20201217095446441](images/image-20201217095446441.png)\n",
    "\n",
    "下面是压缩文件解开后的目录结构。\n",
    "\n",
    "![image-20201104115404253](images/image-20201104115404253.png)\n",
    "\n",
    "其中train和test目录包含了实际的文本数据，详细说明见[IMDB](https://eipi10.cn/others/2020/10/22/dataset/#imdb---large-movie-review-dataset)。\n",
    "\n",
    "### 查看数据\n",
    "\n",
    "下面看一看实际的数据是啥样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:10:13.209671Z",
     "start_time": "2021-12-06T05:10:13.080335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 12500 positive reviews, 12500 negative reviews\n",
      "test dataset: 12500 positive reviews, 12500 negative reviews\n"
     ]
    }
   ],
   "source": [
    "def get_files(base_dir):\n",
    "    pos_dir = os.path.join(base_dir, 'pos')\n",
    "    pos_files = os.listdir(pos_dir)\n",
    "    pos_files = [os.path.join(pos_dir, file_name) for file_name in pos_files]\n",
    "    \n",
    "    neg_dir = os.path.join(base_dir, 'neg')\n",
    "    neg_files = os.listdir(neg_dir)\n",
    "    neg_files = [os.path.join(neg_dir, file_name) for file_name in neg_files]    \n",
    "\n",
    "    return pos_files, neg_files\n",
    "\n",
    "train_pos_files, train_neg_files = get_files(os.path.join(dataset_dir, 'train'))\n",
    "test_pos_files, test_files = get_files(os.path.join(dataset_dir, 'test'))\n",
    "    \n",
    "print('train dataset: {} positive reviews, {} negative reviews'.format(len(train_pos_files), len(train_neg_files)))\n",
    "print('test dataset: {} positive reviews, {} negative reviews'.format(len(test_pos_files), len(test_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:10:15.819170Z",
     "start_time": "2021-12-06T05:10:13.211797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_texts.length=25000, train_labels.shape=(25000,)\n",
      "test_texts.length=25000, test_labels.shape=(25000,)\n"
     ]
    }
   ],
   "source": [
    "def get_samples(texts, labels, rate):\n",
    "    indexs = np.arange(len(texts))\n",
    "    sample_indexs, _, sample_labels, _ = train_test_split(indexs, labels,                                                          \n",
    "                                                          test_size=1-rate,\n",
    "                                                          random_state=12)\n",
    "    sample_texts = [texts[index] for index in sample_indexs]\n",
    "    return sample_texts, sample_labels    \n",
    "\n",
    "def load_imdb_sentiment_analysis_dataset(imdb_data_path, seed=123):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))\n",
    "\n",
    "(train_texts, train_labels), (test_texts, test_labels) = load_imdb_sentiment_analysis_dataset(dataset_dir)\n",
    "\n",
    "if params.sample_perecent<1:\n",
    "    train_texts, train_labels = get_samples(train_texts, train_labels, rate=params.sample_perecent)\n",
    "    test_texts, test_labels = get_samples(test_texts, test_labels, rate=params.sample_perecent)\n",
    "\n",
    "print('train_texts.length={}, train_labels.shape={}'.format(len(train_texts), train_labels.shape))\n",
    "print('test_texts.length={}, test_labels.shape={}'.format(len(test_texts), test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非常非常非常奇怪， 如果第24行如下内容，将会报错\n",
    "~~~\n",
    "                      ngrams=layer._ngrams, output_mode=layer._output_mode,\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:49:23.011543Z",
     "start_time": "2021-12-06T05:49:22.912742Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_layer(layer, layer_path):\n",
    "    if layer_path.endswith('pkl'):\n",
    "        obj = {'config': layer.get_config(),\n",
    "               'weights': layer.get_weights()}\n",
    "        qutils.pickle.ObjectPickle.save(layer_path, obj)\n",
    "    else:\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Input(shape=(1,), dtype=tf.string))\n",
    "        model.add(layer)\n",
    "        model.save(layer_path, save_format=\"tf\")\n",
    "\n",
    "\n",
    "def load_layer(layer_path):\n",
    "    if layer_path.endswith('pkl'):\n",
    "        obj = qutils.pickle.ObjectPickle.load(layer_path)\n",
    "        layer = tf.keras.layers.TextVectorization.from_config(obj['config'])\n",
    "        layer.set_weights(obj['weights'])          \n",
    "    else:\n",
    "        model = tf.keras.models.load_model(layer_path)\n",
    "        layer = model.layers[0]  \n",
    "        \n",
    "    if layer._output_mode == 'int':\n",
    "        layer = get_tv(layer._standardize, layer._max_tokens,\n",
    "                      ngrams=layer._ngrams, output_mode='int',\n",
    "                      max_sequence_length=layer._output_sequence_length,\n",
    "                      vocabulary=layer.get_vocabulary(include_special_tokens=False))    \n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def get_ngram_layer(standardize, texts, max_features):\n",
    "    ngrams = (1, 2)\n",
    "    output_mode = 'tf-idf'\n",
    "    max_sequence_length = None\n",
    "    layer = get_tv(standardize, max_features,\n",
    "                  ngrams=ngrams, output_mode=output_mode,\n",
    "                  max_sequence_length=max_sequence_length,\n",
    "                  vocabulary=None)\n",
    "    text_dataset = tf.data.Dataset.from_tensor_slices(texts)\n",
    "    layer.adapt(text_dataset)  \n",
    "    return layer\n",
    "\n",
    "\n",
    "def get_sequence_layer(standardize, texts, max_features, max_sequence_length):\n",
    "    ngrams = None\n",
    "    output_mode = 'int'\n",
    "    layer = get_tv(standardize, max_features,\n",
    "                  ngrams=ngrams, output_mode=output_mode,\n",
    "                  max_sequence_length=max_sequence_length,\n",
    "                  vocabulary=None)\n",
    "    text_dataset = tf.data.Dataset.from_tensor_slices(texts)\n",
    "    layer.adapt(text_dataset)    \n",
    "    return layer\n",
    "\n",
    "def get_tv(standardize, max_features, ngrams, output_mode, \n",
    "           max_sequence_length, vocabulary):\n",
    "    layer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=max_features, standardize=standardize,\n",
    "        split='whitespace', ngrams=ngrams, output_mode=output_mode,\n",
    "        output_sequence_length=max_sequence_length, pad_to_max_tokens=False,\n",
    "        vocabulary=vocabulary\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "def get_dataset(layer, data, labels, use_shuffle=False, batch_size=None, drop_remainder=False, use_cache=True):\n",
    "    def map_(text):\n",
    "        text = tf.expand_dims(text, axis=-1)\n",
    "        return layer(text)[0]\n",
    "    \n",
    "    \n",
    "    if labels is None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((data,))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "    if use_shuffle and labels is not None:\n",
    "        dataset = dataset.shuffle(len(labels), reshuffle_each_iteration=True)\n",
    "\n",
    "    if labels is None:\n",
    "        dataset = dataset.map(lambda x: map_(x))\n",
    "    else:\n",
    "        dataset = dataset.map(lambda x, y: (map_(x), y))\n",
    "\n",
    "#     if batch_size is not None:\n",
    "#         dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    if use_cache:\n",
    "        dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)  \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_data_from_layer(layer):\n",
    "    dataset = get_dataset(layer, train_texts[100:1000], labels=None, batch_size=32)\n",
    "    data = iter(dataset).next()\n",
    "    print(dataset)\n",
    "    print(data.shape)    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data_from_generator(generator):\n",
    "    dataset = generator(train_texts[100:1000], labels=None, batch_size=32)\n",
    "    data = iter(dataset).next()\n",
    "    print(dataset)\n",
    "    print(data.shape)    \n",
    "    return data\n",
    "\n",
    "def show_tv(tv):\n",
    "    print('-'*20)\n",
    "    attrs = ['_standardize', '_max_tokens', '_ngrams', '_output_sequence_length', '_output_mode']\n",
    "    for attr in attrs:\n",
    "        print(f'{attr}={getattr(tv, attr)}')         \n",
    "\n",
    "def test(layer_or_generator):\n",
    "    def test_layer(layer, layer_save_format):\n",
    "        print(\"=\"*50)\n",
    "        data1 = get_data_from_layer(layer)\n",
    "\n",
    "        layer_path= './output/imdb_0721_p100/my' + layer_save_format\n",
    "        save_layer(layer, layer_path)\n",
    "        show_tv(layer)\n",
    "        layer = load_layer(layer_path)\n",
    "        show_tv(layer)\n",
    "\n",
    "        data2 = get_data_from_layer(layer)\n",
    "        np.testing.assert_allclose(data1, data2)\n",
    "        \n",
    "    def test_generator(generator, layer_save_format):\n",
    "        print(\"=\"*50)\n",
    "        generator.layer_save_format=layer_save_format\n",
    "        data1 = get_data_from_generator(generator)\n",
    "        generator_path = './output/imdb_0721_p100/my.generator'\n",
    "        generator.before_pickle(generator_path)\n",
    "        generator.after_pickle(generator_path)\n",
    "\n",
    "        data2 = get_data_from_generator(generator)\n",
    "        np.testing.assert_allclose(data1, data2)        \n",
    "        \n",
    "    # 传入qbz95中的Generator，或者本地的TextVectorization\n",
    "    print('#'*20, f'test {type(layer_or_generator)}', '#'*20)\n",
    "    if isinstance(layer_or_generator, tf.keras.layers.TextVectorization):\n",
    "        \n",
    "        layer = layer_or_generator\n",
    "        try:\n",
    "            test_layer(layer, '.pkl')\n",
    "        except Exception as e :\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "        try:            \n",
    "            test_layer(layer, '.layer') \n",
    "        except Exception as e :\n",
    "            print(traceback.format_exc())\n",
    "    else:        \n",
    "        generator = layer_or_generator\n",
    "        \n",
    "        try:\n",
    "            test_generator(generator, '.pkl')\n",
    "        except Exception as e :\n",
    "            print(traceback.format_exc())\n",
    "            \n",
    "        try:\n",
    "            test_generator(generator, '.layer')  \n",
    "        except Exception as e :\n",
    "            print(traceback.format_exc())      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:49:29.610464Z",
     "start_time": "2021-12-06T05:49:24.430956Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 05:49:27,819: INFO: save object to ./output/imdb_0721_p100/my.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### test <class 'keras.layers.preprocessing.text_vectorization.TextVectorization'> ####################\n",
      "==================================================\n",
      "<PrefetchDataset shapes: (20000,), types: tf.float32>\n",
      "(20000,)\n",
      "--------------------\n",
      "_standardize=<function standardize_tf_text1 at 0x7f8cb07d81f0>\n",
      "_max_tokens=20000\n",
      "_ngrams=(1, 2)\n",
      "_output_sequence_length=None\n",
      "_output_mode=tf_idf\n",
      "--------------------\n",
      "_standardize=<function standardize_tf_text1 at 0x7f8cb07d81f0>\n",
      "_max_tokens=20000\n",
      "_ngrams=(1, 2)\n",
      "_output_sequence_length=None\n",
      "_output_mode=tf_idf\n",
      "<PrefetchDataset shapes: (20000,), types: tf.float32>\n",
      "(20000,)\n",
      "==================================================\n",
      "<PrefetchDataset shapes: (20000,), types: tf.float32>\n",
      "(20000,)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 05:49:28,097: WARNING: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./output/imdb_0721_p100/my.layer/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 05:49:28,999: INFO: Assets written to: ./output/imdb_0721_p100/my.layer/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "_standardize=<function standardize_tf_text1 at 0x7f8cb07d81f0>\n",
      "_max_tokens=20000\n",
      "_ngrams=(1, 2)\n",
      "_output_sequence_length=None\n",
      "_output_mode=tf_idf\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 05:49:29,485: WARNING: No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "_standardize=<function standardize_tf_text1 at 0x7f8cb07d81f0>\n",
      "_max_tokens=20000\n",
      "_ngrams=(1, 2)\n",
      "_output_sequence_length=None\n",
      "_output_mode=tf_idf\n",
      "<PrefetchDataset shapes: (20000,), types: tf.float32>\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "layer = get_ngram_layer(standardize=qtf.text.standardize_tf_text1, \n",
    "                        texts=train_texts[0:1000], \n",
    "                        max_features=params.max_features)\n",
    "\n",
    "test(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:49:33.357177Z",
     "start_time": "2021-12-06T05:49:29.612763Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 05:49:31,682: INFO: save object to ./output/imdb_0721_p100/my.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### test <class 'keras.layers.preprocessing.text_vectorization.TextVectorization'> ####################\n",
      "==================================================\n",
      "<PrefetchDataset shapes: (500,), types: tf.int64>\n",
      "(500,)\n",
      "--------------------\n",
      "_standardize=<function standardize_tf_text1 at 0x7f8cb07d81f0>\n",
      "_max_tokens=20000\n",
      "_ngrams=None\n",
      "_output_sequence_length=500\n",
      "_output_mode=int\n",
      "--------------------\n",
      "_standardize=<function standardize_tf_text1 at 0x7f8cb07d81f0>\n",
      "_max_tokens=20000\n",
      "_ngrams=None\n",
      "_output_sequence_length=500\n",
      "_output_mode=int\n",
      "<PrefetchDataset shapes: (500,), types: tf.int64>\n",
      "(500,)\n",
      "==================================================\n",
      "<PrefetchDataset shapes: (500,), types: tf.int64>\n",
      "(500,)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 05:49:32,087: WARNING: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./output/imdb_0721_p100/my.layer/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 05:49:32,714: INFO: Assets written to: ./output/imdb_0721_p100/my.layer/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "_standardize=<function standardize_tf_text1 at 0x7f8cb07d81f0>\n",
      "_max_tokens=20000\n",
      "_ngrams=None\n",
      "_output_sequence_length=500\n",
      "_output_mode=int\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 05:49:33,058: WARNING: No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "_standardize=<function standardize_tf_text1 at 0x7f8cb07d81f0>\n",
      "_max_tokens=20000\n",
      "_ngrams=None\n",
      "_output_sequence_length=500\n",
      "_output_mode=int\n",
      "<PrefetchDataset shapes: (500,), types: tf.int64>\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "layer = get_sequence_layer(standardize=qtf.text.standardize_tf_text1, \n",
    "                           texts=train_texts[0:1000], \n",
    "                           max_features=params.max_features,\n",
    "                           max_sequence_length=params.sequence_length)\n",
    "test(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextVectorization on qbz95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T06:01:54.864818Z",
     "start_time": "2021-12-06T06:01:49.093292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### test <class 'qbz95.tf.classification.layer_generator.LayerGenerator'> ####################\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 06:01:53,173: INFO: save object to ./output/imdb_0721_p100/ngram_map.pkl\n",
      "2021-12-06 06:01:53,174: INFO: loading layer from ./output/imdb_0721_p100/ngram_map.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (None, 20000), types: tf.float32>\n",
      "(32, 20000)\n",
      "<PrefetchDataset shapes: (None, 20000), types: tf.float32>\n",
      "(32, 20000)\n",
      "==================================================\n",
      "<PrefetchDataset shapes: (None, 20000), types: tf.float32>\n",
      "(32, 20000)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 06:01:53,506: WARNING: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./output/imdb_0721_p100/ngram_map.layer/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 06:01:54,266: INFO: Assets written to: ./output/imdb_0721_p100/ngram_map.layer/assets\n",
      "2021-12-06 06:01:54,302: INFO: loading layer from ./output/imdb_0721_p100/ngram_map.layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 06:01:54,710: WARNING: No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (None, 20000), types: tf.float32>\n",
      "(32, 20000)\n"
     ]
    }
   ],
   "source": [
    "generator = qtf.classification.LayerGenerator.get_ngram_layer(name='ngram_map', \n",
    "                                                              standardize=qtf.text.standardize_tf_text1, \n",
    "                                                              texts=train_texts[0:1000], \n",
    "                                                              max_features=params.max_features)\n",
    "test(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T06:03:12.599961Z",
     "start_time": "2021-12-06T06:03:05.458562Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 06:03:08,153: INFO: save object to ./output/imdb_0721_p100/sequence_map.pkl\n",
      "2021-12-06 06:03:08,155: INFO: loading layer from ./output/imdb_0721_p100/sequence_map.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### test <class 'qbz95.tf.classification.layer_generator.LayerGenerator'> ####################\n",
      "==================================================\n",
      "<PrefetchDataset shapes: (None, 500), types: tf.int64>\n",
      "(32, 500)\n",
      "<PrefetchDataset shapes: (None, 500), types: tf.int64>\n",
      "(32, 500)\n",
      "==================================================\n",
      "<PrefetchDataset shapes: (None, 500), types: tf.int64>\n",
      "(32, 500)\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 06:03:08,652: WARNING: Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./output/imdb_0721_p100/sequence_map.layer/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 06:03:11,760: INFO: Assets written to: ./output/imdb_0721_p100/sequence_map.layer/assets\n",
      "2021-12-06 06:03:12,010: INFO: loading layer from ./output/imdb_0721_p100/sequence_map.layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 06:03:12,291: WARNING: No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (None, 500), types: tf.int64>\n",
      "(32, 500)\n"
     ]
    }
   ],
   "source": [
    "generator = qtf.classification.LayerGenerator.get_sequence_layer(name='sequence_map', \n",
    "                                                                 standardize=qtf.text.standardize_tf_text1, \n",
    "                                                                 texts=train_texts[0:1000], \n",
    "                                                                 max_features=params.max_features,\n",
    "                                                                 max_sequence_length=params.sequence_length)\n",
    "test(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## root reason\n",
    "\n",
    "经过TextVectorization保存后，处理的text, 会失去padding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:48:24.974516Z",
     "start_time": "2021-12-06T05:48:24.706563Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 05:48:24,945: INFO: save object to ./tv.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'you', 'wind', 'good', 'fire', 'earth', 'and']\n",
      "--------------------------------------------------\n",
      "tf.Tensor([1 1 1 2 0 0 0 0 0 0], shape=(10,), dtype=int64)\n",
      "--------------------------------------------------\n",
      "--------------------\n",
      "_standardize=lower_and_strip_punctuation\n",
      "_max_tokens=100\n",
      "_ngrams=None\n",
      "_output_sequence_length=10\n",
      "_output_mode=int\n",
      "--------------------\n",
      "_standardize=lower_and_strip_punctuation\n",
      "_max_tokens=100\n",
      "_ngrams=None\n",
      "_output_sequence_length=10\n",
      "_output_mode=int\n",
      "['', '[UNK]', 'you', 'wind', 'good', 'fire', 'earth', 'and']\n",
      "10\n",
      "tf.Tensor([1 1 1 2], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "max_features=100\n",
    "vocab_data = [\"earth\", \"wind\", \"and\", \"fire\", \"you\", \"good\"]\n",
    "max_len = 10  # Sequence length to pad the outputs to.\n",
    "\n",
    "# Create the layer, passing the vocab directly. You can also pass the\n",
    "# vocabulary arg a path to a file containing one vocabulary word per\n",
    "# line.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len)\n",
    "\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(vocab_data)\n",
    "vectorize_layer.adapt(text_dataset)\n",
    "\n",
    "print(vectorize_layer.get_vocabulary())\n",
    "\n",
    "print('-'*50)\n",
    "dataset = tf.data.Dataset.from_tensor_slices([\"how old are you\", \"good morning\"])\n",
    "data = next(iter(dataset))\n",
    "print(vectorize_layer(data))\n",
    "\n",
    "print('-'*50)\n",
    "layer_path = \"./tv.pkl\"\n",
    "save_layer(vectorize_layer, layer_path)\n",
    "show_tv(vectorize_layer)\n",
    "\n",
    "vectorize_layer = load_layer(layer_path)\n",
    "\n",
    "show_tv(vectorize_layer)\n",
    "print(vectorize_layer.get_vocabulary())\n",
    "print(vectorize_layer._output_sequence_length) \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([\"how old are you\", \"good morning\"])\n",
    "data = next(iter(dataset))\n",
    "print(vectorize_layer.call(data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T03:36:46.965279Z",
     "start_time": "2021-12-06T03:36:46.823299Z"
    }
   },
   "source": [
    "分析源码，估计可能TextVectorization恢复后，并没有恢复内部的_index_lookup_layer对象。解决办法是，重新构建TextVectorization对象。\n",
    "\n",
    "~~~python\n",
    "  def call(self, inputs):\n",
    "    if isinstance(inputs, (list, tuple, np.ndarray)):\n",
    "      inputs = ops.convert_to_tensor_v2_with_dispatch(inputs)\n",
    "\n",
    "    inputs = self._preprocess(inputs)\n",
    "\n",
    "    # If we're not doing any output processing, return right away.\n",
    "    if self._output_mode is None:\n",
    "      return inputs\n",
    "\n",
    "    lookup_data = self._index_lookup_layer(inputs)\n",
    "    if self._output_mode == INT:\n",
    "\n",
    "      # Maybe trim the output (NOOP if self._output_sequence_length is None).\n",
    "      output_tensor = lookup_data[..., :self._output_sequence_length]\n",
    "\n",
    "      output_shape = output_tensor.shape.as_list()\n",
    "      output_shape[-1] = self._output_sequence_length\n",
    "\n",
    "      # If it is a ragged tensor, convert it to dense with correct shape.\n",
    "      if tf_utils.is_ragged(output_tensor):\n",
    "        return output_tensor.to_tensor(default_value=0, shape=output_shape)\n",
    "\n",
    "      if self._output_sequence_length is None:\n",
    "        return output_tensor\n",
    "\n",
    "      padding, _ = array_ops.required_space_to_batch_paddings(\n",
    "          output_tensor.shape, output_shape)\n",
    "      return array_ops.pad(output_tensor, padding)\n",
    "\n",
    "    return lookup_data\n",
    "~~~\n",
    "\n",
    "解决办法是，重新构建TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:29:32.560347Z",
     "start_time": "2021-12-06T05:29:32.451188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 1 1 2 0 0 0 0 0 0], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len, \n",
    "    vocabulary=vectorize_layer.get_vocabulary(include_special_tokens=False))\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices([\"how old are you\", \"good morning\"])\n",
    "data = next(iter(dataset))\n",
    "print(vectorize_layer(data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:10:34.989841Z",
     "start_time": "2021-12-06T05:10:34.933941Z"
    }
   },
   "outputs": [],
   "source": [
    "layer = vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-06T05:10:35.050096Z",
     "start_time": "2021-12-06T05:10:34.991774Z"
    }
   },
   "outputs": [],
   "source": [
    "layer = get_tv(layer._standardize, layer._max_tokens,\n",
    "              ngrams=layer._ngrams, output_mode=layer._output_mode,\n",
    "              max_sequence_length=layer._output_sequence_length,\n",
    "              vocabulary=layer.get_vocabulary(include_special_tokens=False))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "267.141px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
