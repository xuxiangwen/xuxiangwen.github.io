{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yuytuIllsv1"
   },
   "source": [
    "# Trax Quick Intro\n",
    "\n",
    "[Trax](https://trax-ml.readthedocs.io/en/latest/) is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the [Google Brain team](https://research.google.com/teams/brain/). This notebook ([run it in colab](https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb)) shows how to use Trax and where you can find more information.\n",
    "\n",
    "  1. **Run a pre-trained Transformer**: create a translator in a few lines of code\n",
    "  1. **Features and resources**: [API docs](https://trax-ml.readthedocs.io/en/latest/trax.html), where to [talk to us](https://gitter.im/trax-ml/community), how to [open an issue](https://github.com/google/trax/issues) and more\n",
    "  1. **Walkthrough**: how Trax works, how to make new models and train on your own data\n",
    "\n",
    "We welcome **contributions** to Trax! We welcome PRs with code for new models and layers as well as improvements to our code and documentation. We especially love **notebooks** that explain how models work and show how to use them to solve problems!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIl27504La0G"
   },
   "source": [
    "**General Setup**\n",
    "\n",
    "Execute the following few cells (once) before running any of the code samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:01:08.787788Z",
     "start_time": "2021-06-04T00:01:08.663720Z"
    },
    "id": "oILRLCWN_16u"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Copyright 2020 Google LLC.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:02:06.176089Z",
     "start_time": "2021-06-04T00:01:13.337428Z"
    },
    "id": "vlGjGoGMTt-D",
    "outputId": "3076e638-695d-4017-e757-98d929630e17"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Import Trax\n",
    "\n",
    "# !pip install -q -U trax\n",
    "import trax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LQ89rFFsEdk"
   },
   "source": [
    "## 1. Run a pre-trained Transformer\n",
    "\n",
    "Here is how you create an Engligh-German translator in a few lines of code:\n",
    "\n",
    "* create a Transformer model in Trax with [trax.models.Transformer](https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.transformer.Transformer)\n",
    "* initialize it from a file with pre-trained weights with [model.init_from_file](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Layer.init_from_file)\n",
    "* tokenize your input sentence to input into the model with [trax.data.tokenize](https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize)\n",
    "* decode from the Transformer with [trax.supervised.decoding.autoregressive_sample](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample)\n",
    "* de-tokenize the decoded result to get the translation with [trax.data.detokenize](https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:06:28.472071Z",
     "start_time": "2021-06-04T00:03:53.325954Z"
    },
    "id": "djTiSLcaNFGa",
    "outputId": "a7917337-0a77-4064-8a6e-4e44e4a9c7c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es ist sch√∂n, heute neue Dinge zu lernen!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Transformer model.\n",
    "# Pre-trained model config in gs://trax-ml/models/translation/ende_wmt32k.gin\n",
    "model = trax.models.Transformer(\n",
    "    input_vocab_size=33300,\n",
    "    d_model=512, d_ff=2048,\n",
    "    n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n",
    "    max_len=2048, mode='predict')\n",
    "\n",
    "# Initialize using pre-trained weights.\n",
    "model.init_from_file('gs://trax-ml/models/translation/ende_wmt32k.pkl.gz',\n",
    "                     weights_only=True)\n",
    "\n",
    "# Tokenize a sentence.\n",
    "sentence = 'It is nice to learn new things today!'\n",
    "tokenized = list(trax.data.tokenize(iter([sentence]),  # Operates on streams.\n",
    "                                    vocab_dir='gs://trax-ml/vocabs/',\n",
    "                                    vocab_file='ende_32k.subword'))[0]\n",
    "\n",
    "# Decode from the Transformer.\n",
    "tokenized = tokenized[None, :]  # Add batch dimension.\n",
    "tokenized_translation = trax.supervised.decoding.autoregressive_sample(\n",
    "    model, tokenized, temperature=0.0)  # Higher temperature: more diverse results.\n",
    "\n",
    "# De-tokenize,\n",
    "tokenized_translation = tokenized_translation[0][:-1]  # Remove batch and EOS.\n",
    "translation = trax.data.detokenize(tokenized_translation,\n",
    "                                   vocab_dir='gs://trax-ml/vocabs/',\n",
    "                                   vocab_file='ende_32k.subword')\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMo3OnsGgLNK"
   },
   "source": [
    "## 2. Features and resources\n",
    "\n",
    "Trax includes basic models (like [ResNet](https://github.com/google/trax/blob/master/trax/models/resnet.py#L70), [LSTM](https://github.com/google/trax/blob/master/trax/models/rnn.py#L100), [Transformer](https://github.com/google/trax/blob/master/trax/models/transformer.py#L189) and RL algorithms\n",
    "(like [REINFORCE](https://github.com/google/trax/blob/master/trax/rl/training.py#L244), [A2C](https://github.com/google/trax/blob/master/trax/rl/actor_critic_joint.py#L458), [PPO](https://github.com/google/trax/blob/master/trax/rl/actor_critic_joint.py#L209)). It is also actively used for research and includes\n",
    "new models like the [Reformer](https://github.com/google/trax/tree/master/trax/models/reformer) and new RL algorithms like [AWR](https://arxiv.org/abs/1910.00177). Trax has bindings to a large number of deep learning datasets, including\n",
    "[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor) and [TensorFlow datasets](https://www.tensorflow.org/datasets/catalog/overview).\n",
    "\n",
    "\n",
    "You can use Trax either as a library from your own python scripts and notebooks\n",
    "or as a binary from the shell, which can be more convenient for training large models.\n",
    "It runs without any changes on CPUs, GPUs and TPUs.\n",
    "\n",
    "* [API docs](https://trax-ml.readthedocs.io/en/latest/)\n",
    "* [chat with us](https://gitter.im/trax-ml/community)\n",
    "* [open an issue](https://github.com/google/trax/issues)\n",
    "* subscribe to [trax-discuss](https://groups.google.com/u/1/g/trax-discuss) for news\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wgfJyhdihfR"
   },
   "source": [
    "## 3. Walkthrough\n",
    "\n",
    "You can learn here how Trax works, how to create new models and how to train them on your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM12hgQnp4qo"
   },
   "source": [
    "### Tensors and Fast Math\n",
    "\n",
    "The basic units flowing through Trax models are *tensors* - multi-dimensional arrays, sometimes also known as numpy arrays, due to the most widely used package for tensor operations -- `numpy`. You should take a look at the [numpy guide](https://numpy.org/doc/stable/user/quickstart.html) if you don't know how to operate on tensors: Trax also uses the numpy API for that.\n",
    "\n",
    "In Trax we want numpy operations to run very fast, making use of GPUs and TPUs to accelerate them. We also want to automatically compute gradients of functions on tensors. This is done in the `trax.fastmath` package thanks to its backends -- [JAX](https://github.com/google/jax) and [TensorFlow numpy](https://tensorflow.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:06:28.539254Z",
     "start_time": "2021-06-04T00:06:28.474726Z"
    },
    "id": "kSauPt0NUl_o",
    "outputId": "c7288312-767d-4344-91ae-95ebf386ce57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix =\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "vector = [1. 1. 1.]\n",
      "product = [12. 15. 18.]\n",
      "tanh(product) = [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from trax.fastmath import numpy as fastnp\n",
    "trax.fastmath.use_backend('jax')  # Can be 'jax' or 'tensorflow-numpy'.\n",
    "\n",
    "matrix = fastnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(f'matrix =\\n{matrix}')\n",
    "vector = fastnp.ones(3)\n",
    "print(f'vector = {vector}')\n",
    "product = fastnp.dot(vector, matrix)\n",
    "print(f'product = {product}')\n",
    "tanh = fastnp.tanh(product)\n",
    "print(f'tanh(product) = {tanh}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snLYtU6OsKU2"
   },
   "source": [
    "Gradients can be calculated using `trax.fastmath.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:06:28.601485Z",
     "start_time": "2021-06-04T00:06:28.542042Z"
    },
    "id": "cqjYoxPEu8PG",
    "outputId": "04739509-9d3a-446d-d088-84882b8917bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(2x^2) at 1 = 4.0\n",
      "grad(2x^2) at -2 = -8.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "  return 2.0 * x * x\n",
    "\n",
    "grad_f = trax.fastmath.grad(f)\n",
    "\n",
    "print(f'grad(2x^2) at 1 = {grad_f(1.0)}')\n",
    "print(f'grad(2x^2) at -2 = {grad_f(-2.0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-wtgiWNseWw"
   },
   "source": [
    "### Layers\n",
    "\n",
    "Layers are basic building blocks of Trax models. You will learn all about them in the [layers intro](https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html) but for now, just take a look at the implementation of one core Trax layer, `Embedding`:\n",
    "\n",
    "```\n",
    "class Embedding(base.Layer):\n",
    "  \"\"\"Trainable layer that maps discrete tokens/IDs to vectors.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               d_feature,\n",
    "               kernel_initializer=init.RandomNormalInitializer(1.0)):\n",
    "    \"\"\"Returns an embedding layer with given vocabulary size and vector size.\n",
    "\n",
    "    Args:\n",
    "      vocab_size: Size of the input vocabulary. The layer will assign a unique\n",
    "          vector to each id in `range(vocab_size)`.\n",
    "      d_feature: Dimensionality/depth of the output vectors.\n",
    "      kernel_initializer: Function that creates (random) initial vectors for\n",
    "          the embedding.\n",
    "    \"\"\"\n",
    "    super().__init__(name=f'Embedding_{vocab_size}_{d_feature}')\n",
    "    self._d_feature = d_feature  # feature dimensionality\n",
    "    self._vocab_size = vocab_size\n",
    "    self._kernel_initializer = kernel_initializer\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"Returns embedding vectors corresponding to input token IDs.\n",
    "\n",
    "    Args:\n",
    "      x: Tensor of token IDs.\n",
    "\n",
    "    Returns:\n",
    "      Tensor of embedding vectors.\n",
    "    \"\"\"\n",
    "    return jnp.take(self.weights, x, axis=0, mode='clip')\n",
    "\n",
    "  def init_weights_and_state(self, input_signature):\n",
    "    \"\"\"Randomly initializes this layer's weights.\"\"\"\n",
    "    del input_signature\n",
    "    shape_w = (self._vocab_size, self._d_feature)\n",
    "    w = self._kernel_initializer(shape_w, self.rng)\n",
    "    self.weights = w\n",
    "```\n",
    "\n",
    "Layers with trainable weights like `Embedding` need to be initialized with the signature (shape and dtype) of the input, and then can be run by calling them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:16:05.891281Z",
     "start_time": "2021-06-04T00:16:05.879948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapeDtype{shape:(15,), dtype:int64}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an input tensor x.\n",
    "x = np.arange(15)\n",
    "print(f'x = {x}')\n",
    "trax.shapes.signature(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:13:28.571707Z",
     "start_time": "2021-06-04T00:13:28.081635Z"
    },
    "id": "4MLSQsIiw9Aw",
    "outputId": "394efc9d-9e3c-4f8c-80c2-ce3b5a935e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "shape of y = (15, 32)\n",
      "[[ 3.77153456e-02 -8.38692337e-02 -1.30958632e-01  6.61486685e-02\n",
      "   9.47625935e-02  2.40798593e-02  3.26354504e-02  3.00718844e-02\n",
      "  -2.87488312e-01 -2.11386114e-01  8.85490775e-02  1.26374930e-01\n",
      "   2.74707884e-01 -2.09884346e-02 -2.16003776e-01 -1.20634735e-02\n",
      "   1.28280312e-01  4.87237275e-02 -9.04452801e-03 -2.85806984e-01\n",
      "  -1.27055526e-01 -2.28640512e-01  1.68221384e-01  2.83406407e-01\n",
      "  -2.86621362e-01 -2.25466311e-01  3.05967182e-01 -2.75801718e-02\n",
      "   1.17485523e-02 -1.12152100e-01  2.18897790e-01  4.13620472e-03]\n",
      " [ 1.81800038e-01  2.45238870e-01 -5.94295561e-02 -2.50973761e-01\n",
      "  -2.52144992e-01  1.86799407e-01  4.47009802e-02  2.27686614e-01\n",
      "  -1.23269275e-01  7.57014751e-03  1.82228774e-01 -1.69095993e-01\n",
      "  -1.84106424e-01  4.79276776e-02 -2.34883964e-01  1.16671562e-01\n",
      "   4.39126492e-02  1.33128166e-02  1.35392040e-01 -2.88648456e-01\n",
      "  -4.83144224e-02  2.65146405e-01  1.99378580e-01  8.84759426e-02\n",
      "  -2.31014192e-01 -1.13331363e-01 -2.04302952e-01 -1.71948627e-01\n",
      "   1.58361912e-01 -3.05179983e-01  8.76531005e-03  1.17622554e-01]\n",
      " [-9.87273157e-02  1.75056398e-01 -2.90532649e-01 -1.49222270e-01\n",
      "   2.59748787e-01 -2.89675415e-01  4.33501601e-02  2.97877163e-01\n",
      "  -1.68836400e-01 -2.41482556e-02 -2.20380291e-01 -2.96198875e-01\n",
      "   1.39122874e-01 -1.61564887e-01 -4.12293673e-02  7.14803338e-02\n",
      "  -2.37869829e-01 -2.25470066e-02 -1.51616752e-01 -1.78547651e-01\n",
      "   1.61379397e-01  1.32311761e-01  2.31403738e-01 -1.61470875e-01\n",
      "  -2.73493826e-01  9.16835070e-02  2.53994167e-02  1.25439227e-01\n",
      "   1.53584182e-02  8.07594955e-02  2.67612934e-03 -1.98531240e-01]\n",
      " [ 2.13843316e-01  1.56739622e-01 -4.36309278e-02  2.17977017e-01\n",
      "  -6.53119981e-02 -2.71113575e-01 -1.59600228e-01  1.28477126e-01\n",
      "   1.66373879e-01  2.91466802e-01  3.00328940e-01  2.12800533e-01\n",
      "   2.32499748e-01 -1.79310441e-02 -1.86867952e-01  1.80468261e-02\n",
      "   2.32955486e-01  3.52688730e-02 -1.99782252e-01 -2.60881901e-01\n",
      "   2.32884556e-01 -7.22786635e-02  2.08304852e-01  1.38812184e-01\n",
      "  -3.77879739e-02  1.09712064e-01  3.97508740e-02  3.04619998e-01\n",
      "   1.34841323e-01  1.19391084e-03  1.85908973e-02 -9.75953043e-02]\n",
      " [-2.57135302e-01  1.01023912e-03  6.26157224e-02  1.73255116e-01\n",
      "  -1.59373716e-01  4.58244383e-02 -1.14329860e-01  3.91693711e-02\n",
      "  -1.23381406e-01  1.34577125e-01 -3.52123082e-02 -1.81578845e-01\n",
      "   2.03476578e-01 -1.54557794e-01  2.16623336e-01 -1.58963516e-01\n",
      "  -2.68189996e-01  1.34634078e-01  6.01586699e-02 -2.57514834e-01\n",
      "  -1.49850249e-02 -1.87672421e-01 -4.47684228e-02 -9.72920656e-03\n",
      "  -2.06951112e-01  1.57508254e-01 -1.65138334e-01 -2.68189996e-01\n",
      "   8.91536772e-02 -2.25466236e-01 -2.47138917e-01 -1.83291584e-01]\n",
      " [ 1.96753174e-01  5.69522381e-05  2.20639735e-01  4.04930115e-02\n",
      "   1.54647738e-01  1.57456875e-01  2.49570042e-01 -8.22267234e-02\n",
      "   2.37210780e-01 -8.64943266e-02 -1.62372425e-01 -4.07452285e-02\n",
      "   2.23793060e-01 -2.03896165e-02  3.03736031e-02  2.67550260e-01\n",
      "   2.31608123e-01 -1.84379071e-01 -1.72918439e-02  2.09263057e-01\n",
      "  -1.43493190e-01  9.05888677e-02  1.03197724e-01 -1.18592083e-02\n",
      "  -3.19793820e-02  8.82307887e-02  1.57504618e-01 -6.14764839e-02\n",
      "  -1.77989900e-02 -2.89201647e-01 -1.75361186e-01 -1.38921529e-01]\n",
      " [-2.92117059e-01  8.25769901e-02  1.70788199e-01  2.15317994e-01\n",
      "  -2.45502532e-01  4.96059656e-03 -1.71914697e-03  3.35309505e-02\n",
      "   5.17880023e-02  2.81415552e-01 -2.40538135e-01 -2.21022248e-01\n",
      "  -1.83975607e-01  2.36638159e-01 -2.16209844e-01  1.13999844e-02\n",
      "   2.37316936e-01  2.40658969e-01 -1.69669405e-01  8.76724720e-04\n",
      "   1.12115324e-01 -7.35276192e-02 -2.34674364e-01 -2.51807809e-01\n",
      "   2.22231954e-01 -1.93520486e-01 -1.05464026e-01 -1.33338019e-01\n",
      "  -4.57122922e-03  1.33813471e-01 -2.48589143e-01 -4.42560315e-02]\n",
      " [-1.84660569e-01 -2.30279952e-01  6.08321130e-02  1.96042061e-02\n",
      "  -2.29511619e-01  2.83597022e-01 -2.41270766e-01  1.83048844e-03\n",
      "  -2.80393273e-01 -1.98856756e-01 -1.31163776e-01 -2.06365660e-01\n",
      "   2.85503566e-02  9.19411778e-02  2.14025110e-01 -2.84858614e-01\n",
      "  -8.61696303e-02 -7.19455630e-02 -1.30532026e-01  1.09806657e-03\n",
      "   2.57533640e-01  8.47366452e-03 -2.28881776e-01  2.89058775e-01\n",
      "   1.32914245e-01  2.77662247e-01 -1.17410108e-01 -1.38379350e-01\n",
      "   8.84874761e-02 -4.01993990e-02  7.84773529e-02  5.52803278e-03]\n",
      " [-1.48164928e-01  5.41357696e-02 -1.27683550e-01  1.32082850e-01\n",
      "   2.01341718e-01  1.78259671e-01  2.85761446e-01  5.44367433e-02\n",
      "  -3.03892404e-01  2.32336611e-01  1.25814080e-02  2.76510626e-01\n",
      "  -2.22417086e-01 -4.97466326e-02 -1.19098395e-01  2.22901851e-01\n",
      "  -1.61629274e-01  7.29657412e-02  3.50220501e-02  7.47849941e-02\n",
      "   1.07183337e-01  2.48703331e-01 -1.62492141e-01 -7.13672489e-02\n",
      "   1.66146100e-01 -2.45657444e-01 -1.91788405e-01 -2.23979294e-01\n",
      "   4.75468338e-02 -8.09504539e-02 -2.91870832e-01 -1.25345409e-01]\n",
      " [ 3.84767950e-02 -1.03706717e-02 -4.70472872e-02  5.84567487e-02\n",
      "   1.45424783e-01  2.58001834e-01 -2.93507725e-01 -1.56415507e-01\n",
      "   2.48369575e-03 -3.67023051e-02  2.29573160e-01  1.44543886e-01\n",
      "  -6.68412149e-02 -2.58147448e-01  1.47014141e-01  2.40838140e-01\n",
      "   2.06323832e-01 -1.13951862e-01 -2.55722880e-01  2.76621580e-02\n",
      "   2.97089130e-01  9.09104347e-02 -4.87200022e-02 -5.92913628e-02\n",
      "   1.90908760e-01 -1.07317284e-01 -8.95580947e-02  2.23909765e-01\n",
      "   3.05320233e-01 -6.78579658e-02  2.37573713e-01  1.04451209e-01]\n",
      " [ 1.29242390e-01 -2.03944236e-01 -1.74100384e-01 -5.88944554e-03\n",
      "   2.27184743e-01 -1.97283000e-01 -2.08615065e-02  1.12321407e-01\n",
      "  -9.66301560e-03 -2.72967637e-01  2.07997054e-01  2.23492593e-01\n",
      "   2.70566374e-01 -1.14018947e-01 -3.04353416e-01  5.51645756e-02\n",
      "  -2.79875696e-02  1.47552729e-01  2.09094495e-01  9.60547626e-02\n",
      "   2.42856413e-01  1.53504044e-01  1.81796402e-01 -1.25200644e-01\n",
      "   1.14138603e-01 -1.62605077e-01 -8.31895322e-02 -2.49913588e-01\n",
      "  -2.34524727e-01  2.93612033e-01  2.44970351e-01  8.10627937e-02]\n",
      " [-1.07866839e-01  6.94503188e-02 -5.26982546e-02 -1.00349531e-01\n",
      "   2.00209409e-01  1.86232209e-01 -1.56296596e-01  2.08206087e-01\n",
      "  -8.77147615e-02 -2.43540704e-02 -2.99099207e-01  2.41074562e-02\n",
      "  -1.62185326e-01 -1.08669400e-01 -2.11416274e-01  2.33026296e-01\n",
      "   4.59035039e-02  1.38867378e-01  1.32100075e-01 -2.09107265e-01\n",
      "  -2.80362368e-01  2.93473572e-01  3.63322794e-02  1.97013289e-01\n",
      "   1.14258021e-01 -2.91075140e-01 -1.41215131e-01 -6.45010322e-02\n",
      "  -3.21341455e-02 -8.86455774e-02 -1.25025809e-01  5.34527004e-02]\n",
      " [-6.49996251e-02 -5.92093021e-02 -3.92322838e-02 -1.05756164e-01\n",
      "   5.69346845e-02 -7.25638866e-03 -2.23944902e-01 -4.37026918e-02\n",
      "  -1.39536187e-01  8.89039934e-02 -1.00799516e-01 -1.71560705e-01\n",
      "  -2.99136221e-01  2.74290651e-01 -1.20293409e-01 -1.65219367e-01\n",
      "  -2.31048584e-01  1.11828566e-01  2.89437383e-01  2.23232836e-01\n",
      "   9.35952365e-02 -2.71258414e-01  1.24691695e-01 -2.56478161e-01\n",
      "  -1.75510466e-01 -4.89598513e-03  1.45405799e-01  2.95711905e-01\n",
      "   2.08556652e-04  2.88742632e-01  1.02654964e-01 -1.02165043e-02]\n",
      " [-1.79324821e-01  1.35477006e-01  1.68571055e-01  9.57559049e-02\n",
      "  -1.02465749e-01  5.62680364e-02  1.82342231e-01  1.69882417e-01\n",
      "   1.79241747e-01  4.04629409e-02  2.43976682e-01  1.37004405e-01\n",
      "  -1.62973806e-01  6.73099458e-02  4.71390486e-02 -2.56380260e-01\n",
      "  -1.15514800e-01  2.63726979e-01 -5.87962717e-02  1.52136743e-01\n",
      "  -1.84703350e-01 -7.49287903e-02  2.85769969e-01 -2.14721501e-01\n",
      "  -1.52057752e-01  1.69577301e-02  8.07283223e-02  2.88864523e-01\n",
      "  -1.21146277e-01  2.88563967e-03 -7.13808984e-02 -2.08149254e-02]\n",
      " [-2.27568358e-01  2.95210391e-01  2.10793287e-01 -2.60990679e-01\n",
      "  -9.75090265e-03 -6.96649402e-02 -2.53305614e-01 -1.54188052e-01\n",
      "  -3.36389244e-02 -1.41383186e-01  1.64371550e-01 -6.76547289e-02\n",
      "  -1.49798304e-01 -2.41516858e-01  1.31609291e-01  4.76193130e-02\n",
      "   2.25330621e-01  2.53756136e-01 -1.94180012e-03 -2.42922619e-01\n",
      "  -7.72279501e-02  1.30682111e-01 -2.46107563e-01 -2.33889312e-01\n",
      "  -2.62016624e-01  1.58562303e-01 -2.50006914e-02  5.25984466e-02\n",
      "   1.45036042e-01  2.17669100e-01  9.27729011e-02  2.89358884e-01]]\n"
     ]
    }
   ],
   "source": [
    "from trax import layers as tl\n",
    "\n",
    "# Create an input tensor x.\n",
    "x = np.arange(15)\n",
    "print(f'x = {x}')\n",
    "\n",
    "# Create the embedding layer.\n",
    "embedding = tl.Embedding(vocab_size=20, d_feature=32)\n",
    "embedding.init(trax.shapes.signature(x))\n",
    "\n",
    "# Run the layer -- y = embedding(x).\n",
    "y = embedding(x)\n",
    "print(f'shape of y = {y.shape}')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgCPl9ZOyCJw"
   },
   "source": [
    "### Models\n",
    "\n",
    "Models in Trax are built from layers most often using the `Serial` and `Branch` combinators. You can read more about those combinators in the [layers intro](https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html) and\n",
    "see the code for many models in `trax/models/`, e.g., this is how the [Transformer Language Model](https://github.com/google/trax/blob/master/trax/models/transformer.py#L167) is implemented. Below is an example of how to build a sentiment classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:16:11.142954Z",
     "start_time": "2021-06-04T00:16:11.133287Z"
    },
    "id": "WoSz5plIyXOU",
    "outputId": "f94c84c4-3224-4231-8879-4a68f328b89e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Embedding_8192_256\n",
      "  Mean\n",
      "  Dense_2\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = tl.Serial(\n",
    "    tl.Embedding(vocab_size=8192, d_feature=256),\n",
    "    tl.Mean(axis=1),  # Average on axis 1 (length of sentence).\n",
    "    tl.Dense(2),      # Classify 2 classes.\n",
    ")\n",
    "\n",
    "# You can print model structure.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcnIjFLD0Ju1"
   },
   "source": [
    "### Data\n",
    "\n",
    "To train your model, you need data. In Trax, data streams are represented as python iterators, so you can call `next(data_stream)` and get a tuple, e.g., `(inputs, targets)`. Trax allows you to use [TensorFlow Datasets](https://www.tensorflow.org/datasets) easily and you can also get an iterator from your own text file using the standard `open('my_file.txt')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:16:38.049211Z",
     "start_time": "2021-06-04T00:16:37.687176Z"
    },
    "id": "pKITF1jR0_Of",
    "outputId": "44a73b25-668d-4f85-9133-ebb0f5edd191"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:304: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
      "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n",
      "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:317: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", 0)\n"
     ]
    }
   ],
   "source": [
    "train_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=True)()\n",
    "eval_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=False)()\n",
    "print(next(train_stream))  # See one example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRGj4Skm1kL4"
   },
   "source": [
    "Using the `trax.data` module you can create input processing pipelines, e.g., to tokenize and shuffle your data. You create data pipelines using `trax.data.Serial` and they are functions that you apply to streams to create processed streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:19:14.664345Z",
     "start_time": "2021-06-04T00:19:11.186751Z"
    },
    "id": "AV5wrgjZ10yU",
    "outputId": "82b8e3bc-7812-4cd3-a669-401fef29f1c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes = [(8, 1024), (8,), (8,)]\n"
     ]
    }
   ],
   "source": [
    "data_pipeline = trax.data.Serial(\n",
    "    trax.data.Tokenize(vocab_file='en_8k.subword', keys=[0]),\n",
    "    trax.data.Shuffle(),\n",
    "    trax.data.FilterByLength(max_length=2048, length_keys=[0]),\n",
    "    trax.data.BucketByLength(boundaries=[  32, 128, 512, 2048],\n",
    "                             batch_sizes=[512, 128,  32,    8, 1],\n",
    "                             length_keys=[0]),\n",
    "    trax.data.AddLossWeights()\n",
    "  )\n",
    "train_batches_stream = data_pipeline(train_stream)\n",
    "eval_batches_stream = data_pipeline(eval_stream)\n",
    "example_batch = next(train_batches_stream)\n",
    "print(f'shapes = {[x.shape for x in example_batch]}')  # Check the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:19:38.248517Z",
     "start_time": "2021-06-04T00:19:38.237880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 182,   25,  118, ...,    0,    0,    0],\n",
       "        [3243,    6, 1745, ...,    0,    0,    0],\n",
       "        [ 139, 3304,   20, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8180,    2, 1226, ...,    0,    0,    0],\n",
       "        [1158,   82,  377, ...,    0,    0,    0],\n",
       "        [8180,    2,   28, ...,    0,    0,    0]]),\n",
       " array([0, 0, 1, 0, 1, 1, 1, 1]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l25krioP2twf"
   },
   "source": [
    "### Supervised training\n",
    "\n",
    "When you have the model and the data, use `trax.supervised.training` to define training and eval tasks and create a training loop. The Trax training loop optimizes training and will create TensorBoard logs and model checkpoints for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:22:43.082801Z",
     "start_time": "2021-06-04T00:21:30.726286Z"
    },
    "id": "d6bIKUO-3Cw8",
    "outputId": "038e6ad5-0d2f-442b-ffa1-ed431dc1d2e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:304: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
      "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n",
      "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:317: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 2097666\n",
      "Step      1: Ran 1 train steps in 1.54 secs\n",
      "Step      1: train WeightedCategoryCrossEntropy |  0.69666946\n",
      "Step      1: eval  WeightedCategoryCrossEntropy |  0.69222179\n",
      "Step      1: eval      WeightedCategoryAccuracy |  0.52500000\n",
      "\n",
      "Step    500: Ran 499 train steps in 17.58 secs\n",
      "Step    500: train WeightedCategoryCrossEntropy |  0.49034482\n",
      "Step    500: eval  WeightedCategoryCrossEntropy |  0.42457187\n",
      "Step    500: eval      WeightedCategoryAccuracy |  0.81562500\n",
      "\n",
      "Step   1000: Ran 500 train steps in 14.61 secs\n",
      "Step   1000: train WeightedCategoryCrossEntropy |  0.35335931\n",
      "Step   1000: eval  WeightedCategoryCrossEntropy |  0.40213267\n",
      "Step   1000: eval      WeightedCategoryAccuracy |  0.83125000\n",
      "\n",
      "Step   1500: Ran 500 train steps in 14.49 secs\n",
      "Step   1500: train WeightedCategoryCrossEntropy |  0.34328619\n",
      "Step   1500: eval  WeightedCategoryCrossEntropy |  0.33752164\n",
      "Step   1500: eval      WeightedCategoryAccuracy |  0.85468750\n",
      "\n",
      "Step   2000: Ran 500 train steps in 14.41 secs\n",
      "Step   2000: train WeightedCategoryCrossEntropy |  0.28964674\n",
      "Step   2000: eval  WeightedCategoryCrossEntropy |  0.46175266\n",
      "Step   2000: eval      WeightedCategoryAccuracy |  0.82187500\n"
     ]
    }
   ],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "# Training task.\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=train_batches_stream,\n",
    "    loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
    "    optimizer=trax.optimizers.Adam(0.01),\n",
    "    n_steps_per_checkpoint=500,\n",
    ")\n",
    "\n",
    "# Evaluaton task.\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=eval_batches_stream,\n",
    "    metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()],\n",
    "    n_eval_batches=20  # For less variance in eval numbers.\n",
    ")\n",
    "\n",
    "# Training loop saves checkpoints to output_dir.\n",
    "output_dir = os.path.expanduser('~/output_dir/')\n",
    "!rm -rf {output_dir}\n",
    "training_loop = training.Loop(model,\n",
    "                              train_task,\n",
    "                              eval_tasks=[eval_task],\n",
    "                              output_dir=output_dir)\n",
    "\n",
    "# Run 2000 steps (batches).\n",
    "training_loop.run(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aCkIu3x686C"
   },
   "source": [
    "After training the model, run it like any layer to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T00:25:58.036670Z",
     "start_time": "2021-06-04T00:25:55.694982Z"
    },
    "id": "yuPu37Lp7GST",
    "outputId": "fdc4d832-2f1d-4aee-87b5-9c9dc1238503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example input_str: What has to change in today's attitude towards films like Boogie Nights is the approach. The approach is awful! Comparing it to Pulp Fiction, seeing only the pornography, and all its aspects.. come on people, there is more than that in this beautiful motion picture. And to all the sceptics, hasn't Paul Thomas Anderson proved himself worthy time and time again? Magnolia is one of the main reasons I watch American films at all and still have faith in this \"Industry\" that film-making is today.. And what about There Will Be Blood? That is a film that will stay in film history whether u like it or not! Yeah, you! The so-called consumer.. you know something: F#*k you! you don't deserve this, you don't deserve anything. So many artists today struggle to get recognition and it has become increasingly difficult to make serious films, even mainstream, because people just wanna see celebrities doing stupid stuff.. like that sell-out Britney spears.<br /><br />Anyway, this was very painful for me to say because I don't want to see this, i don't wanna believe that today all it matters is the adding up of numbers.. sales revenue and sales return.. I want to see magic, the magic that Fellini, Bergman and Kurosawa brought and created through the language of cinema.. Because thats what PTA is doing.. he is creating magic!!<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Model returned sentiment probabilities: [[2.6056889e-02 3.6872948e+01]]\n"
     ]
    }
   ],
   "source": [
    "example_input = next(eval_batches_stream)[0][0]\n",
    "example_input_str = trax.data.detokenize(example_input, vocab_file='en_8k.subword')\n",
    "print(f'example input_str: {example_input_str}')\n",
    "sentiment_log_probs = model(example_input[None, :])  # Add batch dimension.\n",
    "print(f'Model returned sentiment probabilities: {np.exp(sentiment_log_probs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Trax Quick Intro",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
