{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pytorch简介\n",
    "--------------\n",
    "\n",
    "PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。\n",
    "它主要由Facebook的人工智能研究小组开发。PyTorch和Google的Tensorflow是目前最火的的两个深度学习框架。\n",
    "\n",
    "PyTorch和Torch都使用相同的C库：TH, THC, THNN,\n",
    "THCUNN。Torch使用LUA作为上层包装语言，而PyTorch使用Python。\n",
    "\n",
    "PyTorch是一个Python包，提供两个高级功能：\n",
    "\n",
    "-   具有强大的GPU加速的张量计算（如NumPy）\n",
    "-   包含自动求导系统的的深度神经网络\n",
    "\n",
    "### 1.1 安装\n",
    "\n",
    "``` shell\n",
    "pip install -U torch torchvision\n",
    "```\n",
    "\n",
    "检查安装\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "```\n",
    "\n",
    "### 1.2 Tensor\n",
    "\n",
    "Tensor和 NumPy ndarray非常相似，不同的是，Tensor可以使用GPU来加速计算。\n",
    "\n",
    "#### 创建Tensor\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "t = torch.tensor([10, 20, 30, 40, 50], dtype=torch.float) \n",
    "print(\"tensor={} \\ntensor.dtype={} \\ntensor.type={} \\ntype={}\".format(t, t.dtype, t.type(), type(t)))\n",
    "print(\"-\"*50)\n",
    "\n",
    "a = np.array([10, 20, 30, 40, 50], dtype=np.float) \n",
    "print(\"array={} \\narray.dtype={} \\narray.type={}\".format(a, a.dtype, type(a)))      \n",
    "print(\"-\"*50)\n",
    "\n",
    "t = torch.rand(5, 3, dtype=torch.double)\n",
    "print(\"tensor={} \\ntensor.dtype={} \\ntensor.type={} \\ntype={}\".format(t, t.dtype, t.type(), type(t)))\n",
    "print(\"-\"*50)\n",
    "\n",
    "a = np.random.rand(5, 3)\n",
    "print(\"array={} \\narray.dtype={} \\narray.type={}\".format(a, a.dtype, type(a)))\n",
    "print(\"-\"*50)\n",
    "\n",
    "```\n",
    "\n",
    "#### CUDA张量\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([10, 20, 30], dtype=torch.double) \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA 设备对象\n",
    "    y = torch.ones_like(x, device=device)  # 直接从GPU创建张量\n",
    "    x = x.to(device)                       # 将张量移动到cuda中\n",
    "    z = torch.rand(3).cuda()\n",
    "    s = x + y + z\n",
    "    print(s)\n",
    "    print(s.to(\"cpu\", torch.double))       # 对变量的类型做更改\n",
    "```\n",
    "\n",
    "#### Tensor vs. tensor\n",
    "\n",
    "创建Tensor有两种方法。它们有一些细微差别。其中torch.Tensor是python类，是默认张量类型torch.FloatTensor()的别名。而torch.tensor()是python函数。\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "\n",
    "t1 = torch.Tensor([1, 2])\n",
    "print(\"tensor={} \\ntensor.dtype={} \\ntensor.type={} \\ntype={}\".format(t1, t1.dtype, t1.type(), type(t1)))\n",
    "print(\"-\"*50)\n",
    "\n",
    "t2 = torch.tensor([1, 2])\n",
    "print(\"tensor={} \\ntensor.dtype={} \\ntensor.type={} \\ntype={}\".format(t2, t2.dtype, t2.type(), type(t2)))\n",
    "print(\"-\"*50)\n",
    "\n",
    "t2 = torch.tensor([1., 2.])\n",
    "print(\"tensor={} \\ntensor.dtype={} \\ntensor.type={} \\ntype={}\".format(t2, t2.dtype, t2.type(), type(t2)))\n",
    "print(\"-\"*50)\n",
    "```\n",
    "\n",
    "结果如下\n",
    "\n",
    "    tensor=tensor([1., 2.]) \n",
    "    tensor.dtype=torch.float32 \n",
    "    tensor.type=torch.FloatTensor \n",
    "    type=<class 'torch.Tensor'>\n",
    "    --------------------------------------------------\n",
    "    tensor=tensor([1, 2]) \n",
    "    tensor.dtype=torch.int64 \n",
    "    tensor.type=torch.LongTensor \n",
    "    type=<class 'torch.Tensor'>\n",
    "    --------------------------------------------------\n",
    "    tensor=tensor([1., 2.]) \n",
    "    tensor.dtype=torch.float32 \n",
    "    tensor.type=torch.FloatTensor \n",
    "    type=<class 'torch.Tensor'>\n",
    "    --------------------------------------------------\n",
    "\n",
    "#### numpy vs. torch\n",
    "\n",
    "| 操作                               | numpy                      | torch                              |\n",
    "|------------------------------------|:---------------------------|------------------------------------|\n",
    "| empty                              | np.empty(\\[5, 3\\])         | torch.empty(5, 3)                  |\n",
    "| rand 均匀分布                      | np.random.rand(5, 3)       | torch.rand(5, 3, dtype=torch.long) |\n",
    "| randn 正态分布                     | np.random.randn(5, 3)      | torch.randn(5, 3)                  |\n",
    "| zeros                              | np.zeros(\\[5, 3\\])         | torch.zeros(5, 3)                  |\n",
    "| ones                               | np.ones(\\[5, 3\\])          | torch.ones(5, 3)                   |\n",
    "| construct from data                | np.array(\\[5.5, 3\\])       | torch.Tensor(\\[5.5, 3\\])           |\n",
    "| contruct: 使用原对象的shape和dtype | np.random.randn(\\*x.shape) | torch.randn\\_like(x)               |\n",
    "| shape                              | x.shape                    | x.size() or x.shape                |\n",
    "| 加法/减法                          | x + y，x - y               | x + y，x - y                       |\n",
    "| 点乘                               | x \\* y                     | x \\* y                             |\n",
    "| 乘法（叉乘）                       | x.dot(y) or x @ y          | x.mm(y) or x @ y                   |\n",
    "| adds x to y                        |                            | y.add\\_(x)                         |\n",
    "| 获取部分数据                       | x\\[:, 1\\]                  | x\\[:, 1\\]                          |\n",
    "| reshape                            | x.reshape((-1, 3))         | x.view(-1, 4) or x.reshape(-1, 3)  |\n",
    "| 转置                               | x.T                        | x.t() or x.T                       |\n",
    "| 向量长度（范数）                   | np.linalg.norm(x)          | x.norm()                           |\n",
    "| 逆矩阵                             | np.linalg.inv(A)           | torch.inverse(A)B                  |\n",
    "| 垂直方向拼接(行增)                 | np.vstack(A, B)            | torch.cat((A, B), 0)               |\n",
    "| 水平方向拼接(列增)                 | np.hstack(A, B)            | torch.cat((A, B), 1)               |\n",
    "\n",
    "**array和tensor之前的转换**\n",
    "\n",
    "无论如何转化，这些对象共享内存。\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "numpy_x = np.array([0.0, 1.0, 2.0, 3.0, 4.0])\n",
    "tensor_x = torch.from_numpy(numpy_x)\n",
    "print(\"numpy_x = {}\".format(numpy_x))\n",
    "print(\"tensor_x = {}\".format(tensor_x))\n",
    "\n",
    "numpy_x_from_tensor = tensor_x.numpy()\n",
    "print(\"numpy_x_from_tensor = {}\".format(numpy_x_from_tensor))\n",
    "\n",
    "numpy_x_from_tensor[1] = 98\n",
    "print(\"-\"*50)\n",
    "print(\"after change the value of one element.\")\n",
    "print(\"numpy_x = {}\".format(numpy_x))\n",
    "print(\"tensor_x = {}\".format(tensor_x))\n",
    "print(\"numpy_x_from_tensor = {}\".format(numpy_x_from_tensor))\n",
    "```\n",
    "\n",
    "#### view vs. reshape\n",
    "\n",
    "view():\n",
    "\n",
    "-   返回具有新形状的张量.返回的张量将与原始张量共享数据.\n",
    "\n",
    "reshape():\n",
    "\n",
    "-   优先执行view()，如果不满足条件，则clone一个。\n",
    "\n",
    "> 什么条件\n",
    "\n",
    "### 1.3 Autograd：自动求导\n",
    "\n",
    "PyTorch 中神经网络的核心是 `autograd` 包。\n",
    "`autograd`包为张量上的所有操作提供了自动求导。\n",
    "它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。下面是张量自动求导的几个重要属性。\n",
    "\n",
    "-   requires\\_grad：当requires\\_grad`为`True，在完成（Forward）计算后，并调用backward()后，则自动计算所有的梯度。\n",
    "-   grad：梯度\n",
    "-   grad\\_fn：梯度函数\n",
    "\n",
    "下面是一个例子。\n",
    "\n",
    "计算公式： $$\n",
    "\\mathbf {y} = 2 \\mathbf x + 1 \\\\\n",
    "\\mathbf {z} = \\mathbf y^2 + 1 \\\\\n",
    "\\mathbf {out} = \\frac 1 4  \\sum \\mathbf  {z}\n",
    "$$ 可以得到： $$\n",
    "\\mathbf {out} = \\frac 1 4  \\sum   {(2 \\mathbf x+1)^2+1}\n",
    "$$ 梯度计算公式： $$\n",
    "\\frac{\\partial \\mathbf {out} }{\\partial \\mathbf {x} } = \\mathbf y =  \\begin{bmatrix}\n",
    "3 & 5 \\\\\n",
    "7 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "``` python\n",
    "import torch\n",
    "\n",
    "def compute():\n",
    "    x = torch.tensor([[1, 2],[3, 4]], dtype=torch.double, requires_grad=True)\n",
    "    y = 2 * x + 1\n",
    "    z = y * y + 1\n",
    "    out = z.mean()\n",
    "    return {'x':x, 'y':y, 'z':z, 'out':out}\n",
    "\n",
    "def print_tensor(key, tensor):\n",
    "    print(\"{}={}\".format(key, tensor))\n",
    "    print(\"  grad_fn={} \\n  grad={} \\n  requires_grad={}\\n  is_leaf={} \\n  dtype={}\".format(\n",
    "        tensor.grad_fn, tensor.grad, \n",
    "        tensor.requires_grad, tensor.is_leaf, tensor.dtype))\n",
    "    print(\"-\"*50)    \n",
    "\n",
    "def print_tensors(tensors):\n",
    "    for key, tensor in tensors.items():\n",
    "        print_tensor(key, tensor)\n",
    "\n",
    "tensors = compute()\n",
    "tensors['out'].backward()\n",
    "print_tensors(tensors)\n",
    "```\n",
    "\n",
    "默认情况下，仅leaf的tensor才会保留grad。可以手工设置打开。\n",
    "\n",
    "``` python\n",
    "tensors = compute()\n",
    "for _, tensor in tensors.items():\n",
    "    tensor.retain_grad()\n",
    "tensors['out'].backward()\n",
    "print_tensors(tensors)\n",
    "```\n",
    "\n",
    "上面例子中backward的tensor是一个标量，所以其导数肯定为1。如果它是一个维度更高的tensor，则可以指定一个相同维度导数（可以看成是下一步梯度计算带来的），然后进行反向传播计算。\n",
    "\n",
    "``` python\n",
    "tensors = compute()\n",
    "gradients = torch.tensor([[0.5, 1], [2, 4]], dtype=torch.float)\n",
    "tensors['z'].backward(gradients)\n",
    "print_tensors(tensors)\n",
    "```\n",
    "\n",
    "梯度计算公式: $$\n",
    "\\frac{\\partial \\mathbf {z} }{\\partial \\mathbf {x} } \\circ  \\mathbf {gradients} = 4\\mathbf y  \\circ \\begin{bmatrix}\n",
    "0.5 & 1 \\\\\n",
    "2 & 4 \n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "6 & 20 \\\\\n",
    "56 & 144\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> 自动求导思路很简单，但是异常强大。从细微入手，把复杂的计算图分解到一层一层中来进行求导，每一层都非常简单，但组合起来千变万化。真如如庖丁解牛。\n",
    "\n",
    "其它\n",
    "----\n",
    "\n",
    "#### torch.nn.Conv2d\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html?highlight=conv2d\\#torch.nn.Conv2d\n",
    "\n",
    "    CLASStorch.nn.Conv2d(\n",
    "      in_channels,  # 输入通道数\n",
    "      out_channels, # 输出通道数\n",
    "      kernel_size,  # 卷积核大小\n",
    "      stride=1,     # 滑动窗口，默认为1，指每次卷积对原数据滑动1个单元格\n",
    "      padding=0,    # 两边的空白填充（一般补0）\n",
    "      dilation=1,   # 卷积核内部之间的间隔，默认1\n",
    "      groups=1,     # 将输入数据分组，通常不用管这个参数，没有太大意义\n",
    "      bias=True, \n",
    "      padding_mode='zeros'\n",
    "    )\n",
    "\n",
    "-   input size: $(N,C_{in}, H_{in},W_{in})$\n",
    "\n",
    "    -   $N$：batch size。也就是每次训练所使用的样本个数\n",
    "    -   $C_{in}$： 输入channel数量\n",
    "    -   $H_{in}$：输入图片的(像素)高度\n",
    "    -   $W_{in}$： 输入图性的(像素)宽度\n",
    "\n",
    "-   output size: $(N,C_{out}, H_{out},W_{out})$\n",
    "\n",
    "    -   $N$：batch size。也就是每次训练所使用的样本个数\n",
    "    -   $C_{out}$： 输出channel数量\n",
    "    -   $H_{out}$：输出图片的(像素)高度\n",
    "    -   $W_{out}$：输出图片的(像素)宽度\n",
    "\n",
    "-   dilation：卷积核内部之间的间隔，默认为1，其卷积核如下图所示：\n",
    "\n",
    "![image-20200113103126914](attachment:images/image-20200113103126914.png)\n",
    "\n",
    "​\n",
    "如果dilation=2，则如下图所示，dilation表示的是灰色格子之间的序号的间隔。\n",
    "\n",
    "![image-20200113102809015](attachment:images/image-20200113102809015.png)\n",
    "\n",
    "-   输出的size大小\n",
    "\n",
    "    设$$\\mathbf {S_{in}} = \\begin{bmatrix} H_{in} & W_{in} \\end{bmatrix}  $$，$$ \\mathbf {S_{out} }= \\begin{bmatrix} H_{out} & W_{out} \\end{bmatrix}  $$，则\n",
    "    $$\n",
    "    \\mathbf {S_{out}} = \\lfloor \\frac {\\mathbf { S_{in}} + 2\\mathbf p  - \\mathbf d \\circ (\\mathbf k -1 ) - 1} { \\mathbf s}  + 1 \\rfloor\n",
    "    $$ 其中$p$表示padding，$d$表示dilation，$k$表示kernal\n",
    "    size，$s$表示stride。\n",
    "\n",
    "    上面的公式中，可以这样逐步理解\n",
    "\n",
    "    1.  \\$d (k -1 ) - 1 \\$- 表示卷积核所占的空间，\n",
    "    2.  $\\mathbf { S_{in}} + 2\\mathbf p - \\mathbf d \\circ (\\mathbf k -1 ) - 1$表示减去一个卷积核所占空间\n",
    "    3.  $\\frac {\\mathbf { S_{in}} + 2\\mathbf p - \\mathbf d \\circ (\\mathbf k -1 ) - 1} { \\mathbf s}$\n",
    "        表示剩下空间可以容纳几个卷积核\n",
    "    4.  $+ 1$：表示把第2步减去的卷积核，再加回来。\n",
    "\n",
    "#### \n",
    "\n",
    "Resource\n",
    "--------\n",
    "\n",
    "-   [Deep-Learning-with-PyTorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)\n",
    "-   [pytorch handbook](https://github.com/zergtant/pytorch-handbook)\n",
    "-   [pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial)\n",
    "-   [动手学深度学习（PyTorch版）](https://tangshusen.me/Dive-into-DL-PyTorch/)"
   ],
   "attachments": {
    "images/image-20200113102809015.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAAEpCAYAAAANqZAGAAAFJElEQVR4nO3YscnrShRG0fFD3Uyg\nWK24QLfieECu53/RBXcwhr1WIpQd8cEO9Ljv+2+tNerO89x9wjb2t3/ZMecca63xfD5337LN6/Ua\nc87dZ2xjf/uX9z++X97v9647trmua/cJP8P+bdX9/9t9BMBOIgikiSCQJoJAmggCaSIIpIkgkCaC\nQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJp\nIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkg\nkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkgkHZ8v1zXtesOfoD926r7P+77\n/ltr7b5ju/M8d5+wjf3tX3bMOccYY/x7Fn0+n/T3j2H/8veP0d7fP0EgTQSBNBEE0kQQSBNBIE0E\ngTQRBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0EQTS\nRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNB\nIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSDtcd/3\n31pr9x3bnee5+4Rt7G//smPOOdZa4/l87r5lm9frNeacu8/Yxv72L+9/fL+83+9dd2xzXdfuE36G\n/duq+/snCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJp\nIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkg\nkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCa\nCAJpIgikiSCQJoJA2vH9cl3Xrjv4AfZvq+7/uO/7b621+47tzvPcfcI29rd/2THnHGOM8e9Z9Pl8\n0t8/hv3L3z9Ge3//BIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0EQTS\nRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNB\nIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0\nEQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgbTHfd9/a63dd2x3nufuE7axv/3LjjnnWGuN5/O5+5Zt\nXq/XmHPuPmMb+9u/vP/x/fJ+v3fdsc11XbtP+Bn2b6vu758gkCaCQJoIAmkiCKSJIJAmgkCaCAJp\nIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkg\nkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCa\nCAJpIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJpx/fLdV277uAH2L+tuv/j\nvu+/tdbuO7Y7z3P3CdvY3/5lx5xzjDHGv2fR5/NJf/8Y9i9//xjt/f8HTMyyxt7T3joAAAAASUVO\nRK5CYII=\n"
    },
    "images/image-20200113103126914.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMAAAACzCAYAAADfahNoAAAC2ElEQVR4nO3XoZHjWhRF0etfnYpR\nQyNFZzsYKxUhw0YORgOm6tdE0A/stZDgPWCXpMt5nudE/fz8zPf39+ozlrH/Zy6v1ysbwOfzmev1\nuvqMZer7Z2Yu5TfA8/mc+/2++oxl6vv3fZ+vmZnjOFbf8uu2bfv/2f7u/v8W3wFLCYA0AZAmANIE\nQJoASBMAaQIgTQCkCYA0AZAmANIEQJoASBMAaQIgTQCkCYA0AZAmANIEQJoASBMAaQIgTQCkCYA0\nAZAmANIEQJoASBMAaQIgTQCkCYA0AZAmANIEQJoASBMAaQIgTQCkCYA0AZAmANIEQJoASBMAaQIg\nTQCkCYA0AZAmANIEQJoASBMAaV8zM9u2rb5jKfu7+y+Px+NcfcQq7/d7brfb6jOWqe+fmbmc55kN\n4Pl8zv1+X33GMvX9+77//QQ6jmP1Lb/u39e+/d39foJJEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBp\nAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRA\nmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQBkCYA0gRAmgBIEwBpAiBNAKQJgDQB\nkCYA0r5mZrZtW33HUvZ3918ej8e5+ohV3u/33G631WcsU98/M3M5zzMbwPP5nPv9vvqMZer7933/\n+wl0HMfqW37dv699+7v7/QSTJgDSBECaAEgTAGkCIE0ApAmANAGQJgDSBECaAEgTAGkCIE0ApAmA\nNAGQJgDSBECaAEgTAGkCIE0ApAmANAGQJgDSBECaAEgTAGkCIE0ApAmANAGQJgDSBECaAEgTAGkC\nIE0ApAmANAGQJgDSBECaAEgTAGkCIE0ApAmANAGQJgDSBECaAEgTAGkCIE0ApH3NzGzbtvqOpezv\n7r+8Xq9z9RGrfD6fuV6vq89Ypr5/ZuYPD0d1jWGftIEAAAAASUVORK5CYII=\n"
    }
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}

