{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64, requires_grad=True)\n",
      "  grad_fn=None \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=True \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "y=tensor([[3., 5.],\n",
      "        [7., 9.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  grad_fn=<AddBackward0 object at 0x7feb540dd048> \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "z=tensor([[10., 26.],\n",
      "        [50., 82.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  grad_fn=<AddBackward0 object at 0x7feb540dd048> \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "out=42.0\n",
      "  grad_fn=<MeanBackward0 object at 0x7feb540dd048> \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute():\n",
    "    x = torch.tensor([[1, 2],[3, 4]], dtype=torch.double, requires_grad=True)\n",
    "    y = 2 * x + 1\n",
    "    z = y * y + 1\n",
    "    out = z.mean()\n",
    "    return {'x':x, 'y':y, 'z':z, 'out':out}\n",
    "\n",
    "def print_tensor(key, tensor):\n",
    "    print(\"{}={}\".format(key, tensor))\n",
    "    print(\"  grad_fn={} \\n  grad={} \\n  requires_grad={}\\n  is_leaf={} \\n  dtype={}\".format(\n",
    "        tensor.grad_fn, tensor.grad, \n",
    "        tensor.requires_grad, tensor.is_leaf, tensor.dtype))\n",
    "    print(\"-\"*50)    \n",
    "\n",
    "def print_tensors(tensors):\n",
    "    for key, tensor in tensors.items():\n",
    "        print_tensor(key, tensor)\n",
    "\n",
    "tensors = compute()\n",
    "tensors['x'] = torch.tensor([[1, 1],[1, 1]], dtype=torch.double, requires_grad=True)\n",
    "tensors['out'].backward()\n",
    "print_tensors(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64, requires_grad=True)\n",
      "  grad_fn=None \n",
      "  grad=tensor([[3., 5.],\n",
      "        [7., 9.]], dtype=torch.float64) \n",
      "  requires_grad=True\n",
      "  is_leaf=True \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "y=tensor([[3., 5.],\n",
      "        [7., 9.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  grad_fn=<AddBackward0 object at 0x7feb540dd358> \n",
      "  grad=tensor([[1.5000, 2.5000],\n",
      "        [3.5000, 4.5000]], dtype=torch.float64) \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "z=tensor([[10., 26.],\n",
      "        [50., 82.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  grad_fn=<AddBackward0 object at 0x7feb540dd358> \n",
      "  grad=tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]], dtype=torch.float64) \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "out=42.0\n",
      "  grad_fn=<MeanBackward0 object at 0x7feb540dd358> \n",
      "  grad=1.0 \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tensors = compute()\n",
    "for _, tensor in tensors.items():\n",
    "    tensor.retain_grad()\n",
    "tensors['out'].backward()\n",
    "print_tensors(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64, requires_grad=True)\n",
      "  grad_fn=None \n",
      "  grad=tensor([[  6.,  20.],\n",
      "        [ 56., 144.]], dtype=torch.float64) \n",
      "  requires_grad=True\n",
      "  is_leaf=True \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "y=tensor([[3., 5.],\n",
      "        [7., 9.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  grad_fn=<AddBackward0 object at 0x7feab6067f28> \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "z=tensor([[10., 26.],\n",
      "        [50., 82.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  grad_fn=<AddBackward0 object at 0x7feab6067588> \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "out=42.0\n",
      "  grad_fn=<MeanBackward0 object at 0x7feab6067588> \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tensors = compute()\n",
    "gradients = torch.tensor([[0.5, 1], [2, 4]], dtype=torch.float)\n",
    "tensors['z'].backward(gradients)\n",
    "print_tensors(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64, requires_grad=True)\n",
      "  grad_fn=None \n",
      "  grad=tensor([[3., 5.],\n",
      "        [7., 9.]], dtype=torch.float64, grad_fn=<CloneBackward>) \n",
      "  requires_grad=True\n",
      "  is_leaf=True \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "y=tensor([[3., 5.],\n",
      "        [7., 9.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  grad_fn=<AddBackward0 object at 0x7feabe4c3748> \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "z=tensor([[10., 26.],\n",
      "        [50., 82.]], dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "  grad_fn=<AddBackward0 object at 0x7feabe4c3748> \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n",
      "out=42.0\n",
      "  grad_fn=<MeanBackward0 object at 0x7feabe4c3748> \n",
      "  grad=None \n",
      "  requires_grad=True\n",
      "  is_leaf=False \n",
      "  dtype=torch.float64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tensors = compute()\n",
    "tensors['out'].backward(retain_graph=True, create_graph=True)\n",
    "print_tensors(tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor的拼接\n",
    "\n",
    "比较torch和Numpy\n",
    "\n",
    "\n",
    "| 操作                               | numpy                     | torch                              |\n",
    "| ---------------------------------- | :------------------------ | ---------------------------------- |\n",
    "| 垂直方向拼接(行增)                 | np.vstack(A, B)           | torch.cat((A, B), 0)               |\n",
    "| 水平方向拼接(列增)                 | np.hstack(A, B)           | torch.cat((A, B), 1)               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------numpy------------------------------\n",
      "a = \n",
      "[[0.57981256 0.32386837]\n",
      " [0.07167079 0.55758686]\n",
      " [0.87771614 0.61672263]]\n",
      "\n",
      "b = \n",
      "[[0.70158077 0.3792917  0.94872452 0.92641252]\n",
      " [0.9133379  0.99182234 0.72041977 0.67072277]\n",
      " [0.90090687 0.18414355 0.93752312 0.44986942]]\n",
      "\n",
      "c = \n",
      "[[0.13479513 0.03405707]\n",
      " [0.89925418 0.87138145]\n",
      " [0.20890133 0.73610866]\n",
      " [0.7030512  0.35181984]]\n",
      "\n",
      "ab = \n",
      "[[0.57981256 0.32386837 0.70158077 0.3792917  0.94872452 0.92641252]\n",
      " [0.07167079 0.55758686 0.9133379  0.99182234 0.72041977 0.67072277]\n",
      " [0.87771614 0.61672263 0.90090687 0.18414355 0.93752312 0.44986942]]\n",
      "\n",
      "ac = \n",
      "[[0.57981256 0.32386837]\n",
      " [0.07167079 0.55758686]\n",
      " [0.87771614 0.61672263]\n",
      " [0.13479513 0.03405707]\n",
      " [0.89925418 0.87138145]\n",
      " [0.20890133 0.73610866]\n",
      " [0.7030512  0.35181984]]\n",
      "\n",
      "------------------------------torch cat------------------------------\n",
      "ab = \n",
      "tensor([[0.5798, 0.3239, 0.7016, 0.3793, 0.9487, 0.9264],\n",
      "        [0.0717, 0.5576, 0.9133, 0.9918, 0.7204, 0.6707],\n",
      "        [0.8777, 0.6167, 0.9009, 0.1841, 0.9375, 0.4499]], dtype=torch.float64)\n",
      "\n",
      "ac = \n",
      "tensor([[0.5798, 0.3239],\n",
      "        [0.0717, 0.5576],\n",
      "        [0.8777, 0.6167],\n",
      "        [0.1348, 0.0341],\n",
      "        [0.8993, 0.8714],\n",
      "        [0.2089, 0.7361],\n",
      "        [0.7031, 0.3518]], dtype=torch.float64)\n",
      "\n",
      "------------------------------torch stack------------------------------\n",
      "aa_0 = \n",
      "torch.Size([2, 3, 2])\n",
      "\n",
      "aa_1 = \n",
      "torch.Size([3, 2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(3, 2)\n",
    "b = np.random.rand(3, 4)\n",
    "c = np.random.rand(4, 2)\n",
    "\n",
    "print('-'*30+'numpy'+'-'*30)\n",
    "\n",
    "print('a = \\n{}\\n'.format(a))\n",
    "print('b = \\n{}\\n'.format(b))\n",
    "print('c = \\n{}\\n'.format(c))\n",
    "\n",
    "ab = np.hstack((a, b))\n",
    "ac = np.vstack((a, c))\n",
    "\n",
    "print('ab = \\n{}\\n'.format(ab))\n",
    "print('ac = \\n{}\\n'.format(ac))\n",
    "\n",
    "print('-'*30+'torch cat'+'-'*30)\n",
    "\n",
    "a = torch.from_numpy(a)\n",
    "b = torch.from_numpy(b)\n",
    "c = torch.from_numpy(c)\n",
    "\n",
    "ab = torch.cat((a, b), 1)\n",
    "ac = torch.cat((a, c), 0)\n",
    "\n",
    "print('ab = \\n{}\\n'.format(ab))\n",
    "print('ac = \\n{}\\n'.format(ac))\n",
    "\n",
    "print('-'*30+'torch stack'+'-'*30)\n",
    "# \"cat 和 .stack的区别在于 cat会增加现有维度的值,可以理解为续接，stack会新加增加一个维度，可以理解为叠加\n",
    "\n",
    "aa_0 = torch.stack((a, a), 0)\n",
    "aa_1 = torch.stack((a, a), 1)\n",
    "\n",
    "print('aa_0 = \\n{}\\n'.format(aa_0.size()))\n",
    "print('aa_1 = \\n{}\\n'.format(aa_1.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动梯度\n",
    "\n",
    "\n",
    "下面是线性回归的求解，分别用四种方式。\n",
    "- 公式计算\n",
    "  $$\n",
    "  \\mathbf w = \\mathbf {(X^{T}X)^{-1}X^{T}y}\n",
    "  $$\n",
    "- 手工梯度下降 \n",
    "- 使用Autograd \n",
    "- 使用pytorch优化\n",
    "\n",
    "可以看到所有方式所计算的结果是相同的。后面三种方式，Loss和梯度也是完全相同的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "tensor([[1., 5., 3., 2.],\n",
      "        [1., 4., 1., 5.],\n",
      "        [1., 7., 6., 8.],\n",
      "        [1., 1., 4., 2.]])\n",
      "\n",
      "y = \n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "\n",
      "------------------------------equation------------------------------\n",
      "w = \n",
      "tensor([[ 0.9063],\n",
      "        [-0.3437],\n",
      "        [ 0.6250],\n",
      "        [-0.0312]])\n",
      "\n",
      "------------------------------manual grad------------------------------\n",
      "epoch 0, loss: 3.241280, grad: tensor([[ 0.9334,  8.8125,  6.7585, 12.4407]])\n",
      "epoch 200, loss: 0.307015, grad: tensor([[-0.4201,  0.0384,  0.0510,  0.0038]])\n",
      "epoch 400, loss: 0.094523, grad: tensor([[-2.3291e-01,  2.3480e-02,  2.8202e-02,  3.8028e-05]])\n",
      "epoch 600, loss: 0.029102, grad: tensor([[-1.2923e-01,  1.3036e-02,  1.5650e-02,  1.9133e-05]])\n",
      "epoch 800, loss: 0.008960, grad: tensor([[-7.1709e-02,  7.2310e-03,  8.6819e-03,  8.1062e-06]])\n",
      "epoch 1000, loss: 0.002759, grad: tensor([[-3.9789e-02,  4.0133e-03,  4.8188e-03,  5.7966e-06]])\n",
      "epoch 1200, loss: 0.000849, grad: tensor([[-2.2078e-02,  2.2245e-03,  2.6717e-03,  5.0664e-07]])\n",
      "epoch 1400, loss: 0.000261, grad: tensor([[-1.2250e-02,  1.2355e-03,  1.4830e-03,  1.5721e-06]])\n",
      "epoch 1600, loss: 0.000081, grad: tensor([[-6.7971e-03,  6.8766e-04,  8.2459e-04,  2.7642e-06]])\n",
      "epoch 1800, loss: 0.000025, grad: tensor([[-3.7717e-03,  3.8078e-04,  4.5637e-04,  7.8231e-07]])\n",
      "w = \n",
      "tensor([[ 0.8991],\n",
      "        [-0.3430],\n",
      "        [ 0.6259],\n",
      "        [-0.0312]])\n",
      "\n",
      "------------------------------autograd------------------------------\n",
      "epoch 0, loss: 3.241280, grad: tensor([[ 0.9334,  8.8125,  6.7585, 12.4407]])\n",
      "epoch 200, loss: 0.307015, grad: tensor([[-0.4201,  0.0384,  0.0510,  0.0038]])\n",
      "epoch 400, loss: 0.094523, grad: tensor([[-2.3291e-01,  2.3480e-02,  2.8202e-02,  3.8028e-05]])\n",
      "epoch 600, loss: 0.029102, grad: tensor([[-1.2923e-01,  1.3036e-02,  1.5650e-02,  1.9133e-05]])\n",
      "epoch 800, loss: 0.008960, grad: tensor([[-7.1709e-02,  7.2310e-03,  8.6819e-03,  8.1062e-06]])\n",
      "epoch 1000, loss: 0.002759, grad: tensor([[-3.9789e-02,  4.0133e-03,  4.8188e-03,  5.7966e-06]])\n",
      "epoch 1200, loss: 0.000849, grad: tensor([[-2.2078e-02,  2.2245e-03,  2.6717e-03,  5.0664e-07]])\n",
      "epoch 1400, loss: 0.000261, grad: tensor([[-1.2250e-02,  1.2355e-03,  1.4830e-03,  1.5721e-06]])\n",
      "epoch 1600, loss: 0.000081, grad: tensor([[-6.7971e-03,  6.8766e-04,  8.2459e-04,  2.7642e-06]])\n",
      "epoch 1800, loss: 0.000025, grad: tensor([[-3.7717e-03,  3.8078e-04,  4.5637e-04,  7.8231e-07]])\n",
      "w = \n",
      "tensor([[ 0.8991],\n",
      "        [-0.3430],\n",
      "        [ 0.6259],\n",
      "        [-0.0312]])\n",
      "\n",
      "------------------------------pytorch optimize------------------------------\n",
      "epoch 0, loss: 3.241280, grad: tensor([[ 0.9334,  8.8125,  6.7585, 12.4407]])\n",
      "epoch 200, loss: 0.307015, grad: tensor([[-0.4201,  0.0384,  0.0510,  0.0038]])\n",
      "epoch 400, loss: 0.094523, grad: tensor([[-2.3291e-01,  2.3480e-02,  2.8202e-02,  3.8028e-05]])\n",
      "epoch 600, loss: 0.029102, grad: tensor([[-1.2923e-01,  1.3036e-02,  1.5650e-02,  1.9133e-05]])\n",
      "epoch 800, loss: 0.008960, grad: tensor([[-7.1709e-02,  7.2310e-03,  8.6819e-03,  8.1062e-06]])\n",
      "epoch 1000, loss: 0.002759, grad: tensor([[-3.9789e-02,  4.0133e-03,  4.8188e-03,  5.7966e-06]])\n",
      "epoch 1200, loss: 0.000849, grad: tensor([[-2.2078e-02,  2.2245e-03,  2.6717e-03,  5.0664e-07]])\n",
      "epoch 1400, loss: 0.000261, grad: tensor([[-1.2250e-02,  1.2355e-03,  1.4830e-03,  1.5721e-06]])\n",
      "epoch 1600, loss: 0.000081, grad: tensor([[-6.7971e-03,  6.8766e-04,  8.2459e-04,  2.7642e-06]])\n",
      "epoch 1800, loss: 0.000025, grad: tensor([[-3.7717e-03,  3.8078e-04,  4.5637e-04,  7.8231e-07]])\n",
      "w = \n",
      "tensor([[ 0.8991],\n",
      "        [-0.3430],\n",
      "        [ 0.6259],\n",
      "        [-0.0312]], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "X = t.Tensor([[5, 3, 2],\n",
    "              [4, 1, 5],\n",
    "              [7, 6, 8],\n",
    "              [1, 4, 2]])\n",
    "m, n = X.size()\n",
    "y = t.Tensor([1, 0, 2, 3]).view(m, 1)\n",
    "X = t.cat((t.ones(m, 1), X), 1)\n",
    "w_init = t.randn(n+1, 1)\n",
    "\n",
    "print('X = \\n{}\\n'.format(X))   \n",
    "print('y = \\n{}\\n'.format(y))   \n",
    "\n",
    "print('-'*30+'equation'+'-'*30)\n",
    "w = t.inverse(X.t() @ X) @ X.t() @ y\n",
    "\n",
    "print('w = \\n{}\\n'.format(w))\n",
    "\n",
    "print('-'*30+'manual grad'+'-'*30)\n",
    "def manual_train(X, y, w_init, epochs=2000, learning_rate=0.01):\n",
    "    w = w_init\n",
    "    for epoch in range(epochs):\n",
    "        delta = X @ w - y\n",
    "        loss = delta.t() @ delta/m      \n",
    "        grad = 2 * X.t() @ delta / m\n",
    "        if epoch % (epochs // 10) ==0:\n",
    "            print('epoch {}, loss: {:7f}, grad: {}'.format(epoch, loss.item(), grad.t()))          \n",
    "        w = w - learning_rate*grad\n",
    "    return w    \n",
    "\n",
    "w = manual_train(X, y, w_init)\n",
    "print('w = \\n{}\\n'.format(w))    \n",
    "\n",
    "print('-'*30+'autograd'+'-'*30)\n",
    "def autograd_train(X, y, w_init, epochs=2000, learning_rate=0.01):\n",
    "    w = w_init    \n",
    "    for epoch in range(epochs):\n",
    "        w.requires_grad_(True)\n",
    "        delta = X @ w - y\n",
    "        loss = delta.t() @ delta/m\n",
    "        loss.backward()\n",
    "        if epoch % (epochs // 10) ==0:\n",
    "            print('epoch {}, loss: {:7f}, grad: {}'.format(epoch, loss.item(), w.grad.t()))          \n",
    "        with t.no_grad():\n",
    "            w = w - learning_rate*w.grad        \n",
    "    return w    \n",
    "\n",
    "w = autograd_train(X, y, w_init)\n",
    "print('w = \\n{}\\n'.format(w))   \n",
    "\n",
    "print('-'*30+'pytorch optimize'+'-'*30)\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def pytorch_train(X, y, w_init, epochs=2000, learning_rate=0.01):\n",
    "    w = w_init    \n",
    "    optimizer = optim.SGD([w], lr=learning_rate)    \n",
    "    criterion = nn.MSELoss()    \n",
    "    for epoch in range(epochs): \n",
    "        optimizer.zero_grad() \n",
    "        loss = criterion(X @ w, y)\n",
    "        loss.backward()\n",
    "        if epoch % (epochs // 10) ==0:\n",
    "            print('epoch {}, loss: {:7f}, grad: {}'.format(epoch, loss.item(), w.grad.t()))           \n",
    "        optimizer.step()          \n",
    "    return w    \n",
    "\n",
    "w = pytorch_train(X, y, w_init)\n",
    "print('w = \\n{}\\n'.format(w))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
