{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "35//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor的拼接\n",
    "\n",
    "比较torch和Numpy\n",
    "\n",
    "\n",
    "| 操作                               | numpy                     | torch                              |\n",
    "| ---------------------------------- | :------------------------ | ---------------------------------- |\n",
    "| 垂直方向拼接(行增)                 | np.vstack(A, B)           | torch.cat((A, B), 0)               |\n",
    "| 水平方向拼接(列增)                 | np.hstack(A, B)           | torch.cat((A, B), 1)               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------numpy------------------------------\n",
      "a = \n",
      "[[0.20742526 0.68117048]\n",
      " [0.06107496 0.05566335]\n",
      " [0.91610351 0.6989592 ]]\n",
      "\n",
      "b = \n",
      "[[0.90011517 0.55089449 0.85735844 0.05841693]\n",
      " [0.49165399 0.19501844 0.96031008 0.25732553]\n",
      " [0.03068124 0.44602039 0.88419661 0.31726   ]]\n",
      "\n",
      "c = \n",
      "[[0.73625492 0.07091267]\n",
      " [0.57339809 0.01795358]\n",
      " [0.25299596 0.85770227]\n",
      " [0.1586333  0.37721679]]\n",
      "\n",
      "ab = \n",
      "[[0.20742526 0.68117048 0.90011517 0.55089449 0.85735844 0.05841693]\n",
      " [0.06107496 0.05566335 0.49165399 0.19501844 0.96031008 0.25732553]\n",
      " [0.91610351 0.6989592  0.03068124 0.44602039 0.88419661 0.31726   ]]\n",
      "\n",
      "ac = \n",
      "[[0.20742526 0.68117048]\n",
      " [0.06107496 0.05566335]\n",
      " [0.91610351 0.6989592 ]\n",
      " [0.73625492 0.07091267]\n",
      " [0.57339809 0.01795358]\n",
      " [0.25299596 0.85770227]\n",
      " [0.1586333  0.37721679]]\n",
      "\n",
      "------------------------------torch cat------------------------------\n",
      "ab = \n",
      "tensor([[0.2074, 0.6812, 0.9001, 0.5509, 0.8574, 0.0584],\n",
      "        [0.0611, 0.0557, 0.4917, 0.1950, 0.9603, 0.2573],\n",
      "        [0.9161, 0.6990, 0.0307, 0.4460, 0.8842, 0.3173]], dtype=torch.float64)\n",
      "\n",
      "ac = \n",
      "tensor([[0.2074, 0.6812],\n",
      "        [0.0611, 0.0557],\n",
      "        [0.9161, 0.6990],\n",
      "        [0.7363, 0.0709],\n",
      "        [0.5734, 0.0180],\n",
      "        [0.2530, 0.8577],\n",
      "        [0.1586, 0.3772]], dtype=torch.float64)\n",
      "\n",
      "------------------------------torch stack------------------------------\n",
      "aa_0 = \n",
      "torch.Size([2, 3, 2])\n",
      "\n",
      "aa_1 = \n",
      "torch.Size([3, 2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(3, 2)\n",
    "b = np.random.rand(3, 4)\n",
    "c = np.random.rand(4, 2)\n",
    "\n",
    "print('-'*30+'numpy'+'-'*30)\n",
    "\n",
    "print('a = \\n{}\\n'.format(a))\n",
    "print('b = \\n{}\\n'.format(b))\n",
    "print('c = \\n{}\\n'.format(c))\n",
    "\n",
    "ab = np.hstack((a, b))\n",
    "ac = np.vstack((a, c))\n",
    "\n",
    "print('ab = \\n{}\\n'.format(ab))\n",
    "print('ac = \\n{}\\n'.format(ac))\n",
    "\n",
    "print('-'*30+'torch cat'+'-'*30)\n",
    "\n",
    "a = torch.from_numpy(a)\n",
    "b = torch.from_numpy(b)\n",
    "c = torch.from_numpy(c)\n",
    "\n",
    "ab = torch.cat((a, b), 1)\n",
    "ac = torch.cat((a, c), 0)\n",
    "\n",
    "print('ab = \\n{}\\n'.format(ab))\n",
    "print('ac = \\n{}\\n'.format(ac))\n",
    "\n",
    "print('-'*30+'torch stack'+'-'*30)\n",
    "# \"cat 和 .stack的区别在于 cat会增加现有维度的值,可以理解为续接，stack会新加增加一个维度，可以理解为叠加\n",
    "\n",
    "aa_0 = torch.stack((a, a), 0)\n",
    "aa_1 = torch.stack((a, a), 1)\n",
    "\n",
    "print('aa_0 = \\n{}\\n'.format(aa_0.size()))\n",
    "print('aa_1 = \\n{}\\n'.format(aa_1.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动梯度\n",
    "\n",
    "\n",
    "下面是线性回归的求解，分别用四种方式。\n",
    "- 公式计算\n",
    "  $$\n",
    "  \\mathbf w = \\mathbf {(X^{T}X)^{-1}X^{T}y}\n",
    "  $$\n",
    "- 手工梯度下降 \n",
    "- 使用Autograd \n",
    "- 使用pytorch优化\n",
    "\n",
    "可以看到所有方式所计算的结果是相同的。后面三种方式，Loss和梯度也是完全相同的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "tensor([[1., 5., 3., 2.],\n",
      "        [1., 4., 1., 5.],\n",
      "        [1., 7., 6., 8.],\n",
      "        [1., 1., 4., 2.]])\n",
      "\n",
      "y = \n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "\n",
      "------------------------------equation------------------------------\n",
      "w = \n",
      "tensor([[ 0.9063],\n",
      "        [-0.3437],\n",
      "        [ 0.6250],\n",
      "        [-0.0312]])\n",
      "\n",
      "------------------------------manual grad------------------------------\n",
      "epoch 0, loss: 3.241280, grad: tensor([[ 0.9334,  8.8125,  6.7585, 12.4407]])\n",
      "epoch 200, loss: 0.307015, grad: tensor([[-0.4201,  0.0384,  0.0510,  0.0038]])\n",
      "epoch 400, loss: 0.094523, grad: tensor([[-2.3291e-01,  2.3480e-02,  2.8202e-02,  3.8028e-05]])\n",
      "epoch 600, loss: 0.029102, grad: tensor([[-1.2923e-01,  1.3036e-02,  1.5650e-02,  1.9133e-05]])\n",
      "epoch 800, loss: 0.008960, grad: tensor([[-7.1709e-02,  7.2310e-03,  8.6819e-03,  8.1062e-06]])\n",
      "epoch 1000, loss: 0.002759, grad: tensor([[-3.9789e-02,  4.0133e-03,  4.8188e-03,  5.7966e-06]])\n",
      "epoch 1200, loss: 0.000849, grad: tensor([[-2.2078e-02,  2.2245e-03,  2.6717e-03,  5.0664e-07]])\n",
      "epoch 1400, loss: 0.000261, grad: tensor([[-1.2250e-02,  1.2355e-03,  1.4830e-03,  1.5721e-06]])\n",
      "epoch 1600, loss: 0.000081, grad: tensor([[-6.7971e-03,  6.8766e-04,  8.2459e-04,  2.7642e-06]])\n",
      "epoch 1800, loss: 0.000025, grad: tensor([[-3.7717e-03,  3.8078e-04,  4.5637e-04,  7.8231e-07]])\n",
      "w = \n",
      "tensor([[ 0.8991],\n",
      "        [-0.3430],\n",
      "        [ 0.6259],\n",
      "        [-0.0312]])\n",
      "\n",
      "------------------------------autograd------------------------------\n",
      "epoch 0, loss: 3.241280, grad: tensor([[ 0.9334,  8.8125,  6.7585, 12.4407]])\n",
      "epoch 200, loss: 0.307015, grad: tensor([[-0.4201,  0.0384,  0.0510,  0.0038]])\n",
      "epoch 400, loss: 0.094523, grad: tensor([[-2.3291e-01,  2.3480e-02,  2.8202e-02,  3.8028e-05]])\n",
      "epoch 600, loss: 0.029102, grad: tensor([[-1.2923e-01,  1.3036e-02,  1.5650e-02,  1.9133e-05]])\n",
      "epoch 800, loss: 0.008960, grad: tensor([[-7.1709e-02,  7.2310e-03,  8.6819e-03,  8.1062e-06]])\n",
      "epoch 1000, loss: 0.002759, grad: tensor([[-3.9789e-02,  4.0133e-03,  4.8188e-03,  5.7966e-06]])\n",
      "epoch 1200, loss: 0.000849, grad: tensor([[-2.2078e-02,  2.2245e-03,  2.6717e-03,  5.0664e-07]])\n",
      "epoch 1400, loss: 0.000261, grad: tensor([[-1.2250e-02,  1.2355e-03,  1.4830e-03,  1.5721e-06]])\n",
      "epoch 1600, loss: 0.000081, grad: tensor([[-6.7971e-03,  6.8766e-04,  8.2459e-04,  2.7642e-06]])\n",
      "epoch 1800, loss: 0.000025, grad: tensor([[-3.7717e-03,  3.8078e-04,  4.5637e-04,  7.8231e-07]])\n",
      "w = \n",
      "tensor([[ 0.8991],\n",
      "        [-0.3430],\n",
      "        [ 0.6259],\n",
      "        [-0.0312]])\n",
      "\n",
      "------------------------------pytorch optimize------------------------------\n",
      "epoch 0, loss: 3.241280, grad: tensor([[ 0.9334,  8.8125,  6.7585, 12.4407]])\n",
      "epoch 200, loss: 0.307015, grad: tensor([[-0.4201,  0.0384,  0.0510,  0.0038]])\n",
      "epoch 400, loss: 0.094523, grad: tensor([[-2.3291e-01,  2.3480e-02,  2.8202e-02,  3.8028e-05]])\n",
      "epoch 600, loss: 0.029102, grad: tensor([[-1.2923e-01,  1.3036e-02,  1.5650e-02,  1.9133e-05]])\n",
      "epoch 800, loss: 0.008960, grad: tensor([[-7.1709e-02,  7.2310e-03,  8.6819e-03,  8.1062e-06]])\n",
      "epoch 1000, loss: 0.002759, grad: tensor([[-3.9789e-02,  4.0133e-03,  4.8188e-03,  5.7966e-06]])\n",
      "epoch 1200, loss: 0.000849, grad: tensor([[-2.2078e-02,  2.2245e-03,  2.6717e-03,  5.0664e-07]])\n",
      "epoch 1400, loss: 0.000261, grad: tensor([[-1.2250e-02,  1.2355e-03,  1.4830e-03,  1.5721e-06]])\n",
      "epoch 1600, loss: 0.000081, grad: tensor([[-6.7971e-03,  6.8766e-04,  8.2459e-04,  2.7642e-06]])\n",
      "epoch 1800, loss: 0.000025, grad: tensor([[-3.7717e-03,  3.8078e-04,  4.5637e-04,  7.8231e-07]])\n",
      "w = \n",
      "tensor([[ 0.8991],\n",
      "        [-0.3430],\n",
      "        [ 0.6259],\n",
      "        [-0.0312]], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "\n",
    "X = t.Tensor([[5, 3, 2],\n",
    "              [4, 1, 5],\n",
    "              [7, 6, 8],\n",
    "              [1, 4, 2]])\n",
    "m, n = X.size()\n",
    "y = t.Tensor([1, 0, 2, 3]).view(m, 1)\n",
    "X = t.cat((t.ones(m, 1), X), 1)\n",
    "w_init = t.randn(n+1, 1)\n",
    "\n",
    "print('X = \\n{}\\n'.format(X))   \n",
    "print('y = \\n{}\\n'.format(y))   \n",
    "\n",
    "print('-'*30+'equation'+'-'*30)\n",
    "w = t.inverse(X.t() @ X) @ X.t() @ y\n",
    "\n",
    "print('w = \\n{}\\n'.format(w))\n",
    "\n",
    "print('-'*30+'manual grad'+'-'*30)\n",
    "def manual_train(X, y, w_init, epochs=2000, learning_rate=0.01):\n",
    "    w = w_init\n",
    "    for epoch in range(epochs):\n",
    "        delta = X @ w - y\n",
    "        loss = delta.t() @ delta/m      \n",
    "        grad = 2 * X.t() @ delta / m\n",
    "        if epoch % (epochs // 10) ==0:\n",
    "            print('epoch {}, loss: {:7f}, grad: {}'.format(epoch, loss.item(), grad.t()))          \n",
    "        w = w - learning_rate*grad\n",
    "    return w    \n",
    "\n",
    "w = manual_train(X, y, w_init)\n",
    "print('w = \\n{}\\n'.format(w))    \n",
    "\n",
    "print('-'*30+'autograd'+'-'*30)\n",
    "def autograd_train(X, y, w_init, epochs=2000, learning_rate=0.01):\n",
    "    w = w_init    \n",
    "    for epoch in range(epochs):\n",
    "        w.requires_grad_(True)\n",
    "        delta = X @ w - y\n",
    "        loss = delta.t() @ delta/m\n",
    "        loss.backward()\n",
    "        if epoch % (epochs // 10) ==0:\n",
    "            print('epoch {}, loss: {:7f}, grad: {}'.format(epoch, loss.item(), w.grad.t()))          \n",
    "        with t.no_grad():\n",
    "            w = w - learning_rate*w.grad        \n",
    "    return w    \n",
    "\n",
    "w = autograd_train(X, y, w_init)\n",
    "print('w = \\n{}\\n'.format(w))   \n",
    "\n",
    "print('-'*30+'pytorch optimize'+'-'*30)\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "def pytorch_train(X, y, w_init, epochs=2000, learning_rate=0.01):\n",
    "    w = w_init    \n",
    "    optimizer = optim.SGD([w], lr=learning_rate)    \n",
    "    criterion = nn.MSELoss()    \n",
    "    for epoch in range(epochs): \n",
    "        optimizer.zero_grad() \n",
    "        loss = criterion(X @ w, y)\n",
    "        loss.backward()\n",
    "        if epoch % (epochs // 10) ==0:\n",
    "            print('epoch {}, loss: {:7f}, grad: {}'.format(epoch, loss.item(), w.grad.t()))           \n",
    "        optimizer.step()          \n",
    "    return w    \n",
    "\n",
    "w = pytorch_train(X, y, w_init)\n",
    "print('w = \\n{}\\n'.format(w))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
