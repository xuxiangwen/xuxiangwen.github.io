{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionaryDictionary(89 unique tokens: ['买卖', '介绍', '定性', '居间', '应']...)\n",
      "[(6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]\n",
      "[(6, 0.1284266598983105), (7, 0.15237436771781762), (8, 0.4307288472439022), (9, 0.4307288472439022), (10, 0.4307288472439022), (11, 0.4307288472439022), (12, 0.4307288472439022), (13, 0.18069849432038634)]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import urllib\n",
    "from gensim import corpora, models\n",
    "from gensim.similarities import Similarity\n",
    "\n",
    "response = urllib.request.urlopen('https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt')\n",
    "stop_words = [str(word[:-1],'utf-8') for word in response.readlines()]\n",
    "\n",
    "docs = [\n",
    "    '无偿居间介绍买卖毒品的行为应如何定性',\n",
    "    '吸毒男动态持有大量毒品的行为该如何认定',\n",
    "    '如何区分是非法种植毒品原植物罪还是非法制造毒品罪',\n",
    "    '为毒贩贩卖毒品提供帮助构成贩卖毒品罪',\n",
    "    '将自己吸食的毒品原价转让给朋友吸食的行为该如何认定',\n",
    "    '为获报酬帮人购买毒品的行为该如何认定',\n",
    "    '毒贩出狱后再次够买毒品途中被抓的行为认定',\n",
    "    '虚夸毒品功效劝人吸食毒品的行为该如何认定',\n",
    "    '妻子下落不明丈夫又与他人登记结婚是否为无效婚姻',\n",
    "    '一方未签字办理的结婚登记是否有效',\n",
    "    '夫妻双方1990年按农村习俗举办婚礼没有结婚证 一方可否起诉离婚',\n",
    "    '结婚前对方父母出资购买的住房写我们二人的名字有效吗',\n",
    "    '身份证被别人冒用无法登记结婚怎么办？',\n",
    "    '同居后又与他人登记结婚是否构成重婚罪',\n",
    "    '未办登记只举办结婚仪式可起诉离婚吗',\n",
    "    '同居多年未办理结婚登记，是否可以向法院起诉要求离婚'\n",
    "]\n",
    "\n",
    "corpus = [[word for word in jieba.cut(text) if word not in stop_words] for text in docs]\n",
    "    \n",
    "# 生成字典和向量语料\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "print(\"dictionary\"+str(dictionary))\n",
    "\n",
    "processed_corpus = [dictionary.doc2bow(text) for text in corpus]\n",
    "print(processed_corpus[1])\n",
    "\n",
    "#转化成tf-idf向量\n",
    "tfidf = models.TfidfModel(processed_corpus)\n",
    "tfidf_corpus = [tfidf[doc] for doc in processed_corpus]\n",
    "print(tfidf_corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[1.23519887 1.16004215 1.09302261 1.06277901 1.03973657 1.0073313\n",
      " 0.99018854 0.97605087 0.96920406 0.95874886 0.9545628  0.93878116\n",
      " 0.92867664 0.90997187 0.86879529 0.82408782]\n",
      "[(0, 0.015016122971703624), (1, -0.27431681349417036), (2, 0.0626972951828514), (3, -0.02818881846385694), (4, 0.09040453624457288), (5, -0.14198292253692507), (6, 0.9041874197315449), (7, -0.22480955340025588), (8, 0.11689494581337463), (9, 0.05053389409734962), (10, 0.014760580485360993), (11, -0.03748669410957461), (12, 0.07083480987014054), (13, -0.019328591790182776), (14, -0.007572190488805301), (15, 0.00911883816736171)]\n"
     ]
    }
   ],
   "source": [
    "#转化成lsi向量\n",
    "lsi = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=16)\n",
    "corpus_lsi = [lsi[doc] for doc in tfidf_corpus]\n",
    "\n",
    "print(type(lsi.projection.u))\n",
    "print(lsi.projection.s)\n",
    "print(lsi[tfidf_corpus[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_lsi = Similarity('Similarity-Lsi-index', corpus_lsi, num_features=400, num_best=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2.转换成bow向量 # [(51, 1), (59, 1)]，即在字典的52和60的地方出现重复的字段，这个值可能会变化\n",
    "test_corpus_3 = dictionary.doc2bow(test_cut_raw_1)  \n",
    "print(test_corpus_3)\n",
    "# 3.计算tfidf值  # 根据之前训练生成的model，生成query的IFIDF值，然后进行相似度计算\n",
    "test_corpus_tfidf_3 = tfidf_model[test_corpus_3]  \n",
    "print(test_corpus_tfidf_3) # [(51, 0.7071067811865475), (59, 0.7071067811865475)]\n",
    "# 4.计算lsi值\n",
    "test_corpus_lsi_3 = lsi[test_corpus_tfidf_3]  \n",
    "print(test_corpus_lsi_3)\n",
    "# lsi.add_documents(test_corpus_lsi_3) #更新LSI的值\n",
    "print('——————————————lsi———————————————')\n",
    "# 返回最相似的样本材料,(index_of_document, similarity) tuples\n",
    "print(similarity_lsi[test_corpus_lsi_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Topics and Transformations\n",
    "===========================\n",
    "\n",
    "Introduces transformations and demonstrates their use on a toy corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will show how to transform documents from one vector representation\n",
    "into another. This process serves two goals:\n",
    "\n",
    "1. To bring out hidden structure in the corpus, discover relationships between\n",
    "   words and use them to describe the documents in a new and\n",
    "   (hopefully) more semantic way.\n",
    "2. To make the document representation more compact. This both improves efficiency\n",
    "   (new representation consumes less resources) and efficacy (marginal data\n",
    "   trends are ignored, noise-reduction).\n",
    "\n",
    "Creating the Corpus\n",
    "-------------------\n",
    "\n",
    "First, we need to create a corpus to work with.\n",
    "This step is the same as in the previous tutorial;\n",
    "if you completed it, feel free to skip to the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a transformation\n",
    "++++++++++++++++++++++++++\n",
    "\n",
    "The transformations are standard Python objects, typically initialized by means of\n",
    "a :dfn:`training corpus`:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)  # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used our old corpus from tutorial 1 to initialize (train) the transformation model. Different\n",
    "transformations may require different initialization parameters; in case of TfIdf, the\n",
    "\"training\" consists simply of going through the supplied corpus once and computing document frequencies\n",
    "of all its features. Training other models, such as Latent Semantic Analysis or Latent Dirichlet\n",
    "Allocation, is much more involved and, consequently, takes much more time.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Transformations always convert between two specific vector\n",
    "  spaces. The same vector space (= the same set of feature ids) must be used for training\n",
    "  as well as for subsequent vector transformations. Failure to use the same input\n",
    "  feature space, such as applying a different string preprocessing, using different\n",
    "  feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will\n",
    "  result in feature mismatch during transformation calls and consequently in either\n",
    "  garbage output and/or runtime exceptions.</p></div>\n",
    "\n",
    "\n",
    "Transforming vectors\n",
    "+++++++++++++++++++++\n",
    "\n",
    "From now on, ``tfidf`` is treated as a read-only object that can be used to convert\n",
    "any vector from the old representation (bag-of-words integer counts) to the new representation\n",
    "(TfIdf real-valued weights):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])  # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to apply a transformation to a whole corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, we are transforming the same corpus that we used\n",
    "for training, but this is only incidental. Once the transformation model has been initialized,\n",
    "it can be used on any vectors (provided they come from the same vector space, of course),\n",
    "even if they were not used in the training corpus at all. This is achieved by a process called\n",
    "folding-in for LSA, by topic inference for LDA etc.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Calling ``model[corpus]`` only creates a wrapper around the old ``corpus``\n",
    "  document stream -- actual conversions are done on-the-fly, during document iteration.\n",
    "  We cannot convert the entire corpus at the time of calling ``corpus_transformed = model[corpus]``,\n",
    "  because that would mean storing the result in main memory, and that contradicts gensim's objective of memory-indepedence.\n",
    "  If you will be iterating over the transformed ``corpus_transformed`` multiple times, and the\n",
    "  transformation is costly, `serialize the resulting corpus to disk first <corpus-formats>` and continue\n",
    "  using that.</p></div>\n",
    "\n",
    "Transformations can also be serialized, one on top of another, in a sort of chain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation\n",
    "corpus_lsi = lsi_model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transformed our Tf-Idf corpus via `Latent Semantic Indexing <http://en.wikipedia.org/wiki/Latent_semantic_indexing>`_\n",
    "into a latent 2-D space (2-D because we set ``num_topics=2``). Now you're probably wondering: what do these two latent\n",
    "dimensions stand for? Let's inspect with :func:`models.LsiModel.print_topics`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the topics are printed to log -- see the note at the top of this page about activating\n",
    "logging)\n",
    "\n",
    "It appears that according to LSI, \"trees\", \"graph\" and \"minors\" are all related\n",
    "words (and contribute the most to the direction of the first topic), while the\n",
    "second topic practically concerns itself with all the other words. As expected,\n",
    "the first five documents are more strongly related to the second topic while the\n",
    "remaining four documents to the first topic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "for doc, as_text in zip(corpus_lsi, documents):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model persistency is achieved with the :func:`save` and :func:`load` functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n",
    "    lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n",
    "\n",
    "loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "\n",
    "os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question might be: just how exactly similar are those documents to each other?\n",
    "Is there a way to formalize the similarity, so that for a given input document, we can\n",
    "order some other set of documents according to their similarity? Similarity queries\n",
    "are covered in the next tutorial (`sphx_glr_auto_examples_core_run_similarity_queries.py`).\n",
    "\n",
    "\n",
    "Available transformations\n",
    "--------------------------\n",
    "\n",
    "Gensim implements several popular Vector Space Model algorithms:\n",
    "\n",
    "* `Term Frequency * Inverse Document Frequency, Tf-Idf <http://en.wikipedia.org/wiki/Tf%E2%80%93idf>`_\n",
    "  expects a bag-of-words (integer values) training corpus during initialization.\n",
    "  During transformation, it will take a vector and return another vector of the\n",
    "  same dimensionality, except that features which were rare in the training corpus\n",
    "  will have their value increased.\n",
    "  It therefore converts integer-valued vectors into real-valued ones, while leaving\n",
    "  the number of dimensions intact. It can also optionally normalize the resulting\n",
    "  vectors to (Euclidean) unit length.\n",
    "\n",
    " .. sourcecode:: pycon\n",
    "\n",
    "    model = models.TfidfModel(corpus, normalize=True)\n",
    "\n",
    "* `Latent Semantic Indexing, LSI (or sometimes LSA) <http://en.wikipedia.org/wiki/Latent_semantic_indexing>`_\n",
    "  transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into\n",
    "  a latent space of a lower dimensionality. For the toy corpus above we used only\n",
    "  2 latent dimensions, but on real corpora, target dimensionality of 200--500 is recommended\n",
    "  as a \"golden standard\" [1]_.\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)\n",
    "\n",
    "  LSI training is unique in that we can continue \"training\" at any point, simply\n",
    "  by providing more training documents. This is done by incremental updates to\n",
    "  the underlying model, in a process called `online training`. Because of this feature, the\n",
    "  input document stream may even be infinite -- just keep feeding LSI new documents\n",
    "  as they arrive, while using the computed transformation model as read-only in the meanwhile!\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model.add_documents(another_tfidf_corpus)  # now LSI has been trained on tfidf_corpus + another_tfidf_corpus\n",
    "    lsi_vec = model[tfidf_vec]  # convert some new document into the LSI space, without affecting the model\n",
    "\n",
    "    model.add_documents(more_documents)  # tfidf_corpus + another_tfidf_corpus + more_documents\n",
    "    lsi_vec = model[tfidf_vec]\n",
    "\n",
    "  See the :mod:`gensim.models.lsimodel` documentation for details on how to make\n",
    "  LSI gradually \"forget\" old observations in infinite streams. If you want to get dirty,\n",
    "  there are also parameters you can tweak that affect speed vs. memory footprint vs. numerical\n",
    "  precision of the LSI algorithm.\n",
    "\n",
    "  `gensim` uses a novel online incremental streamed distributed training algorithm (quite a mouthful!),\n",
    "  which I published in [5]_. `gensim` also executes a stochastic multi-pass algorithm\n",
    "  from Halko et al. [4]_ internally, to accelerate in-core part\n",
    "  of the computations.\n",
    "  See also `wiki` for further speed-ups by distributing the computation across\n",
    "  a cluster of computers.\n",
    "\n",
    "* `Random Projections, RP <http://www.cis.hut.fi/ella/publications/randproj_kdd.pdf>`_ aim to\n",
    "  reduce vector space dimensionality. This is a very efficient (both memory- and\n",
    "  CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness.\n",
    "  Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset.\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model = models.RpModel(tfidf_corpus, num_topics=500)\n",
    "\n",
    "* `Latent Dirichlet Allocation, LDA <http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>`_\n",
    "  is yet another transformation from bag-of-words counts into a topic space of lower\n",
    "  dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA),\n",
    "  so LDA's topics can be interpreted as probability distributions over words. These distributions are,\n",
    "  just like with LSA, inferred automatically from a training corpus. Documents\n",
    "  are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)\n",
    "\n",
    "  `gensim` uses a fast implementation of online LDA parameter estimation based on [2]_,\n",
    "  modified to run in `distributed mode <distributed>` on a cluster of computers.\n",
    "\n",
    "* `Hierarchical Dirichlet Process, HDP <http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf>`_\n",
    "  is a non-parametric bayesian method (note the missing number of requested topics):\n",
    "\n",
    "  .. sourcecode:: pycon\n",
    "\n",
    "    model = models.HdpModel(corpus, id2word=dictionary)\n",
    "\n",
    "  `gensim` uses a fast, online implementation based on [3]_.\n",
    "  The HDP model is a new addition to `gensim`, and still rough around its academic edges -- use with care.\n",
    "\n",
    "Adding new :abbr:`VSM (Vector Space Model)` transformations (such as different weighting schemes) is rather trivial;\n",
    "see the `apiref` or directly the `Python code <https://github.com/piskvorky/gensim/blob/develop/gensim/models/tfidfmodel.py>`_\n",
    "for more info and examples.\n",
    "\n",
    "It is worth repeating that these are all unique, **incremental** implementations,\n",
    "which do not require the whole training corpus to be present in main memory all at once.\n",
    "With memory taken care of, I am now improving `distributed`,\n",
    "to improve CPU efficiency, too.\n",
    "If you feel you could contribute by testing, providing use-cases or code, see the `Gensim Developer guide <https://github.com/RaRe-Technologies/gensim/wiki/Developer-page>`__.\n",
    "\n",
    "What Next?\n",
    "----------\n",
    "\n",
    "Continue on to the next tutorial on `sphx_glr_auto_examples_core_run_similarity_queries.py`.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    ".. [1] Bradford. 2008. An empirical study of required dimensionality for large-scale latent semantic indexing applications.\n",
    "\n",
    ".. [2] Hoffman, Blei, Bach. 2010. Online learning for Latent Dirichlet Allocation.\n",
    "\n",
    ".. [3] Wang, Paisley, Blei. 2011. Online variational inference for the hierarchical Dirichlet process.\n",
    "\n",
    ".. [4] Halko, Martinsson, Tropp. 2009. Finding structure with randomness.\n",
    "\n",
    ".. [5] Řehůřek. 2011. Subspace tracking for Latent Semantic Analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('run_topics_and_transformations.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
