{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心概念（Core Concepts）\n",
    "\n",
    "### 文档（Document）\n",
    "\n",
    "一段字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:06:30.655104Z",
     "start_time": "2022-05-03T14:06:30.651850Z"
    }
   },
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语料库（Corpus）\n",
    "\n",
    "Corpus指一组Document。\n",
    "\n",
    "#### 原始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:06:31.814635Z",
     "start_time": "2022-05-03T14:06:31.811284Z"
    }
   },
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本处理\n",
    "\n",
    "一般的行为有：\n",
    "\n",
    "- 分词：对于英文来说，相对比较简单，而对于中文则复杂的多。\n",
    "- 去除stopwords\n",
    "- 删除低频词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:06:33.861505Z",
     "start_time": "2022-05-03T14:06:33.854190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200609082203231](images/image-20200609082203231.png)-\n",
    "\n",
    "对于中文，可以类似这样处理。其中分词采用了[jieba](https://github.com/fxsjy/jieba)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:06:42.436879Z",
     "start_time": "2022-05-03T14:06:35.233405Z"
    },
    "attributes": {
     "classes": [
      "shell"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.781 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['自然语言', '处理', '计算机科学', '领域', '人工智能', '领域', '中', '一个', '重要', '方向'],\n",
      " ['研究', '实现', '计算机', '之间', '自然语言', '进行', '有效', '通信', '理论', '方法'],\n",
      " ['自然语言', '处理', '一门', '融', '语言学', '计算机科学', '数学', '一体', '科学'],\n",
      " ['这一', '领域', '研究', '涉及', '自然语言', '日常', '使用', '语言'],\n",
      " ['语言学', '研究', '有着', '密切', '联系', '重要', '区别'],\n",
      " ['自然语言', '处理', '研究', '自然语言'],\n",
      " ['研制', '有效', '实现', '自然语言', '通信', '计算机系统'],\n",
      " ['特别', '软件系统', '计算机科学', '一部分']]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "\n",
    "# 下载停用词\n",
    "response = urllib.request.urlopen('https://raw.githubusercontent.com/goto456/stopwords/master/cn_stopwords.txt')\n",
    "stop_words = [str(word[:-1],'utf-8') for word in response.readlines()]\n",
    "\n",
    "docs =  ['自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。',\n",
    "     '它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。',\n",
    "     '自然语言处理是一门融语言学、计算机科学、数学于一体的科学。',\n",
    "     '因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，',\n",
    "     '所以它与语言学的研究有着密切的联系，但又有重要的区别。',\n",
    "     '自然语言处理并不是一般地研究自然语言，',\n",
    "     '而在于研制能有效地实现自然语言通信的计算机系统，',\n",
    "     '特别是其中的软件系统。因而它是计算机科学的一部分。']\n",
    "\n",
    "corpus = [[word for word in jieba.cut(doc) if word not in stop_words] for doc in docs ]\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200609084113301](images/image-20200609084113301.png)\n",
    "\n",
    "基本处理后，再把语料库用向量的形式来表达，详见下一节。\n",
    "\n",
    "### 向量（Vector）\n",
    "\n",
    "为了能够便于Model的调用，还需要把语料库用向量的形式来表示。而向量化，也意味着语料所占空间大大减少，这样就能处理更大的语料库。\n",
    "\n",
    "#### 字典（Dictionary）\n",
    "\n",
    "首先要构建Dictionary，即创建gensim.corpora.dictionary.Dictionary类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:06:42.970534Z",
     "start_time": "2022-05-03T14:06:42.440258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200609085338894](images/image-20200609085338894.png)\n",
    "\n",
    "Dictionary非常好用，它的详细描述如下。\n",
    "\n",
    "- Attributes\n",
    "  - id2token：Reverse mapping for token2id, initialized in a lazy manner to save memory (not created until needed).\n",
    "  - token2id：token -> tokenId。\n",
    "  - cfs： 每个token出现的频次。\n",
    "  - dfs：每个token出现在多少个文档中。\n",
    "  - num_nnz：每篇文档unique word的个数汇总。\n",
    "  - num_docs： 文档数量。\n",
    "  - num_pos：总共多少个word。\n",
    "- Methods\n",
    "  - doc2bow：把文档转化为BOW格式\n",
    "\n",
    "下面通过一个简单的例子来体验这些属性和方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:06:42.981949Z",
     "start_time": "2022-05-03T14:06:42.972823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n",
      "dict.id2token = {0: 'black', 1: 'cat', 2: 'white', 3: 'outer', 4: 'space', 5: 'dog', 6: 'wag'}\n",
      "--------------------------------------------------\n",
      "dict.token2id = {'black': 0, 'cat': 1, 'white': 2, 'outer': 3, 'space': 4, 'dog': 5, 'wag': 6}\n",
      "--------------------------------------------------\n",
      "dict.cfs = {0: 1, 1: 4, 2: 1, 3: 1, 4: 1, 6: 1, 5: 1}\n",
      "--------------------------------------------------\n",
      "dict.dfs = {0: 1, 1: 2, 2: 1, 3: 1, 4: 1, 6: 1, 5: 1}\n",
      "--------------------------------------------------\n",
      "dict.num_nnz = 8\n",
      "--------------------------------------------------\n",
      "dict.num_docs = 3\n",
      "--------------------------------------------------\n",
      "dict.num_pos = 10\n",
      "--------------------------------------------------\n",
      "dict.doc2bow(docs[0]) = [(0, 1), (1, 2), (2, 1)]\n",
      "--------------------------------------------------\n",
      "dict.doc2idx(docs[0]) = [0, 1, 2, 1]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "docs = [\n",
    "    [\"black\", \"cat\", \"white\", \"cat\"],\n",
    "    [\"cat\", \"outer\", \"space\", \"cat\"],\n",
    "    [\"wag\", \"dog\"]\n",
    "]\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "dict = Dictionary(docs)\n",
    "\n",
    "# 由于id2token是lazy manner， 所以先调用任何一个元素，触发生成id2token\n",
    "print(dict[0])\n",
    "print(\"dict.id2token =\", dict.id2token)   \n",
    "print('-'*50)\n",
    "print(\"dict.token2id =\", dict.token2id)\n",
    "print('-'*50)\n",
    "print(\"dict.cfs =\", dict.cfs)\t\t\t\n",
    "print('-'*50)\n",
    "print(\"dict.dfs =\", dict.dfs)\t\t\t# \n",
    "print('-'*50)\n",
    "print(\"dict.num_nnz =\", dict.num_nnz)\n",
    "print('-'*50)\n",
    "print(\"dict.num_docs =\", dict.num_docs)\n",
    "print('-'*50)\n",
    "print(\"dict.num_pos =\", dict.num_pos)\n",
    "print('-'*50)\n",
    "\n",
    "print(\"dict.doc2bow(docs[0]) =\", dict.doc2bow(docs[0]))\n",
    "print('-'*50)\n",
    "print(\"dict.doc2idx(docs[0]) =\", dict.doc2idx(docs[0]))\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果如下：\n",
    "\n",
    "![image-20200612231218544](images/image-20200612231218544.png)\n",
    "\n",
    "#### BOW（Bag of Word）\n",
    "\n",
    "通过统计文档的BOW来，重新构建语料库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:06:42.990898Z",
     "start_time": "2022-05-03T14:06:42.984545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200609093123150](images/image-20200609093123150.png)\n",
    "\n",
    "每篇文档由一个向量表示，由于文本的稀疏性，仅仅记录哪些数量大于零的word。\n",
    "\n",
    "### 模型（Model）\n",
    "\n",
    "在Gensim，把Model看成是一种转换（Transformation），即把语料库的向量空间（vector space）转化为模型所在的向量空间。比如：  [tf-idf](https://en.wikipedia.org/wiki/Tf–idf)。它根据word在语料库中的稀缺性对BOW进行加权。\n",
    "\n",
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:06:42.998470Z",
     "start_time": "2022-05-03T14:06:42.993228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1), (11, 1)]\n",
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "doc = dictionary.doc2bow(words)\n",
    "print(doc)\n",
    "print(tfidf[doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200609094514104](images/image-20200609094514104.png)\n",
    "\n",
    "> The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus.\n",
    "\n",
    "进一步可以获得基于tf-idf的语料库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:38:36.627726Z",
     "start_time": "2022-05-03T14:38:36.619654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n",
      "--------------------------------------------------\n",
      "[[(0, 0.5774), (1, 0.5774), (2, 0.5774)],\n",
      " [(0, 0.4442), (3, 0.4442), (4, 0.4442), (5, 0.3245), (6, 0.4442), (7, 0.3245)],\n",
      " [(2, 0.571), (5, 0.4171), (7, 0.4171), (8, 0.571)],\n",
      " [(1, 0.4918), (5, 0.7185), (8, 0.4918)],\n",
      " [(3, 0.6283), (6, 0.6283), (7, 0.4589)],\n",
      " [(9, 1.0)],\n",
      " [(9, 0.7071), (10, 0.7071)],\n",
      " [(9, 0.508), (10, 0.508), (11, 0.6955)],\n",
      " [(4, 0.6283), (10, 0.4589), (11, 0.6283)]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_corpus =  tfidf[bow_corpus]\n",
    "print('-'*50)\n",
    "pprint(bow_corpus)\n",
    "print('-'*50)\n",
    "pprint([[(id, round(tfidf, 4))for id, tfidf in doc] for doc in tfidf_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200609095802255](images/image-20200609095802255.png)\n",
    "\n",
    "除了tf-idf模型，gensim还支持很多其它模型，比如：LSI（[Latent Semantic Indexing](https://en.wikipedia.org/wiki/Latent_semantic_indexing)），RP（[Random Projections](http://www.cis.hut.fi/ella/publications/randproj_kdd.pdf)）, LDA（[Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)），HDP（[Hierarchical Dirichlet Process](http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf)）。下面介绍一下LSI。\n",
    "\n",
    "#### LSI\n",
    "\n",
    "全称[Latent Semantic Indexing(或 Latent Semantic Analysis )](https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing). 它采用奇异值分解（Singular Value Decomposition）对文档矩阵进行分解。下面代码中选取6个最大的特征值（主题）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:38:39.438820Z",
     "start_time": "2022-05-03T14:38:39.428368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.066), (1, -0.5201)]\n",
      "[(0, 0.1967), (1, -0.761)]\n",
      "[(0, 0.0899), (1, -0.7242)]\n",
      "[(0, 0.0759), (1, -0.6321)]\n",
      "[(0, 0.1015), (1, -0.5737)]\n",
      "[(0, 0.7032), (1, 0.1612)]\n",
      "[(0, 0.8775), (1, 0.1676)]\n",
      "[(0, 0.9099), (1, 0.1409)]\n",
      "[(0, 0.6166), (1, -0.0539)]\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=2)  \n",
    "lsi_corpus = lsi[tfidf_corpus]  \n",
    "for doc in lsi_corpus:\n",
    "    print([(i, round(w,4)) for i, w in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200613132419899](images/image-20200613132419899.png)\n",
    "\n",
    "#### 保存和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:38:43.446318Z",
     "start_time": "2022-05-03T14:38:43.439342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/model-sm2_j6iy.lsi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='model-', suffix='.lsi', delete=False) as tmp:\n",
    "    print(tmp.name)\n",
    "    lsi.save(tmp.name)  # same for tfidf, lda, ...\n",
    "\n",
    "loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "os.unlink(tmp.name)\t# 删除文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相似性（Similarity）\n",
    "\n",
    "接下来，我们就可以进行相似性比较了。比如，下面使用[SparseMatrixSimilarity](https://tedboy.github.io/nlps/generated/generated/gensim.similarities.SparseMatrixSimilarity.html)，它采用cosine similarity来计算向量之间的相似性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:38:45.475907Z",
     "start_time": "2022-05-03T14:38:45.457303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.055935505979526706), (1, -0.40537266758066764)]\n",
      "-------------------------tfidf-------------------------\n",
      "(0, 0.81649655) Human machine interface for lab abc computer applications\n",
      "(3, 0.3477732) A survey of user opinion of computer system response time\n",
      "(1, 0.31412902) The EPS user interface management system\n",
      "(2, 0.0) System and human system engineering testing of EPS\n",
      "(4, 0.0) Relation of user perceived response time to error measurement\n",
      "(5, 0.0) The generation of random binary unordered trees\n",
      "(6, 0.0) The intersection graph of paths in trees\n",
      "(7, 0.0) Graph minors IV Widths of trees and well quasi ordering\n",
      "(8, 0.0) Graph minors A survey\n",
      "-------------------------lsi-------------------------\n",
      "(0, 0.9999408) Human machine interface for lab abc computer applications\n",
      "(2, 0.99990785) A survey of user opinion of computer system response time\n",
      "(3, 0.99984384) The EPS user interface management system\n",
      "(4, 0.9992786) System and human system engineering testing of EPS\n",
      "(1, 0.99330217) Relation of user perceived response time to error measurement\n",
      "(8, 0.2224844) The generation of random binary unordered trees\n",
      "(7, -0.016480923) The intersection graph of paths in trees\n",
      "(6, -0.0515742) Graph minors IV Widths of trees and well quasi ordering\n",
      "(5, -0.08804217) Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "tfidf_index = similarities.Similarity(\"tfidf\", tfidf_corpus, num_features=len(dictionary))\n",
    "lsi_index = similarities.MatrixSimilarity(lsi_corpus, num_features=len(dictionary))\n",
    "\n",
    "query_doc = 'Human computer interaction'.lower().split()\n",
    "query_bow = dictionary.doc2bow(query_doc)\n",
    "\n",
    "tfidf_query = tfidf[query_bow]\n",
    "tfidf_sims = tfidf_index[tfidf_query]\n",
    "\n",
    "lsi_query = lsi[tfidf_query]\n",
    "print(lsi_query)\n",
    "lsi_sims = lsi_index[lsi_query]\n",
    "\n",
    "# 排序\n",
    "print('-'*25, \"tfidf\" , '-'*25, sep=\"\")\n",
    "sims = sorted(enumerate(tfidf_sims), key=lambda item: -item[1])\n",
    "for i, s in enumerate(sims):\n",
    "    print(s, text_corpus[i])    \n",
    "    \n",
    "print('-'*25, \"lsi\" , '-'*25, sep=\"\")    \n",
    "sims = sorted(enumerate(lsi_sims), key=lambda item: -item[1])\n",
    "for i, s in enumerate(sims):\n",
    "    print(s, text_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200613132402805](images/image-20200613132402805.png)\n",
    "\n",
    "上面的结果可以看出，LSI可以使得相似性更加平滑的显示。\n",
    "\n",
    "#### 其它Similarity类\n",
    "\n",
    "- [MatrixSimilarity](https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.MatrixSimilarity)\n",
    "\n",
    "除了[SparseMatrixSimilarity](https://tedboy.github.io/nlps/generated/generated/gensim.similarities.SparseMatrixSimilarity.html)，gensim还包含其它相似性计算的类，比如：[MatrixSimilarity](https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.MatrixSimilarity)，[WmdSimilarity](https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.WmdSimilarity)。\n",
    "\n",
    "### Corpus Streaming\n",
    "\n",
    "上面的例子中，语料库被完整的加载到内存中，然后，实际工作中，语料库往往包含几百万甚至更多的文档，没有办法能够一次加载到内存中来，所以必须要采用Streaming的方式来读取数据。下面是一个例子。\n",
    "\n",
    "首先创建语料库文件。每一行是一篇文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:38:51.231827Z",
     "start_time": "2022-05-03T14:38:51.209751Z"
    },
    "attributes": {
     "classes": [
      "shell"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human machine interface for lab abc computer applications\n",
      "A survey of user opinion of computer system response time\n",
      "The EPS user interface management system\n",
      "System and human system engineering testing of EPS\n",
      "Relation of user perceived response time to error measurement\n",
      "The generation of random binary unordered trees\n",
      "The intersection graph of paths in trees\n",
      "Graph minors IV Widths of trees and well quasi ordering\n",
      "Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p data\n",
    "cat << EOF > data/mycorpus.txt\n",
    "Human machine interface for lab abc computer applications\n",
    "A survey of user opinion of computer system response time\n",
    "The EPS user interface management system\n",
    "System and human system engineering testing of EPS\n",
    "Relation of user perceived response time to error measurement\n",
    "The generation of random binary unordered trees\n",
    "The intersection graph of paths in trees\n",
    "Graph minors IV Widths of trees and well quasi ordering\n",
    "Graph minors A survey\n",
    "EOF\n",
    "\n",
    "cat data/mycorpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取语料库，构建Dictionary。采用Streaming的方式来读取原始语料库文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:40:32.606458Z",
     "start_time": "2022-05-03T14:40:32.595052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_ids = [9, 3, 19, 8, 18, 26, 33]\n",
      "once_ids = [7, 3, 6, 0, 1, 10, 17, 20, 21, 25, 24, 26, 22, 23, 28, 29, 27, 31, 34, 35, 33, 36, 41, 40, 39, 38]\n",
      "Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...>\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from six import iteritems\n",
    "from smart_open import open # for transparently opening remote files\n",
    "\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('data/mycorpus.txt'))\n",
    "\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "\n",
    "# 移除stop words,和只出现一次的words\n",
    "print(\"stop_ids =\", stop_ids)\n",
    "print(\"once_ids =\", once_ids)\n",
    "dictionary.filter_tokens(stop_ids + once_ids) \n",
    "\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()  \n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200609125110444](images/image-20200609125110444.png)\n",
    "\n",
    "把语料库向量化。每次仅读取一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:43:37.516002Z",
     "start_time": "2022-05-03T14:43:37.508884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x7f88f1c4c8e0>\n",
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "class MyCorpus(object):\n",
    "    def __init__(self, file_path='data/mycorpus.txt'):\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        with open(self.file_path) as f:\n",
    "            for i, _ in enumerate(f):\n",
    "                pass\n",
    "        return i + 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for line in open(self.file_path):\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "\n",
    "corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)\n",
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    print(vector)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到和之前相同的corpus。\n",
    "\n",
    "![image-20200609125418965](images/image-20200609125418965.png)\n",
    "\n",
    "### Corpus格式\n",
    "\n",
    "gensim可以serialize语料库，支持多种文件格式，比如：[Market Matrix format](http://math.nist.gov/MatrixMarket/formats.html)，[Joachim’s SVMlight format](http://svmlight.joachims.org/), [Blei’s LDA-C format](https://www.cs.princeton.edu/~blei/lda-c/) ，[GibbsLDA++ format](http://gibbslda.sourceforge.net/)。这些格式都是采用Streaming方式加载数据的。\n",
    "\n",
    "- 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:45:48.276245Z",
     "start_time": "2022-05-03T14:45:48.268456Z"
    }
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('/tmp/corpus.mm', corpus_memory_friendly)\n",
    "corpora.SvmLightCorpus.serialize('/tmp/corpus.svmlight', corpus_memory_friendly)\n",
    "corpora.BleiCorpus.serialize('/tmp/corpus.lda', corpus_memory_friendly)\n",
    "corpora.LowCorpus.serialize('/tmp/corpus.low', corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:48:21.425496Z",
     "start_time": "2022-05-03T14:48:21.409115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "MmCorpus(9 documents, 12 features, 28 non-zero entries)\n",
      "[(0, 1.0), (1, 1.0), (2, 1.0)]\n",
      "[(0, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(2, 1.0), (5, 1.0), (7, 1.0), (8, 1.0)]\n",
      "[(1, 1.0), (5, 2.0), (8, 1.0)]\n",
      "[(3, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(9, 1.0)]\n",
      "[(9, 1.0), (10, 1.0)]\n",
      "[(9, 1.0), (10, 1.0), (11, 1.0)]\n",
      "[(4, 1.0), (10, 1.0), (11, 1.0)]\n",
      "--------------------------------------------------\n",
      "<gensim.corpora.svmlightcorpus.SvmLightCorpus object at 0x7f88f1c3e4f0>\n",
      "[(0, 1.0), (1, 1.0), (2, 1.0)]\n",
      "[(0, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(2, 1.0), (5, 1.0), (7, 1.0), (8, 1.0)]\n",
      "[(1, 1.0), (5, 2.0), (8, 1.0)]\n",
      "[(3, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(9, 1.0)]\n",
      "[(9, 1.0), (10, 1.0)]\n",
      "[(9, 1.0), (10, 1.0), (11, 1.0)]\n",
      "[(4, 1.0), (10, 1.0), (11, 1.0)]\n",
      "--------------------------------------------------\n",
      "<gensim.corpora.bleicorpus.BleiCorpus object at 0x7f88f1ba9e50>\n",
      "[(0, 1.0), (1, 1.0), (2, 1.0)]\n",
      "[(0, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(2, 1.0), (5, 1.0), (7, 1.0), (8, 1.0)]\n",
      "[(1, 1.0), (5, 2.0), (8, 1.0)]\n",
      "[(3, 1.0), (6, 1.0), (7, 1.0)]\n",
      "[(9, 1.0)]\n",
      "[(9, 1.0), (10, 1.0)]\n",
      "[(9, 1.0), (10, 1.0), (11, 1.0)]\n",
      "[(4, 1.0), (10, 1.0), (11, 1.0)]\n",
      "--------------------------------------------------\n",
      "<gensim.corpora.lowcorpus.LowCorpus object at 0x7f88f24f97f0>\n",
      "[(0, 1), (1, 1), (4, 1)]\n",
      "[(0, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n",
      "[(4, 1), (7, 1), (9, 1), (10, 1)]\n",
      "[(1, 1), (7, 2), (10, 1)]\n",
      "[(5, 1), (8, 1), (9, 1)]\n",
      "[(11, 1)]\n",
      "[(11, 1), (2, 1)]\n",
      "[(11, 1), (2, 1), (3, 1)]\n",
      "[(6, 1), (2, 1), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "print('-'*50)\n",
    "corpus = corpora.MmCorpus('/tmp/corpus.mm')\n",
    "print(corpus)\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "    \n",
    "print('-'*50)\n",
    "corpus = corpora.SvmLightCorpus('/tmp/corpus.svmlight')\n",
    "print(corpus)\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "    \n",
    "print('-'*50)\n",
    "corpus = corpora.BleiCorpus('/tmp/corpus.lda')\n",
    "print(corpus)\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "    \n",
    "print('-'*50)\n",
    "corpus = corpora.LowCorpus('/tmp/corpus.low')\n",
    "print(corpus)\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image-20200609141658160](images/image-20200609141658160.png)\n",
    "\n",
    "### 兼容NumPy和SciPy\n",
    "\n",
    "gensim的corpus的格式和Numpy和SciPy中的矩阵并不相同，但可以采用[matutils](https://radimrehurek.com/gensim/matutils.html)进行相互转化。首先看一般的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:50:35.540148Z",
     "start_time": "2022-05-03T14:50:35.532834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy_matrix =\n",
      " [[8 1]\n",
      " [7 0]\n",
      " [5 0]\n",
      " [7 9]\n",
      " [1 4]]\n",
      "corpus = <gensim.matutils.Dense2Corpus object at 0x7f88f1b85a00>\n",
      "[(0, 8.0), (1, 7.0), (2, 5.0), (3, 7.0), (4, 1.0)]\n",
      "[(0, 1.0), (3, 9.0), (4, 4.0)]\n",
      "new_matrix =\n",
      " [[8 1]\n",
      " [7 0]\n",
      " [5 0]\n",
      " [7 9]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(20200609)\n",
    "numpy_matrix = np.random.randint(10, size=[5, 2])  # random matrix as an example\n",
    "print(\"numpy_matrix =\\n\", numpy_matrix)\n",
    "\n",
    "# 把dense matrix转为为 corpus\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "print(\"corpus =\",corpus)\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "\n",
    "# 把corpus转化为 dense matrix\n",
    "new_matrix = gensim.matutils.corpus2dense(corpus, num_terms=5)\n",
    "print(\"new_matrix =\\n\", numpy_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200609150517053](images/image-20200609150517053.png)\n",
    "\n",
    "再看稀疏矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T14:53:27.609591Z",
     "start_time": "2022-05-03T14:53:27.601593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy_sparse_matrix =\n",
      "   (4, 0)\t0.6501933321119864\n",
      "  (3, 0)\t0.16918575812686232\n",
      "  (4, 1)\t0.9668114763742701\n",
      "  (1, 1)\t0.6894940466416551\n",
      "  (2, 0)\t0.6503421133138086\n",
      "  (0, 1)\t0.8242847957249896\n",
      "corpus = <gensim.matutils.Sparse2Corpus object at 0x7f88f1b858e0>\n",
      "[(2, 0.6503421133138086), (3, 0.16918575812686232), (4, 0.6501933321119864)]\n",
      "[(0, 0.8242847957249896), (1, 0.6894940466416551), (4, 0.9668114763742701)]\n",
      "new_matrix =\n",
      "   (2, 0)\t0.6503421133138086\n",
      "  (3, 0)\t0.16918575812686232\n",
      "  (4, 0)\t0.6501933321119864\n",
      "  (0, 1)\t0.8242847957249896\n",
      "  (1, 1)\t0.6894940466416551\n",
      "  (4, 1)\t0.9668114763742701\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "np.random.seed(20200609)\n",
    "scipy_sparse_matrix = scipy.sparse.random(5, 2, density=0.6) \n",
    "print(\"scipy_sparse_matrix =\\n\", scipy_sparse_matrix)\n",
    "\n",
    "# 把sparse matrix 转化为 corpus\n",
    "corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)\n",
    "print(\"corpus =\", corpus)\n",
    "for doc in corpus:\n",
    "    print(doc)\n",
    "\n",
    "# 把corpus转化为 sparse matrix\n",
    "new_matrix = gensim.matutils.corpus2csc(corpus)\n",
    "print(\"new_matrix =\\n\", new_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image-20200609151431396](images/image-20200609151431396.png)\n",
    "\n",
    "需要注意的是corpus转化的矩阵为term-document矩阵，即行代表term，列代表document.\n",
    "\n",
    "\n",
    "\n",
    "## 参考\n",
    "\n",
    "- [Core Concepts](https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html)\n",
    "- [Corpora and Vector Spaces](https://tedboy.github.io/nlps/gensim_tutorial/tut1.html)\n",
    "- [Topics and Transformations](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html)\n",
    "- [Similarity Queries](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html#sphx-glr-auto-examples-core-run-similarity-queries-py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
