# ![image-20210927143735136](images/image-20210927143735136.png)

- Pythonè½¯ä»¶åŒ…

![image-20210902105438767](images/image-20210902105438767.png)

- spaCyçš„Githubå¼€æºåœ°å€

  https://github.com/explosion/spaCy

- ä»£ç åœ°å€ï¼š

  [Mastering-spaCy](https:// github.com/PacktPublishing/Mastering-spaCy)

- [25-match](..\..\..\25-match) ä¹¦ä¸­æ‰€æœ‰çš„å›¾ç‰‡

  https://static.packt-cdn.com/downloads/9781800563353_ColorImages.pdf

- æŸ¥çœ‹spaCyçš„å®‰è£…ä¿¡æ¯

  ~~~shell
  python3 -m spacy -info
  ~~~

  ![image-20210902120827965](images/image-20210902120827965.png)

![image-20210902111938724](images/image-20210902111938724.png)

# äº†è§£spaCY

## spaCy vs. NLTK vs. CoreNLP

![image-20210902113855731](images/image-20210902113855731.png)

## Visualization with displaCy

### Getting started with displaCy

åœ¨çº¿å¯è§†åŒ–ï¼š[explosion.ai/demos/displacy](https://explosion.ai/demos/displacy?text=%E4%BB%8A%E5%A4%A9%E5%A4%A9%E6%B0%94%E4%B8%8D%E9%94%99%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%8B%E5%8D%88%E6%B2%A1%E6%9C%89%E8%AF%BE&model=zh_core_web_sm&cpu=1&cph=1)

![image-20210902114800174](images/image-20210902114800174.png)

### Entity visualizer

https://explosion.ai/demos/displacy-ent/

![image-20210902120927346](images/image-20210902120927346.png)

### Visualizing within Python

å¯è§†åŒ–ä¾èµ–å…³ç³»ã€‚

~~~python
import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_md')
doc = nlp('I own a ginger cat.')
displacy.serve(doc, style='dep', port=6010)
~~~

![image-20210902121156132](images/image-20210902121156132.png)

æ‰“å¼€æµè§ˆå™¨`http://[jupyter notebook ip]:6010/`å¯ä»¥å¦‚ä¸Šçœ‹åˆ°ç»“æœã€‚

ä¹Ÿå¯ä»¥å¯è§†åŒ–å®ä½“ã€‚

~~~python
import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_md')
doc= nlp('Bill Gates is the CEO of Microsoft.')
displacy.serve(doc, style='ent', port=6010)
~~~

![image-20210902122219849](images/image-20210902122219849.png)

ä¸Šé¢çš„åŠŸèƒ½ä¹Ÿå¯ä»¥åœ¨jupyter notebookä¸­æ‰§è¡Œï¼Œåªéœ€è¦å‡½æ•°ä¿®æ”¹æˆrenderã€‚

~~~python
import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_md')
doc= nlp('Bill Gates is the CEO of Microsoft.')
displacy.render(doc, style='dep')
~~~

æœ€åï¼Œè¿˜å¯ä»¥æŠŠä¸Šé¢çš„ç»“æœè¾“å‡ºåˆ°å›¾ç‰‡ã€‚

~~~python
import spacy
from spacy import displacy
from pathlib import Path
nlp = spacy.load('en_core_web_md')
doc = nlp('I''m a butterfly.')
svg = displacy.render(doc, style='dep', jupyter=False)
          
file_name = 'butterfly.svg'
file_path = './output/' + file_name
output_path = Path (file_path)
output_path.open('w', encoding='utf-8').write(svg) 
~~~

> SVG æ˜¯ä¸€ç§åŸºäº XML è¯­æ³•çš„å›¾åƒæ ¼å¼ï¼Œå…¨ç§°æ˜¯å¯ç¼©æ”¾çŸ¢é‡å›¾ï¼ˆScalable Vector Graphicsï¼‰ã€‚å…¶ä»–å›¾åƒæ ¼å¼éƒ½æ˜¯åŸºäºåƒç´ å¤„ç†çš„ï¼ŒSVG åˆ™æ˜¯å±äºå¯¹å›¾åƒçš„å½¢çŠ¶æè¿°ï¼Œæ‰€ä»¥å®ƒæœ¬è´¨ä¸Šæ˜¯æ–‡æœ¬æ–‡ä»¶ï¼Œä½“ç§¯è¾ƒå°ï¼Œä¸”ä¸ç®¡æ”¾å¤§å¤šå°‘å€éƒ½ä¸ä¼šå¤±çœŸã€‚

## Online Demo

https://explosion.ai/software#demos

![image-20210907111842846](images/image-20210907111842846.png)

# spaCyæ ¸å¿ƒæ“ä½œ

## spaCyæ¦‚è§ˆ

### spaCyçš„pipeline

![image-20210902141240984](images/image-20210902141240984.png)

![image-20210902141317691](images/image-20210902141317691.png)

| NAME           | COMPONENT                                                    | CREATES                                                   | DESCRIPTION                                      |
| -------------- | ------------------------------------------------------------ | --------------------------------------------------------- | ------------------------------------------------ |
| **tokenizer**  | [`Tokenizer`](https://spacy.io/api/tokenizer)                | `Doc`                                                     | Segment text into tokens.                        |
| **tagger**     | [`Tagger`](https://spacy.io/api/tagger)                      | `Token.tag`                                               | Assign part-of-speech tags.                      |
| **parser**     | [`DependencyParser`](https://spacy.io/api/dependencyparser)  | `Token.head`, `Token.dep`, `Doc.sents`, `Doc.noun_chunks` | Assign dependency labels.                        |
| **ner**        | [`EntityRecognizer`](https://spacy.io/api/entityrecognizer)  | `Doc.ents`, `Token.ent_iob`, `Token.ent_type`             | Detect and label named entities.                 |
| **lemmatizer** | [`Lemmatizer`](https://spacy.io/api/lemmatizer)              | `Token.lemma`                                             | Assign base forms.                               |
| **textcat**    | [`TextCategorizer`](https://spacy.io/api/textcategorizer)    | `Doc.cats`                                                | Assign document labels.                          |
| **custom**     | [custom components](https://spacy.io/usage/processing-pipelines#custom-components) | `Doc._.xxx`, `Token._.xxx`, `Span._.xxx`                  | Assign custom attributes, methods or properties. |

### spaCyæ¶æ„

spaCyæ¶æ„å›¾å¦‚ä¸‹ã€‚

![image-20210902143008512](images/image-20210902143008512.png)

## æ ‡è®°åŒ–ï¼ˆtokenizationï¼‰

~~~python
import spacy
nlp = spacy.load("en_core_web_md")
doc = nlp("Im ordering Rm2-0866-000cn, can-0866-000cn, K0q14-60001-bulk To my hfpu. ")
print([token.text for token in doc])
~~~

![image-20210902144006795](images/image-20210902144006795.png)

å¯ä»¥æ³¨æ„åˆ°ï¼š

- Let'sè¢«æ‹†åˆ†æˆäº†Letå’Œ'sä¸¤ä¸ªtokenã€‚
- å¯¹äºé•¿çš„å­—ç¬¦ä¸²ï¼Œä¹Ÿè¢«æ‹†åˆ†æˆå¤šä¸ªtokenã€‚

tokenizationå¹¶ä¸ä¾èµ–äºç»Ÿè®¡æ¨¡å‹ï¼Œè€Œæ˜¯åŸºäºå„ä¸ªè¯­è¨€çš„ä¸€äº›è§„åˆ™ã€‚Tokenizer exceptionsä¸­å®šä¹‰äº†ä¸€äº›å¼‚å¸¸çš„è§„åˆ™ã€‚æ¯”å¦‚ {ORTH: "n't", NORM: "not"}ã€‚

![image-20210902145157283](images/image-20210902145157283.png)

è¯¦è§: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py

![image-20210902145650992](images/image-20210902145650992.png)

### è‡ªå®šä¹‰tokenzier

~~~python
import spacy
from spacy.symbols import ORTH
nlp = spacy.load("en_core_web_md")
doc = nlp("lemme that")
print([w.text for w in doc])

special_case = [{ORTH: "lem"}, {ORTH: "me"}]
nlp.tokenizer.add_special_case("lemme", special_case)
print([w.text for w in nlp("lemme that")])
~~~

![image-20210902150131397](images/image-20210902150131397.png)

### Debugging the tokenizer

å¯ä»¥é€šè¿‡nlp.tokenizer.explainæ¥ç†è§£tokenizationã€‚

~~~python
import spacy
nlp = spacy.load("en_core_web_md")
text = "Let's go!"
doc = nlp(text)
tok_exp = nlp.tokenizer.explain(text)
for t in tok_exp:
    print(t[1], "\t", t[0])
~~~

![image-20210902151752085](images/image-20210902151752085.png)

### å¥å­æ‹†åˆ†

å¥å­æ‹†åˆ†ï¼ˆSentence segmentationï¼‰æŠŠæ–‡æœ¬æ‰åˆ†æˆå¥å­ã€‚

~~~python
import spacy
nlp = spacy.load("en_core_web_md")
text = "I flied to N.Y yesterday. It was around 5 pm."
doc = nlp(text)
for sent in doc.sents:
    print(sent.text)
~~~

![image-20210902152049451](images/image-20210902152049451.png)

å¥å­æ‹†åˆ†æ¯”tokenizationæ›´åŠ çš„å¤æ‚ã€‚spaCyä½¿ç”¨dependency parseræ¥è¿›è¡Œå¥å­æ‹†åˆ†ï¼Œè¿™æ˜¯spaCyç‹¬ç‰¹çš„ç‰¹æ€§ã€‚

## è¯å½¢è¿˜åŸï¼ˆlemmatizationï¼‰

### Lemmatizationä»‹ç» 

è¯å½¢è¿˜åŸï¼ˆlemmatizationï¼‰è·å–åŸè¯ï¼ˆlemmaï¼‰çš„è¿‡ç¨‹ã€‚ä¸€ä¸ªlemmaæ˜¯ä¸€ä¸ªtokençš„åŸºç¡€å½¢å¼ã€‚

~~~python
import spacy
nlp = spacy.load("en_core_web_md")
doc = nlp("I went there for working and worked for 3 years.")
for token in doc:
    print(token.text, token.lemma_)
~~~

![image-20210902152818677](images/image-20210902152818677.png)

åŒæ ·ä¹Ÿå¯ä»¥è‡ªå®šä¹‰lemmatization

~~~python
import spacy
from spacy.symbols import ORTH, NORM
nlp = spacy.load('en_core_web_md')

special_case = [{ORTH: 'Angeltown', NORM: 'Los Angeles'}]
nlp.tokenizer.add_special_case(u'Angeltown', special_case)

doc = nlp(u'I am flying to Angeltown')
for token in doc:
    print(token.text, token.norm_, token.lemma_)
~~~

![image-20210902155915664](images/image-20210902155915664.png)

> åœ¨spaCyä¸­ä¼¼ä¹ç”¨NORMæ›¿æ¢äº†LEMMAï¼Œä¹¦ä¸­çš„ä»£ç å¦‚ä¸‹ï¼Œä½†ä¼šæŠ¥é”™ã€‚
>
> ~~~python
> import spacy
> from spacy.symbols import ORTH, LEMMA
> nlp = spacy.load('en_core_web_md')
> 
> special_case = [{ORTH: 'Angeltown', LEMMA: 'Los Angeles'}]
> nlp.tokenizer.add_special_case(u'Angeltown', special_case)
> 
> doc = nlp(u'I am flying to Angeltown')
> for token in doc:
>     print(token.text, token.lemma_)
> ~~~
>
> ![image-20210902160206953](images/image-20210902160206953.png)

### lemmatization vs. stemming

lemmaä»£è¡¨åŸè¯ï¼ŒæŒ‡wordçš„åŸºç¡€å½¢å¼ï¼Œè€Œstemæ˜¯ä»£è¡¨wordå«ä¹‰çš„æœ€å°éƒ¨åˆ†ï¼Œå®ƒå¾€å¾€å¹¶ä¸æ˜¯ä¸€ä¸ªvalidçš„wordã€‚

| Word         | Lemma        | Stem     |
| ------------ | ------------ | -------- |
| university   | university   | univers  |
| universe     | universe     | univer   |
| universal    | universal    | univers  |
| universities | university   | universi |
| universes    | universe     | univers  |
| improvement  | improvement  | improv   |
| improvements | improvements | improv   |
| improves     | improve      | improv   |

ä¸Šé¢çš„ä¾‹å­å¯ä»¥çœ‹å‡ºï¼Œä¸€ä¸ªstemå¯ä»¥æ˜ å°„åˆ°å¤šä¸ªlemmaï¼Œä¸€ä¸ªlemmaå¯ä»¥æ˜ å°„åˆ°å¤šä¸ªwordã€‚

Lemmatizationä¸€èˆ¬é€šè¿‡å­—å…¸æŸ¥æ‰¾æ¥è·å–åŸè¯ï¼Œè€ŒStemmingç®—æ³•å¾€å¾€é€šè¿‡è·å–å‰ç¼€æˆ–è€…åæœ€æ¥è·å–stemï¼Œæœ‰äº›ç²—ç³™ã€‚è‹±æ–‡å¸¸è§çš„stemmingç®—æ³•æœ‰Porterå’ŒLancasterï¼Œç›¸å…³demoå‚è§[NLTK's demo](https://text-processing.com/demo/stem/)ã€‚

![image-20210902162309560](images/image-20210902162309560.png)

## spaCy container objects

### [Doc](https://spacy.io/api/doc)

å¯ä»¥é€šè¿‡to_jsonè¾“å‡ºï¼Œæ–¹ä¾¿æŸ¥çœ‹Docé‡Œé¢çš„å†…å®¹

~~~
import spacy
from spacy.symbols import ORTH, LEMMA
nlp = spacy.load('en_core_web_md')

doc = nlp("I'm a Chinese")
doc.to_json()
~~~

![image-20210902163631017](images/image-20210902163631017.png)

### [Token](https://spacy.io/api/token)

~~~python
import spacy 
import utils

nlp = spacy.load("en_core_web_md")
doc = nlp("I'm a Chinese. However, you are a Japanese. DCBA")

df_token = utils.tokens_to_df(doc, is_display=False) 
df_token.index = list(df_token.text)
df_token.pop('text')
df_token.T
~~~

![image-20210906154256560](images/image-20210906154256560.png)

- text_with_ws: æŒ‡åŒ…å«ç©ºæ ¼çš„text
- is_stop: æ˜¯å¦æ˜¯stop_wordsã€‚é»˜è®¤çš„è‹±æ–‡stopwordså®šä¹‰åœ¨https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py

### [Span](https://spacy.io/api/span)

Spanè¡¨ç¤ºæ–‡æœ¬çš„ä¸€éƒ¨ä»½ï¼Œå¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ªå°çš„Docå¯¹è±¡ã€‚

~~~python
import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp("You went there after you saw me")
span = doc[2:-1]
print(type(span), span)

sub_span1 = span[2:4]
print(type(sub_span1), sub_span1)

sub_span2 = span.char_span(0, 2)   # ä¸èƒ½å·¥ä½œï¼Œä¸çŸ¥é“å•¥å…ƒå©´
print(type(sub_span2), sub_span2)
~~~

![image-20210903122121504](images/image-20210903122121504.png)

Spanå¯ä»¥è½¬åŒ–ä¸ºDocã€‚

~~~python
doc1 = span.as_doc()
print(type(doc1), doc1)
~~~

![image-20210903121126136](images/image-20210903121126136.png)



# è¯­è¨€ç‰¹å¾ï¼ˆLinguistic Featuresï¼‰

## è¯æ€§æ ‡æ³¨ï¼ˆPOS taggingï¼‰

POS taggingå³part-of-speech taggingï¼ŒæŒ‡ä¸ºåˆ†è¯åçš„æ¯ä¸ªå•è¯æ ‡æ³¨ä¸€ä¸ªæ­£ç¡®çš„è¯æ€§çš„è¿‡ç¨‹ã€‚

spaCYä¸­ä½¿ç”¨çš„è¯æ€§å¦‚ä¸‹ï¼š

| è¯æ€§ç¼©å†™                                                     | è¯æ€§                     | è¯´æ˜       | ç¤ºä¾‹                               |
| ------------------------------------------------------------ | ------------------------ | ---------- | ---------------------------------- |
| [ADJ](https://universaldependencies.org/docs/u/pos/ADJ.html) | adjective                | å½¢å®¹è¯çš„   | big, old, good, first, useful      |
| [ADP](https://universaldependencies.org/docs/u/pos/ADP.html) | adposition/preposition   | ä»‹è¯       | in, to, on, during                 |
| [ADV](https://universaldependencies.org/docs/u/pos/ADV.html) | adverb                   | å‰¯è¯       | very, tomorrow, down, where, there |
| [AUX](https://universaldependencies.org/docs/u/pos/AUX_.html) | auxiliaryverb            | åŠ©åŠ¨è¯     | is, has, will, should              |
| [CONJ](https://universaldependencies.org/docs/u/pos/CONJ.html) | conjunction              | è¿æ¥è¯     | for,and,nor,but,or,yet,so          |
| CCONJ                                                        | coordinating conjunction | å¯¹ç­‰è¿æ¥è¯ | for,and,nor,but,or,yet,so          |
| [DET](https://universaldependencies.org/docs/u/pos/DET.html) | determiner               | é™å®šè¯     | a, an, the                         |
| [INTJ](https://universaldependencies.org/docs/u/pos/INTJ.html) | interjection             | æ„Ÿå¹è¯     | psst, ouch, bravo, hello           |
| [NOUN](https://universaldependencies.org/docs/u/pos/NOUN.html) | noun                     | åè¯       | girl, boy, sky, man                |
| [NUM](https://universaldependencies.org/docs/u/pos/NUM.html) | numeral                  | æ•°è¯       | 1, 2021, one, ninty-six, IV, MMXIV |
| [PART](https://universaldependencies.org/docs/u/pos/PART.html) | particle                 | å°å“è¯     | 's, not                            |
| [PRON](https://universaldependencies.org/docs/u/pos/PRON.html) | pronoun                  | ä»£è¯       | Iï¼Œ himï¼Œyou, myself, somebody     |
| [PROPN](https://universaldependencies.org/docs/u/pos/PROPN.html) | proper noun              | ä¸“æœ‰åè¯   | Mary, John, London, WTO, NASA      |
| [PUNCT](https://universaldependencies.org/docs/u/pos/PUNCT.html) | punctuation              | æ ‡ç‚¹ç¬¦å·   | .,!                                |
| [SCONJ](https://universaldependencies.org/docs/u/pos/SCONJ.html) | subordinatingconjunction | ä»å±è¿è¯   | although, becauseç­‰                |
| [SYM](https://universaldependencies.org/docs/u/pos/SYM.html) | symbol                   | ç¬¦å·       |                                    |
| [VERB](https://universaldependencies.org/docs/u/pos/VERB.html) | verb                     | åŠ¨è¯       |                                    |
| [X](https://universaldependencies.org/docs/u/pos/X.html)     | other                    | å…¶å®ƒ       |                                    |

å…³äºPOSçš„æ›´å¤šå®šä¹‰ï¼Œå¯ä»¥å‚è§[Part of Speech Overview](http://partofspeech.org/)å’Œ[The Eight Parts of Speech](http://www.butte.edu/departments/cas/tipsheets/grammar/parts_of_speech.html)

spaCä¹Ÿæä¾›äº†`pos_`å’Œ`tag_`æ¥è¿›è¡Œè¯æ€§æ ‡æ³¨ã€‚å…¶ä¸­å‰è€…æ›´åŠ çš„åŸºç¡€ï¼Œåè€…æ˜¯æ‰©å±•ï¼Œç§ç±»æ›´å¤šã€‚

~~~python
import spacy
import utils

nlp = spacy.load('en_core_web_md')
doc = nlp("My friend will fly to New York fast and she is staying there for 3 days.")
properties = ['text', 'pos_', 'tag_', 
              ('pos_explain', utils.token_explain('pos_')),
              ('tag_explain', utils.token_explain('tag_')),
             ]
df_token = utils.tokens_to_df(doc, properties=properties) 
df_token
~~~

![image-20210903150627889](images/image-20210903150627889.png)

~~~python
doc = nlp("My cat will fish for a fish tomorrow in a fishy way.")
df_token = utils.tokens_to_df(doc, properties=properties) 
df_token
~~~

![image-20210903151038733](images/image-20210903151038733.png)

å¯ä»¥çœ‹åˆ°fishçš„è¯æ€§åœ¨ä¸åŒä½ç½®æ˜¯ä¸åŒçš„ã€‚å¦‚æœè¦æŠŠä¸Šé¢çš„å¥å­ç¿»è¯‘æˆå…¶å®ƒè¯­è¨€ï¼Œä¸åŒçš„è¯æ€§å¯èƒ½å¸¦æ¥ä¸åŒçš„ç¿»è¯‘ï¼Œæ¯”å¦‚ï¼šåœ¨è¥¿ç­ç‰™è¯­ä¸­ï¼Œfishçš„åè¯å’ŒåŠ¨è¯å¯¹åº”çš„æ˜¯ä¸åŒçš„å•è¯ã€‚

~~~
I will fish/VB tomorrow.  ->  PescarÃ©/V maÃ±ana.
I eat fish/NN.  -> Como pescado/N.
~~~

### è¯ä¹‰æ¶ˆæ­§ï¼ˆWSDï¼‰

WSDï¼Œå…¨ç§°word-sense disambiguationï¼ŒæŒ‡å¤šä¹‰è¯çš„è¯†åˆ«ã€‚åœ¨æˆ‘ä»¬çš„è‡ªç„¶è¯­è¨€ä¸­ï¼Œå¤šä¹‰è¯å¹¿æ³›å­˜åœ¨ã€‚äººç±»å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œç²¾ç¡®çš„ç†è§£è¿™ç§å·®å¼‚æ€§ï¼Œè€Œè¿™å¯¹æœºå™¨å­¦ä¹ æ¥è¯´ï¼Œæ˜¯å¾ˆå¤§çš„æŒ‘æˆ˜ã€‚ä»¥è‹¹æœè¿™ä¸ªè¯ä¸ºä¾‹ï¼Œå¯ä»¥æ˜¯è‹¹æœå…¬å¸ä¹Ÿå¯ä»¥æŒ‡åƒçš„è‹¹æœï¼Œè¦æ ¹æ®ä¸åŒè¯­å¢ƒä¸‹æ¥åˆ¤æ–­ã€‚

åœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œè¯æ€§æ ‡æ³¨å¯ä»¥ç”¨äºè¯†åˆ«å¤šä¹‰è¯çš„å‡†ç¡®å«ä¹‰ã€‚æ¯”å¦‚å¯¹äºBeatï¼Œä¸åŒè¯æ€§ä¸‹ï¼Œæ„ä¹‰ä¹Ÿæœ‰æ‰€ä¸åŒã€‚

 - Beatâ€”to strike violently (verb (V))
 - Beatâ€”to defeat someone else in a game or a competition (V)
 - Beatâ€”rhythm in music or poetry (N)
 - Beatâ€”bird wing movement (N)
 - Beatâ€”completely exhausted (adjective (ADJ))

ä½†æ˜¯ï¼Œè¯æ€§å¾ˆå¤šæ—¶å€™æä¾›å¤ªå¤šå¸®åŠ©ï¼Œæ¯”å¦‚ï¼š

- Bassâ€”seabass, fish (noun (N))
- Bassâ€”lowest male voice (N)
- Bassâ€”male singer with lowest voice range (N)

### åŠ¨è¯æ—¶æ€ï¼ˆVerb tense and aspectï¼‰

ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæœ‰äº”ä¸ªå¥å­ã€‚

~~~tex
I flew to Rome.
I have flown to Rome.
I'm flying to Rome.
I need to fly to Rome.
I will fly to Rome.
~~~

è¿™äº›å¥å­ä¸­ï¼Œå¯èƒ½åªæœ‰åé¢ä¸‰ä¸ªè¯­å¥æœ‰ticket bookingçš„çš„æ„å›¾ï¼Œå‰é¢ä¸¤ä¸ªå¾€å¾€æ„å‘³ç€å®¢æˆ·çš„æŠ•è¯‰æˆ–è€…æœåŠ¡çš„é—®é¢˜ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œä¸Šé¢å¥å­çš„ä¸‹æ–‡ï¼Œå¯èƒ½æ˜¯è¿™æ ·çš„ã€‚

~~~
I flew to Rome 3 days ago. I still didn't get the bill, please send it ASAP.
I have flown to Rome this morning and forgot my laptop on the airplane. Can you please connect me to lost and found?
I'm flying to Rome next week. Can you check flight availability?
I need to fly to Rome. Can you check flights on next Tuesday?
I will fly to Rome next week. Can you check the flights? 
~~~

ä½¿ç”¨spaCyä¸­çš„è¯æ€§æ ‡æ³¨ï¼Œå¯ä»¥å¸®åŠ©æœºå™¨è¯†åˆ«ç”¨æˆ·çš„æ„å›¾ã€‚

- è¿‡å»å¼æˆ–è€…å®Œæˆå¼ï¼Œæ„å‘³ç€å¾ˆå¯èƒ½æ˜¯å®¢æˆ·æŠ•è¯‰ã€‚
- è¿›è¡Œæ—¶æˆ–é™æ¥æ—¶ï¼Œæ„å‘³ç€å¾ˆå¯èƒ½æ˜¯æƒ³æŸ¥è¯¢èˆªç­æˆ–è€…è®¢ç¥¨ã€‚

~~~python
import spacy

nlp = spacy.load('en_core_web_md')
texts = ['I flew to Rome', 
         'I have flown to Rome.',
         'I''m flying to Rome.', 
         'I need to fly to Rome.',
         'I will fly to Rome.']

for doc in nlp.pipe(texts):
    print([(w.text, w.lemma_, w.pos_, w.tag_, spacy.explain(w.tag_)) for w in doc if w.lemma_=='fly'])
~~~

![image-20210903162340852](images/image-20210903162340852.png)

- verb, past tenseï¼š åŠ¨è¯è¿‡å»å¼
- verb, past participleï¼šåŠ¨è¯è¿‡å»åˆ†è¯
- verb, gerund or present participleï¼š åŠ¨è¯çš„åŠ¨åè¯æˆ–ç°åœ¨åˆ†è¯
- verb, base formï¼š åŠ¨è¯åŸå½¢

### æ•°å­—ï¼Œç¬¦å·å’Œæ ‡ç‚¹ç¬¦å·

è¯æ€§æ ‡æ³¨èƒ½å¤Ÿå‡†ç¡®è¯†åˆ«æ•°å­—ï¼Œç¬¦å·å’Œæ ‡ç‚¹

~~~python
import spacy
import utils

nlp = spacy.load('en_core_web_md')
properties = ['text', 'pos_', 'tag_', 
              ('pos_explain', utils.token_explain('pos_')),
              ('tag_explain', utils.token_explain('tag_')),
             ]
doc = nlp("He earned $5.5 million in 2020 and paid %35 tax.")
df_token = utils.tokens_to_df(doc, properties=properties) 
df_token
~~~



![image-20210903165339581](images/image-20210903165339581.png)

## ä¾èµ–è§£æï¼ˆdependency parsingï¼‰

![image-20210903203841557](images/image-20210903203841557.png)

ä¾èµ–è§£æï¼ˆdependency parsingï¼‰åˆ†æä¸€ä¸ªå¥å­çš„è¯­æ³•ç»“æ„å¹¶æ‰¾å‡ºç›¸å…³çš„è¯ä»¥åŠå®ƒä»¬ä¹‹é—´çš„ä¾å­˜å…³ç³»ã€‚ä¾å­˜å…³ç³»æ˜¯ä¸€ä¸ªä¸­å¿ƒè¯ä¸å…¶ä»å±ä¹‹é—´çš„äºŒå…ƒéå¯¹ç§°å…³ç³»ï¼Œä¸€ä¸ªå¥å­çš„ä¸­å¿ƒè¯é€šå¸¸æ˜¯åŠ¨è¯ï¼ˆVerbï¼‰ï¼Œæ‰€æœ‰å…¶ä»–è¯è¦ä¹ˆä¾èµ–äºä¸­å¿ƒè¯ï¼Œè¦ä¹ˆé€šè¿‡ä¾èµ–è·¯å¾„ä¸å®ƒå…³è”ã€‚

ä¾èµ–è§£æå¯¹äºNLUä¹Ÿéå¸¸æœ‰ç”¨ï¼Œæ¯”å¦‚ï¼Œå¯¹äºä¸‹é¢çš„ä¸¤ä¸ªå¥å­ã€‚

- I forwarded you the email. -> forwarded email
- You forwarded me the email. -> forwarded email

å¦‚æœå»é™¤åœç”¨è¯ï¼Œå®ƒä»¬å‰©ä¸‹çš„è¯­å¥å®Œå…¨ç›¸åŒï¼Œç„¶è€Œå®ƒä»¬çš„å«ä¹‰å·®åˆ«å¾ˆå¤§ã€‚spaCyå¯ä»¥è·å–è¯è¯­ä¹‹é—´çš„å…³ç³»ï¼Œä»¥ä¾¿æ›´åŠ æ·±å…¥åœ°ç†è§£è¯­ä¹‰ã€‚

~~~python
import spacy

nlp = spacy.load('en_core_web_md')
texts = ['I forwarded you the email.', 
         'You forwarded me the email.']

for doc in nlp.pipe(texts):
    spacy.displacy.render(doc, style='dep') 
~~~

![image-20210905100132512](images/image-20210905100132512.png)

ä»ä¸Šå›¾å¯ä»¥çœ‹åˆ°ï¼Œä¾å­˜ç»“æ„æ˜¯åŠ æ ‡ç­¾çš„æœ‰å‘å›¾ï¼Œç®­å¤´ä»ä¸­å¿ƒè¯æŒ‡å‘ä»å±ï¼Œå…·ä½“æ¥è¯´ï¼Œç®­å¤´æ˜¯ä»headï¼ˆsyntactic parentï¼‰æŒ‡å‘childï¼ˆdependentï¼‰ï¼Œæ¯ä¸ªTokenåªæœ‰ä¸€ä¸ªHeadã€‚

~~~python
def children(token):
    return ', '.join([child.text for child in token.children])

def ancestors(token):
    return ', '.join([ancestor.text for ancestor in token.ancestors])


for doc in nlp.pipe(texts):
    print('='*95)
    utils.tokens_to_sheet(doc, properties=[('text', 15), ('head', 15), ('dep_', 15), (('children', children), 30), (('ancestors', ancestors), 30)])
~~~

![image-20210906100918424](images/image-20210906100918424.png)

å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªå¥å­çš„ä¸­å¿ƒè¯ï¼ˆROOTï¼‰éƒ½æ˜¯forwardedã€‚

æ‰€æœ‰ä¾èµ–labelæè¿°å¦‚ä¸‹ã€‚å…¶ä¸­æ ‡æ³¨*çš„æ˜¯å¸¸ç”¨çš„ã€‚

|      | Dep       | explain                                      | description                                      |
| ---- | :-------- | -------------------------------------------- | ------------------------------------------------ |
|      | acl       | clausal modifier of noun (adjectival clause) |                                                  |
|      | acomp     | adjectival complement                        | å½¢å®¹è¯çš„è¡¥å……                                     |
|      | advcl     | adverbial clause modifier                    | çŠ¶è¯­ä»å¥ä¿®é¥°è¯                                   |
|      | advmod    | adverbial modifier                           | çŠ¶è¯­                                             |
|      | agent     | agent                                        | ä»£ç†ï¼Œä¸€èˆ¬æœ‰byçš„æ—¶å€™ä¼šå‡ºç°è¿™ä¸ª                   |
| *    | amod      | adjectival modifier                          | å½¢å®¹è¯ä¿®é¥°è¯­                                     |
|      | appos     | appositional modifier                        | åŒä½è¯                                           |
|      | attr      | attribute                                    | å±æ€§                                             |
| *    | aux       | auxiliary                                    | éä¸»è¦åŠ¨è¯å’ŒåŠ©è¯ï¼Œå¦‚BE,HAVE SHOULD/COULDç­‰åˆ°     |
|      | auxpass   | auxiliary (passive)                          | è¢«åŠ¨è¯                                           |
|      | case      | case marking                                 |                                                  |
|      | cc        | coordinating conjunction                     | å¹¶åˆ—å…³ç³»ï¼Œä¸€èˆ¬å–ç¬¬ä¸€ä¸ªè¯                         |
|      | ccomp     | clausal complement                           | ä»å¥è¡¥å……                                         |
| *    | compound  | compound                                     | åè¯å¤åˆã€‚æ¯”å¦‚ï¼š the **phone** book              |
|      | conj      | conjunct                                     | è¿æ¥ä¸¤ä¸ªå¹¶åˆ—çš„è¯ã€‚                               |
|      | cop       | copula                                       | ç³»åŠ¨è¯                                           |
|      | csubj     | clausal subject                              | å­å¥çš„ä¸»é¢˜                                       |
|      | csubjpass | clausal subject (passive)                    | å­å¥çš„ä¸»é¢˜ï¼ˆè¢«åŠ¨ï¼‰                               |
| *    | dative    | dative                                       | é—´æ¥å®¾è¯­                                         |
|      | dep       | unclassified dependent                       | ä¾èµ–å…³ç³»                                         |
| *    | det       | determiner                                   | é™å®šè¯ï¼Œå¦‚å† è¯ç­‰                                 |
| *    | dobj      | direct object                                | ç›´æ¥å®¾è¯­                                         |
|      | expl      | expletive                                    |                                                  |
|      | intj      | interjection                                 |                                                  |
|      | mark      | marker                                       | ä¸»è¦å‡ºç°åœ¨æœ‰â€œthatâ€ or â€œwhetherâ€â€œbecauseâ€, â€œwhenâ€ |
|      | meta      | meta modifer                                 |                                                  |
|      | neg       | negation modifier                            | å¦å®šè¯                                           |
|      | npadvmod  | noun phrase as an adverbial modifier         | åè¯çŸ­è¯­ä½œçŠ¶è¯­ä¿®é¥°è¯­                             |
| *    | nsubj     | nominal subject                              | åè¯ä¸»è¯­                                         |
| *    | nsubjpass | nominal subject (passive)                    | è¢«åŠ¨çš„åè¯ä¸»è¯­                                   |
| *    | nummod    | numeric modifier                             | æ•°å€¼ä¿®é¥°                                         |
|      | oprd      | object predicate                             | å¯¹è±¡è°“è¯                                         |
|      | parataxis | parataxis                                    | å¹¶åˆ—                                             |
|      | pcomp     | complement of preposition                    | ä»‹è¯è¡¥å……                                         |
|      | pobj      | object of preposition                        | ä»‹è¯å®¾è¯­                                         |
| *    | poss      | Possession modifier                          | æ‹¥æœ‰ä¿®è¾è¯ï¼Œæ¯”å¦‚**my** book, **your** room       |
|      | preconj   | Pre-correlative conjunction                  |                                                  |
|      | predet    | Pre-determiner                               |                                                  |
|      | prep      | prepositional modifier                       | ä»‹è¯                                             |
|      | prt       | particle                                     | å°å“è¯                                           |
|      | punct     | punctuation                                  | æ ‡ç‚¹ç¬¦å·                                         |
|      | quantmod  | Modifier of quantifier                       |                                                  |
|      | relcl     | relative clause modifier                     |                                                  |
| *    | root      | root                                         | ä¸­å¿ƒè¯                                           |
|      | xcomp     | open clausal complement                      |                                                  |

ä¸Šæ–‡çš„ç¿»è¯‘éƒ¨åˆ†å‚è€ƒ[è¯­æ³•åˆ†ææ ‡æ³¨](https://gist.github.com/guodong000/2ad6b546eaff95f185b67608b1782656)ã€‚

ä¸‹é¢å†æ¥åˆ†æå‡ ä¸ªä¾‹å­ã€‚

~~~python
import spacy

def parse_dependency(text):
    nlp = spacy.load('en_core_web_md')
    doc = nlp(text)    

    properties = [('text', 15), ('tag_', 15), ('head', 15), ('dep_', 15)]
    utils.tokens_to_sheet(doc, properties=properties)
        
    # ç”±äºå›¾ç‰‡å¤ªå¤§ï¼Œä½¿ç”¨renderï¼Œï¼ˆjupyterä¸­ï¼‰ä¼šå‡ºç°æ»šåŠ¨æ¡ï¼Œæ‰€ä»¥ä½¿ç”¨serve
    # æ‰“å¼€http://[jupyter notebook ip]:6010/
    spacy.displacy.serve(doc, style='dep', port=6010) 
    #spacy.displacy.render(doc, style='dep') 

parse_dependency('We are trying to understand the difference.') 
~~~

![image-20210906105105491](images/image-20210906105105491.png)

![image-20210906111050812](images/image-20210906111050812.png)

éœ€è¦æ³¨æ„çš„æ˜¯ï¼š

- trying -> understand: xcomp

~~~python
parse_dependency('Queen Katherine, who was the mother of Mary Tudor, died at 1536.') 
~~~

![image-20210906110115143](images/image-20210906110115143.png)

![image-20210906110437144](images/image-20210906110437144.png)

éœ€è¦æ³¨æ„çš„æ˜¯ï¼š

- Katherine -> was: relclã€‚Katherineåœ¨ä»å¥ä¸­è¢«æåˆ°ï¼Œæ‰€ä»¥æ˜¯relative clause modifierå…³ç³»ã€‚

ä¸¤ä¸ªéå¸¸åœ¨çº¿demo:

- https://explosion.ai/demos/displacy

  ![image-20210907104536740](images/image-20210907104536740.png)

- https://huggingface.co/spaces/spacy/pipeline-visualizerï¼š æ— æ‰€ä¸åŒ…ã€‚

  ![image-20210907103932127](images/image-20210907103932127.png)

## å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰

NERå…¨ç§°Name Entity Recognitionï¼Œæ˜¯æŒ‡è¯†åˆ«æ–‡æœ¬ä¸­å…·æœ‰ç‰¹å®šæ„ä¹‰çš„å®ä½“ï¼Œä¸»è¦åŒ…æ‹¬äººåã€åœ°åã€æœºæ„åã€ä¸“æœ‰åè¯ç­‰ã€‚å¸¸è§çš„å››ä¸ªç±»åˆ«å¦‚ä¸‹ã€‚

| Type | Description                    |
| ---- | ------------------------------ |
| PER  | äººå                           |
| LOC  | åœ°å                           |
| ORG  | ç»„ç»‡æˆ–æœºæ„                     |
| MISC | æ‚é¡¹ã€‚æ¯”å¦‚ï¼šäº‹ä»¶ï¼Œå›½å®¶ï¼Œäº§å“ç­‰ |

åœ¨spaCyä¸­æœ‰ä¸ªæ›´å¤šçš„å®ä½“ï¼Œåˆ—è¡¨å¦‚ä¸‹ã€‚

![image-20210906114008569](images/image-20210906114008569.png)

å½“å‰state-of-the-artçš„NERæ¨¡å‹æ˜¯LSTMæˆ–è€…LSTM+CRMã€‚

> æ˜¯ä¸æ˜¯è¿™æœ¬ä¹¦å†™çš„æ—¶å€™æœ‰ç‚¹æ—©ï¼Œç›®å‰spaCyåº”è¯¥ä½¿ç”¨æ—¶transformæ¨¡å‹å•Šã€‚

ä¸‹é¢çœ‹çœ‹spaCyä¸­NERã€‚

~~~python
import spacy
import utils

nlp = spacy.load('en_core_web_md')
properties = ['text', 'pos_', 'tag_', 'ent_type_',
              ('ent_type_explain', utils.token_explain('ent_type_')),
             ]
doc = nlp("Albert Einstein was born in Ulm on 1879. He studied electronical engineering at ETH Zurich.")
df_token = utils.tokens_to_df(doc, properties=properties)
~~~

![image-20210906115233131](images/image-20210906115233131.png)

ä¸‰ä¸ªéå¸¸å¥½çš„åœ¨çº¿demo

- https://explosion.ai/demos/displacy-ent

  ![image-20210907110945226](images/image-20210907110945226.png)

- https://huggingface.co/spaces/spacy/pipeline-visualizerï¼š æ— æ‰€ä¸åŒ…ã€‚

  ![image-20210907111258380](images/image-20210907111258380.png)

- https://prodi.gy/demoï¼šé™¤äº†å‘½ä»¤å®ä½“ï¼Œè¿˜æœ‰å¾ˆå¤šDEMOã€‚

  éå¸¸å¼ºå¤§ï¼Œåªæ˜¯ä¸æ˜¯å…è´¹çš„ã€‚

  ![image-20210907104053156](images/image-20210907104053156.png)

## åˆå¹¶å’Œæ‹†åˆ†Tokens

### åˆå¹¶

~~~python
import spacy
import utils

nlp = spacy.load('en_core_web_md')
properties = ['text', 'pos_', 'tag_', 'ent_type_',
              ('ent_type_explain', utils.token_explain('ent_type_')),
             ]
doc = nlp("She lived in New Hampshire.")

print('-'*50)
properties = ['text', 'lemma_', 'i', 'ent_type_', 'pos_', 'tag_', 'dep_']
df_token = utils.tokens_to_df(doc, properties=properties)
spacy.displacy.render(doc, style='dep')

with doc.retokenize() as retokenizer:
    retokenizer.merge(doc[3:5], attrs={"LEMMA":"new hampshire"})

print('-'*50)
df_token = utils.tokens_to_df(doc, properties=properties)
spacy.displacy.render(doc, style='dep')
~~~

![image-20210906122259396](images/image-20210906122259396.png)

### æ‹†åˆ†

~~~python
import spacy
import utils

nlp = spacy.load('en_core_web_md')
properties = ['text', 'pos_', 'tag_', 'ent_type_',
              ('ent_type_explain', utils.token_explain('ent_type_')),
             ]
doc = nlp("She lived in NewHampshire.")

print('-'*50)
properties = ['text', 'lemma_', 'i', 'ent_type_', 'pos_', 'tag_', 'dep_']
df_token = utils.tokens_to_df(doc, properties=properties)
spacy.displacy.render(doc, style='dep')

with doc.retokenize() as retokenizer:
    heads = [(doc[3], 1), doc[2]]
    attrs = {"TAG":["NNP", "NNP"], "DEP":["compound", "pobj"]}
    retokenizer.split(doc[3], ["New", "Hampshire"], heads=heads, attrs=attrs)

print('-'*50)
df_token = utils.tokens_to_df(doc, properties=properties)
spacy.displacy.render(doc, style='dep')
~~~

![image-20210906122418244](images/image-20210906122418244.png)



# Rule-Based Matching

## Token-based matching

### Matcher

é¦–å…ˆçœ‹åŸºæœ¬çš„ç¤ºä¾‹ã€‚

~~~python
import spacy
from spacy.matcher import Matcher

nlp = spacy.load("en_core_web_md")
doc = nlp("Good morning, I want to reserve a ticket. I will then say good evening!")
matcher = Matcher(nlp.vocab)

pattern1 = [{"LOWER": "good"}, {"LOWER": "morning"}, {"IS_PUNCT": True}]
matcher.add("morningGreeting",  [pattern1])
pattern2 = [{"LOWER": "good"}, {"LOWER": "evening"}, {"IS_PUNCT": True}]
matcher.add("eveningGreeting",  [pattern2])
matches = matcher(doc)

for match_id, start, end in matches:
    pattern_name = nlp.vocab.strings[match_id]
    m_span = doc[start:end]  
    print(pattern_name, start, end, m_span.text)
~~~

![image-20210906214458121](images/image-20210906214458121.png)

å¯ä»¥ä½¿ç”¨Tokençš„ä»»ä½•å±æ€§æŸ¥è¯¢ï¼Œæ¯”å¦‚ENT_TYPE

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc = nlp("Today German chancellor Angela Merkel met with the US president.")

pattern = [{"ENT_TYPE": "PERSON", "OP": "+"}, {"POS" : "VERB"}]
matcher = Matcher(nlp.vocab)
matcher.add("personEnt",  [pattern])

show_matches(nlp, doc, matcher(doc))
~~~



### ä¸€äº›æ‰©å±•

Matcheræ”¯æŒä¸€äº›æ‰©å±•ï¼Œæ¯”å¦‚ï¼š

- inï¼Œ not in
- ==, >=, <=, >, <

~~~python
matcher = Matcher(nlp.vocab)
pattern1 = [{"LOWER": "good"}, {"LOWER": {"IN": ["morning", "evening"]}}, {"IS_PUNCT": True}]
matcher.add("greetings", [pattern1])
pattern2 = [{"LENGTH": {">=" : 7}}]
matcher.add("longWords",  [pattern2])
matches = matcher(doc)

for match_id, start, end in matches:
    pattern_name = nlp.vocab.strings[match_id]
    m_span = doc[start:end]  
    print(pattern_name, start, end, m_span.text)
~~~

![image-20210907082642075](images/image-20210907082642075.png)

### ç±»æ­£åˆ™è¡¨è¾¾å¼

Matcheræ”¯æŒä¸€äº›ç±»ä¼¼æ­£åˆ™è¡¨è¾¾å¼çš„è¯­æ³•ã€‚

| OP   | DESCRIPTION   |
| ---- | ------------- |
| !    | ä¸å­˜åœ¨        |
| ?    | å‡ºç°0æˆ–1æ¬¡    |
| +    | å‡ºç°1æ¬¡æˆ–å¤šæ¬¡ |
| *    | å‡ºç°0æ¬¡æˆ–å¤šæ¬¡ |

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc1 = nlp("Barack Obama visited France.")
doc2 = nlp("Barack Hussein Obama visited France.")

pattern = [{"LOWER": "barack"},
           {"LOWER": "hussein", "OP": "?"},
           {"LOWER": "obama"}]
matcher = Matcher(nlp.vocab)
matcher.add("obamaNames",  [pattern])

show_matches(nlp, doc1, matcher(doc1))
print('-'*50)
show_matches(nlp, doc2, matcher(doc2))
~~~

![image-20210907083928348](images/image-20210907083928348.png)

ç¨å¾®å¤æ‚ä¸€ç‚¹çš„ä¾‹å­ã€‚

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc1 = nlp("Hello hello hello, how are you?")
doc2 = nlp("Hello, how are you?")
doc3 = nlp("How are you?")

matcher = Matcher(nlp.vocab)
pattern = [{"LOWER": {"IN": ["hello", "hi", "hallo"]}, "OP":"+"}, 
           {"IS_PUNCT": True}]
matcher.add("greetings",  [pattern])

show_matches(nlp, doc1, matcher(doc1))
print('-'*50)
show_matches(nlp, doc2, matcher(doc2))
print('-'*50)
show_matches(nlp, doc3, matcher(doc3))
~~~

![image-20210907084558631](images/image-20210907084558631.png)

æœ€å ï¼ŒMatcherçš„patternä¸­å¦‚æœæ·»åŠ {}ï¼Œè¡¨ç¤ºæ¥å—ä»»æ„ä¸€ä¸ªtokenã€‚

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc = nlp("My name is Alice and his name was Elliot.")

matcher = Matcher(nlp.vocab)
pattern = [{"LOWER": "name"},{"LEMMA": "be"},{}]
matcher.add("pickName", [pattern])

show_matches(nlp, doc, matcher(doc))
~~~

![image-20210907085015735](images/image-20210907085015735.png)

{"OP":"*"} è¡¨ç¤ºåŒ¹é…ä»»æ„ä¸ªtokenï¼Œä»£ç å¦‚ä¸‹ã€‚

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc1 = nlp("I forwarded his email to you.")
doc2 = nlp("I forwarded an email to you.")
doc3 = nlp("I forwarded an very important email to you.")

matcher = Matcher(nlp.vocab)
pattern = [{"LEMMA": "forward"}, {"OP":"*"}, {"LOWER": "email"}]
matcher.add("forwardMail",  [pattern])

show_matches(nlp, doc1, matcher(doc1))
print('-'*50)
show_matches(nlp, doc2, matcher(doc2))
print('-'*50)
show_matches(nlp, doc3, matcher(doc3))
~~~

![image-20210907085518318](images/image-20210907085518318.png)

### æ­£åˆ™è¡¨è¾¾å¼

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc1 = nlp("I travelled by bus.")
doc2 = nlp("She traveled by bike.")

matcher = Matcher(nlp.vocab)
pattern = [{"POS": "PRON"},
           {"TEXT": {"REGEX": "[Tt]ravell?ed"}}]
matcher.add("travel",  [pattern])

show_matches(nlp, doc1, matcher(doc1))
print('-'*50)
show_matches(nlp, doc2, matcher(doc2))
~~~

![image-20210907091053632](images/image-20210907091053632.png)

ä¸Šé¢çš„æ­£åˆ™è¡¨è¾¾å¼ä½œç”¨çš„èŒƒå›´æ˜¯tokenï¼Œå¦‚æœå¸Œæœ›åœ¨docå…¨å±€èŒƒå›´æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ï¼Œå¯ä»¥å‚è€ƒå¦‚ä¸‹å®ä¾‹ã€‚

~~~python
import spacy
import re

nlp = spacy.load("en_core_web_sm")
doc = nlp("The United States of America (USA) are commonly known as the United States (U.S. or US) or America.")

expression = r"[Uu](nited|\.?) ?[Ss](tates|\.?)"
for match in re.finditer(expression, doc.text):
    start, end = match.span()
    span = doc.char_span(start, end)
    # This is a Span object or None if match doesn't map to valid token sequence
    if span is not None:
        print("Found match:", start, end, span.text)
~~~

![image-20210907091944198](images/image-20210907091944198.png)

### åœ¨çº¿å·¥å…·

ä¸€ä¸ªéå¸¸å¥½çš„regexåœ¨çº¿ç½‘ç«™https://regex101.com/

![image-20210907092632832](images/image-20210907092632832.png)

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢ç½‘ç«™ä¸Šè¿”å›çš„matchæ˜¯5ä¸ªï¼Œè€Œpythonæ­£åˆ™è¡¨è¾¾å¼ä¸­æ˜¯4ä¸ªï¼Œè¿™æœ‰äº›å¥‡æ€ªã€‚

spaCyä¹Ÿæä¾›äº†ä¸€ä¸ªéå¸¸é…·çš„å·¥å…·ã€‚https://explosion.ai/demos/matcher

![image-20210907102910832](images/image-20210907102910832.png)

> éœ€è¦ç‰¹åˆ«å…³æ³¨çš„æ˜¯

## PhraseMatcher

æŠŠè¦æŸ¥è¯¢çš„çŸ­è¯­æ”¾åˆ°PhraseMatcherä¸­ã€‚

~~~python
import spacy
from spacy.matcher import PhraseMatcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc = nlp("3 EU leaders met in Berlin. German chancellor Angela Merkel first welcomed the US president Donald Trump. The following day Alexis Tsipras joined them in Brandenburg.")

matcher = PhraseMatcher(nlp.vocab)
terms = ["Angela Merkel", "Donald Trump", "Alexis Tsipras"]
patterns = list(nlp.pipe(terms))
# patterns = [nlp.make_doc(term) for term in terms]
matcher.add("politiciansList",  patterns)

show_matches(nlp, doc, matcher(doc))
~~~

![image-20210907113307506](images/image-20210907113307506.png)

æ¥ä¸‹æ¥çš„ä¾‹å­æ˜¯å¿½ç•¥å¤§å°å†™ã€‚

~~~python
import spacy
from spacy.matcher import PhraseMatcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc = nlp("During the last decade, derivatives market became an asset class of their own and influenced the financial landscape strongly.")
matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
terms = ["Asset", "Investment", "Derivatives",
         "Demand",  "Market"]
patterns = list(nlp.pipe(terms))
# patterns = [nlp.make_doc(term) for term in terms]
matcher.add("financeTerms",  patterns)

show_matches(nlp, doc, matcher(doc))
~~~

![image-20210907113605946](images/image-20210907113605946.png)

æœ€åä¸€ä¸ªä¾‹å­ï¼Œä½¿ç”¨shapeè¿›è¡ŒåŒ¹é…ã€‚

~~~python
import spacy
import utils 
from spacy.matcher import PhraseMatcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc = nlp("This log contains the following IP addresses: 192.1.1.2 and 192.21.1.1 and 192.160.1.1 .")
matcher = PhraseMatcher(nlp.vocab, attr="SHAPE")
terms = ["127.0.0.0", "127.256.0.0"]
patterns = list(nlp.pipe(terms))
# patterns = [nlp.make_doc(term) for term in terms] 
matcher.add("IPNums",  patterns)

show_matches(nlp, doc, matcher(doc))
~~~

![image-20210907114655846](images/image-20210907114655846.png)

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ192.21.1.1æ²¡æœ‰åŒ¹é…ï¼ŒåŸå› åœ¨äºç¬¬äºŒä¸ªæ•°ï¼Œåªæœ‰ä¸¤ä½ã€‚

## EntityRuler

ä½¿ç”¨EntityRulerå¯ä»¥æ‰©å……Entityã€‚æ¯”å¦‚ï¼šå¢åŠ chimeï¼ˆå¾®è½¯åäººåä¼šï¼‰ä½œä¸ºä¸€ä¸ªORGã€‚

~~~python
import spacy

nlp = spacy.load("en_core_web_md")
doc = nlp("I have an acccount with chime since 2017")
print(doc.ents)

patterns = [{"label": "ORG", "pattern": [{"LOWER": "chime"}]}]
ruler = nlp.add_pipe("entity_ruler")
ruler.add_patterns(patterns)

doc = nlp("I have an acccount with chime since 2017")
print(doc.ents)
~~~

![image-20210907121335359](images/image-20210907121335359.png)

## åˆå¹¶modelså’Œmatchers

ä¸‹é¢çœ‹å‡ ä¸ªä¾‹å­ï¼ŒæŠŠæ¨¡å‹å’ŒMatcheråˆå¹¶èµ·æ¥ä½¿ç”¨ã€‚

### æŠ½å–IBANå’Œaccount numbers

IBANï¼ˆInternational Bank Account Numberï¼‰ï¼ŒæŒ‡å›½é™…é“¶è¡Œå¸æˆ·å·ç ï¼Œæ˜¯ç”±æ¬§æ´²é“¶è¡Œæ ‡å‡†å§”å‘˜ä¼šï¼ˆ European Committee for Banking Standardsï¼Œç®€ç§° ECBSï¼‰æŒ‰ç…§å…¶æ ‡å‡†åˆ¶å®šçš„ä¸€ä¸ªé“¶è¡Œå¸æˆ·å·ç ã€‚IBANçš„ç¼–å·è§„å®šåŒ…æ‹¬å›½åˆ«ä»£ç ï¼‹é“¶è¡Œä»£ç ï¼‹åœ°åŒºï¼‹è´¦æˆ·äººè´¦å·ï¼‹æ ¡éªŒç ã€‚

![image-20210907115822427](images/image-20210907115822427.png)

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc1 = nlp("My IBAN number is BE71 0961 2345 6769, please send the money there. My account number is 8921273.")
doc2 = nlp("My IBAN number is FR76 3000 6000 0112 3456 7890 189, please send the money there. My account num is 3239212.")

pattern1 = [{"SHAPE": "XXdd"}, {"TEXT": {"REGEX": "\d{1,4}"}, "OP":"+"}]
pattern2 = [{"LOWER": "account"}, 
            {"LOWER": {"IN": ["num", "number"]}}, {}, 
            {"IS_DIGIT": True}]
matcher = Matcher(nlp.vocab)
matcher.add("ibanNum",  [pattern1])
matcher.add("accountNum",  [pattern2])

show_matches(nlp, doc1, matcher(doc1))
print('-'*50)
show_matches(nlp, doc2, matcher(doc2))
~~~

![image-20210907122335903](images/image-20210907122335903.png)

### æŠ½å–ç”µè¯å·ç 

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
doc1 = nlp("You can call my office on +1 (221) 102-2423 or email me directly.")
doc2 = nlp("You can call me on (221) 102 2423 or text me.")

pattern = [{"TEXT": "+1", "OP": "?"}, {"TEXT": "("},
           {"SHAPE": "ddd"}, {"TEXT": ")"},
           {"SHAPE": "ddd"}, {"TEXT": "-", "OP": "?"},
           {"SHAPE": "dddd"}]
matcher = Matcher(nlp.vocab)
matcher.add("ibanNum",  [pattern])

show_matches(nlp, doc1, matcher(doc1))
print('-'*50)
show_matches(nlp, doc2, matcher(doc2))
~~~

![image-20210907122536068](images/image-20210907122536068.png)

### æŠ½å–mentions

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
texts = ['KFC is very generous with the portions.', 
         'Mcdonald''s is horrible, we waited for mins for a table.', 
         'Quanjude is terribly expensive, stay away!', 
         'RestaurantB is pretty amazing, we recommend.']
docs = nlp.pipe(texts)

pattern = [{"ENT_TYPE": "ORG"}, {"LEMMA": "be"}, {"POS": "ADV", "OP": "*"}, {"POS": "ADJ"}]
matcher = Matcher(nlp.vocab)
matcher.add("mentions",  [pattern])

for doc in docs:
    print('-'*50)
    print([ent for ent in doc.ents])
    show_matches(nlp, doc, matcher(doc))
~~~

![image-20210907123557046](images/image-20210907123557046.png)

### æŠ½å–æ ‡ç­¾ï¼ˆHashtagï¼‰å’Œè¡¨æƒ…ç¬¦å·ï¼ˆemojiï¼‰

~~~python
import spacy
from spacy.matcher import Matcher

def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)    

nlp = spacy.load("en_core_web_md")
texts = ["Start working out now #WeekendShred", 
         " I love China ğŸ˜",
         " I never buy Zara ğŸ˜­"
        ]
docs = nlp.pipe(texts)

pattern1 = [{"TEXT": "#"}, {"IS_ASCII": True}]
matcher = Matcher(nlp.vocab)
matcher.add("hashTag",  [pattern1])

pos_emoji = ["ğŸ˜€", "ğŸ˜ƒ", "ğŸ˜„", " ğŸ˜", "ğŸ˜", "ğŸ˜"]  
neg_emoji = ["ğŸ¥º", "ğŸ˜¢", "ğŸ˜­", " ğŸ˜Ÿ", "ğŸ˜©", "ğŸ˜ª"]
pos_patterns = [[{"ORTH": emoji}] for emoji in pos_emoji]
neg_patterns = [[{"ORTH": emoji}] for emoji in neg_emoji]
matcher.add("posEmoji", pos_patterns)
matcher.add("negEmoji", neg_patterns)

for doc in docs:
    print('-'*50)
    show_matches(nlp, doc, matcher(doc))
~~~

![image-20210907124604047](images/image-20210907124604047.png)

### åˆå¹¶linguistic featureså’Œnamed entities

æ‰¾å‡ºçˆ±å› æ–¯å¦çš„å±…ä½åœ°å€ã€‚é‡‡ç”¨çš„æŠ€æœ¯æœ‰ï¼š

- name entities
- dependency parsing
- lemmatization

~~~python
import spacy
from spacy.matcher import Matcher  

nlp = spacy.load("en_core_web_md")
texts = ["Einstein lived in Zurich."]
docs = nlp.pipe(texts)
doc = list(docs)[0]
print([(ent.text, ent.label_) for ent in doc.ents])
spacy.displacy.render(doc, style='dep')

person_ents = [ent for ent in doc.ents if ent.label_ == "PERSON"]
for person_ent in person_ents:
    #We use head of the entity's last token
    head = person_ent[-1].head  
    if head.lemma_ == "live":
        preps = [token for token in head.children if token.dep_ == "prep"]
        for prep in preps:         
            places = [token for token in prep.children if token.ent_type_ == "GPE"]   
            # Verb is in past or present tense
            print({'person': person_ent, 'city': places,'past': head.tag_ == "VBD"})
~~~

![image-20210907125348880](images/image-20210907125348880.png)

# Working with Word Vectors and Semantic Similarity

## ç†è§£è¯å‘é‡

å¸¸ç”¨çš„è¯å‘é‡æœ‰ï¼š

- word2vecï¼šhttps://code.google.com/archive/p/word2vec/

  > ä¼¼ä¹å¥½ä¹…æ²¡æœ‰æ›´æ–°äº†

- Gloveï¼š https://nlp.stanford.edu/projects/glove/

- fastText: https://fasttext.cc/docs/en/english-vectors.html

> æœ‰ç©ºå¯ä»¥å†å­¦ä¹ ç³»ç»Ÿä¸€ä¸‹[Gensim](https://radimrehurek.com/gensim)ã€‚é‡Œé¢æ•´åˆäº†WordsVectorï¼Œ FastTextï¼Œ LSIï¼Œ LDAç­‰ç®—æ³•ã€‚

## è¯å‘é‡ï¼ˆword vectorsï¼‰

spacyä¸­çš„å¾ˆå¤šé¢„è®­ç»ƒï¼ˆpretrained ï¼‰æ¨¡å‹åŒ…å«äº†word vectorsã€‚ä»¥è‹±æ–‡ä¸ºä¾‹ã€‚

- [en_core_web_sm](https://spacy.io/models/en#en_core_web_sm) ï¼š 12MB

  åŒ…å«ï¼štok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizerã€‚**ä¸åŒ…å«è¯å‘é‡ã€‚**

- [en_core_web_md](https://spacy.io/models/en#en_core_web_md)ï¼š 43 MB

  åŒ…å«ï¼štok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizerã€‚ **è¯å‘é‡ï¼š685k keys, 20k unique vectors (300 dimensions)ã€‚**

- [en_core_web_lg](https://spacy.io/models/en#en_core_web_lg)ï¼š 741 MB

  åŒ…å«ï¼štok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizerã€‚ **è¯å‘é‡ï¼š685k keys, 685k unique vectors (300 dimensions)**

- [en_core_web_trf](https://spacy.io/models/en#en_core_web_trf)ï¼š438 MB

  åŒ…å«ï¼štransformer, tagger, parser, ner, attribute_ruler, lemmatizerã€‚**ä¸åŒ…å«è¯å‘é‡**

éœ€è¦æ³¨æ„çš„ï¼Œå¯¹äºOOVï¼ˆout-of-vocabularyï¼‰çš„è¯æ±‡ï¼Œæ²¡æœ‰å¯¹åº”çš„è¯å‘é‡ã€‚

~~~python
import spacy 
import utils

nlp = spacy.load("en_core_web_md")
doc = nlp("You went there afskfsd.")

df_token = utils.tokens_to_df(doc, properties=['text', 'is_oov', 'has_vector'])
~~~

![image-20210906150625642](images/image-20210906150625642.png)

## ç›¸ä¼¼åº¦ï¼ˆSimilarityï¼‰

é»˜è®¤çš„ç›¸ä¼¼åº¦å‡½æ•°ï¼ˆSimilarityï¼‰æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

~~~python
import spacy 
import utils

nlp = spacy.load("en_core_web_md")

doc1 = nlp("I visited England.")
doc2 = nlp("I went to London.")
print('doc similarity: {}'.format(doc1.similarity(doc2)))
print('span similarity: {}'.format(doc1[1:3].similarity(doc2[1:4])))
print('token similarity: {}'.format(doc1[2].similarity(doc2[3])))
~~~

![image-20210906161434794](images/image-20210906161434794.png)

ä¸‹é¢ä½¿ç”¨PCAé™ç»´åˆ°2ç»´åº¦ï¼Œè¿™æ ·å¯ä»¥æ–¹ä¾¿æŸ¥çœ‹å¤šä¸ªwordä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚

~~~python
import matplotlib.pyplot as plt
import numpy as np
import spacy
from sklearn.decomposition import PCA

nlp = spacy.load("en_core_web_md")
vocab = nlp("""cat dog tiger elephant bird monkey lion cheetah
burger pizza food cheese wine salad noodles macaroni fruit
vegetable""")
            
words = [word.text for word in vocab]
vecs = np.vstack([word.vector for word in vocab if word.has_vector])

pca = PCA(n_components=2)
vecs_transformed = pca.fit_transform(vecs)

plt.figure(figsize=(20,15))
plt.scatter(vecs_transformed[:,0], vecs_transformed[:,1])
for word, coord in zip(words, vecs_transformed):
    x,y = coord
    plt.text(x,y,word, size=15)
plt.show()
~~~

![image-20210906163645856](images/image-20210906163645856.png)

## third-partyè¯å‘é‡

ä¸‹é¢ä¸‹è½½fastTextçš„è¯å‘é‡ã€‚

~~~python
!curl -L https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip --output ./data/wiki-news-300d-1M-subword.vec.zip
# wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip -P ./data
unzip data/wiki-news-300d-1M-subword.vec.zip -d data
~~~

![image-20210906173955163](images/image-20210906173955163.png)

![image-20210906174015277](images/image-20210906174015277.png)

å°†è¯å‘é‡å¯¼å…¥åˆ°spaCyä¸­ã€‚

~~~python
!python -m spacy init vectors en data/wiki-news-300d-1M-subword.vec ./output/en_subwords_wiki_lg --name en_subwords_wiki_lg
~~~

åŠ è½½æ¨¡å‹ï¼Œæ³¨æ„è¯¥æ¨¡å‹æ˜¯ä¸€ä¸ªç©ºæ¨¡å‹ã€‚

~~~python
import spacy
nlp = spacy.load("en_core_web_md")
print(f'en_core_web_md pipe_names={nlp.pipe_names}')

nlp = spacy.load("output/en_subwords_wiki_lg")  
print(f'en_subwords_wiki_lg pipe_names={nlp.pipe_names}')

doc1 = nlp("I visited England.")
doc2 = nlp("I went to London.")

print('doc similarity: {}'.format(doc1.similarity(doc2)))
print('span similarity: {}'.format(doc1[1:3].similarity(doc2[1:4])))
print('token similarity: {}'.format(doc1[2].similarity(doc2[3])))
~~~

![image-20210906175624758](images/image-20210906175624758.png)

## ç›¸ä¼¼åº¦æ¯”è¾ƒçš„æŠ€å·§

é¦–å…ˆçœ‹ä¾‹å­ã€‚

~~~python
import spacy
nlp = spacy.load("en_core_web_md")

sentences = nlp("I purchased a science fiction book last week. "
                "I loved everything related to this fragrance: light, floral and feminine... "
                "I loved everything related to this wine: light, floral and feminine... "
                "I purchased a bottle of wine. "
                "I purchased some wine. "
                "I purchased some water. "
                "I purchased some chicken. "
               )
key = nlp("perfume")
for sent in sentences.sents:
    print(sent.similarity(key), sent)
~~~

![image-20210906183039687](images/image-20210906183039687.png)

- perfumeå’Œç¬¬äºŒä¸ªå¥å­æœ€ä¸ºç›¸ä¼¼ã€‚è€Œå…³é”®è¯è¯­æ˜¯fragrance
- perfumeå’Œç¬¬å››ä¸ªå¥å­ç›¸ä¼¼åº¦ä¹Ÿæ¯”è¾ƒé«˜ã€‚å…¶å…³é”®è¯æ±‡æ˜¯bottleå’Œwineã€‚

### æ¯”è¾ƒåè¯çŸ­è¯­

è¦æ¯”è¾ƒæ–‡æ¡£çš„ç›¸ä¼¼æ€§ï¼Œå¯ä»¥è·å–å„è‡ªçš„å…³é”®çš„wordï¼Œç„¶åè®¡ç®—ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œè¿™æ˜¯ä¸€ç§æé«˜å‡†ç¡®ç‡çš„æœ‰æ•ˆåŠæ³•ã€‚ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œä»…ä»…æ¯”è¾ƒåè¯çŸ­è¯­ï¼Œæ¥è®¡ç®—ç›¸ä¼¼åº¦ã€‚

~~~python
for sent in sentences.sents:
    nchunks = [nchunk.text for nchunk in sent.noun_chunks]
    nchunk_doc = nlp(" ".join(nchunks))
    print(nchunk_doc.similarity(key), sent)
~~~

![image-20210906183446831](images/image-20210906183446831.png)

éå¸¸æ˜æ˜¾ï¼Œç¬¬äºŒå¥è¯çš„ç›¸ä¼¼åº¦æé«˜å¾ˆå¤šã€‚

å†æ¥çœ‹ä¸€ä¸ªä¾‹å­ã€‚å¾ˆæ˜æ˜¾ï¼Œç¬¬ä¸€ä¸ªå¥å­å’Œç¬¬äºŒä¸ªå¥å­ç›¸å…³ç¨‹åº¦æ¯”è¾ƒé«˜ï¼ˆéƒ½æ˜¯æœ‰å…³äºæœç´¢å¼•æ“çš„ï¼‰ï¼Œä½†æ˜¯è¾“å‡ºçš„ç»“æœæœ‰æ˜æ˜¾é”™è¯¯ï¼šç¬¬ä¸€ä¸ªå¥å­å’Œç¬¬ä¸‰ä¸ªå¥å­çš„ç›¸ä¼¼åº¦éå¸¸é«˜ã€‚

~~~python
import spacy
import pandas as pd 
from IPython.display import display

def compare1(docs):
    length = len(docs)
    columns = list(range(length))
    df_sim = pd.DataFrame(columns = columns,
                          index = columns)   
    for i in range(length):
        doc1 = docs[i]
        for j in range(i, length):
            doc2 = docs[j]
            sim = round(doc1.similarity(doc2), 4)
            df_sim.loc[j, i] = sim
            df_sim.loc[i, j] = sim
    
    display(df_sim)

nlp = spacy.load("en_core_web_md")
texts = ["Google Search, often referred as Google, is the most popular search engine nowadays. It answers a huge volume of queries everyday.",
         "Microsoft Bing is another popular search engine. Microsoft is known by its star product Microsoft Windows, a popular operating system sold over the world.",
         "The Dead Sea is the lowest lake in the world, located in the Jordan Valley of Israel. It is also the saltiest lake in the world."]
docs = list(nlp.pipe(texts))
compare1(docs)
~~~

![image-20210906190927303](images/image-20210906190927303.png)

å¦‚æœé‡‡ç”¨ä¸Šä¸€èŠ‚æå–åè¯çŸ­è¯­çš„æ–¹æ³•ï¼Œç¬¬ä¸€å¥å’Œç¬¬ä¸‰å¥é™ä½äº†å¾ˆå¤šï¼Œè¿™ç‚¹å¾ˆä¸é”™ã€‚

~~~python
def compare2(docs):
    length = len(docs)
    columns = list(range(length))
    df_sim = pd.DataFrame(columns = columns,
                          index = columns) 
    new_docs = []
    for doc in docs:
        nchunks = [nchunk.text for nchunk in doc.noun_chunks]
        nchunk_doc = nlp(" ".join(nchunks))
        new_docs.append(nchunk_doc)
    
    docs = new_docs
    for i in range(length):
        doc1 = docs[i]
        for j in range(i, length):
            doc2 = docs[j]
            sim = round(doc1.similarity(doc2), 4)
            df_sim.loc[j, i] = sim
            df_sim.loc[i, j] = sim
    
    display(df_sim)
    
compare2(docs)
~~~

![image-20210906190757059](images/image-20210906190757059.png)

### æ¯”è¾ƒå‘½ä»¤å®ä½“

ä¸‹é¢ä¸€ç§æ–¹æ³•ï¼Œé‡‡ç”¨æ¯”è¾ƒå‘½åå®ä½“ç›¸ä¼¼åº¦ã€‚æ•ˆæœä¹Ÿä¸é”™ã€‚

~~~python
def compare3(docs):
    length = len(docs)
    columns = list(range(length))
    df_sim = pd.DataFrame(columns = columns,
                          index = columns) 
    new_docs = []
    for doc in docs:
        ents = [ent.text for ent in doc.ents]
        ents_doc = nlp(" ".join(ents))
        new_docs.append(ents_doc)
    
    
    docs = new_docs
    for i in range(length):
        doc1 = docs[i]
        for j in range(i, length):
            doc2 = docs[j]
            sim = round(doc1.similarity(doc2), 4)
            df_sim.loc[j, i] = sim
            df_sim.loc[i, j] = sim
    
    display(df_sim)
    
compare3(docs)
~~~

![image-20210906190910777](images/image-20210906190910777.png)

### å»é™¤åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·

æœ€åä¸€ç§æ–¹æ³•æ˜¯å»é™¤åœç”¨è¯ï¼ˆstop wordsï¼‰å’Œæ ‡ç‚¹ç¬¦å·ã€‚çœ‹èµ·æ¥æ•ˆæœæœ€å¥½ã€‚

~~~python
def compare4(docs):
    length = len(docs)
    columns = list(range(length))
    df_sim = pd.DataFrame(columns = columns,
                          index = columns) 
    new_docs = []
    for doc in docs:
        tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]
        new_doc = nlp(" ".join(tokens))
#         print(new_doc) 
        new_docs.append(new_doc)
        
    docs = new_docs
    for i in range(length):
        doc1 = docs[i]
        for j in range(i, length):
            doc2 = docs[j]
            sim = round(doc1.similarity(doc2), 4)
            df_sim.loc[j, i] = sim
            df_sim.loc[i, j] = sim
    
    display(df_sim)
    
compare4(docs)
~~~

![image-20210906191423278](images/image-20210906191423278.png)

# ç»¼åˆ:  è¯­ä¹‰è§£æï¼ˆSemantic Parsingï¼‰

ç»¼åˆè¿ç”¨å‰å‡ èŠ‚æ‰€å­¦æ¥åˆ†æAirline Travel Information System (ATISï¼‰ï¼Œä¸€ä¸ªçŸ¥åçš„èˆªç©ºè®¢ç¥¨ç³»ç»Ÿæ•°æ®é›†ã€‚

æœ¬ç« å†…å®¹è§[ATIS_dataset_exploration](http://15.15.166.35:18888/notebooks/eipi10/xuxiangwen.github.io/_notes/05-ai/45-nlp/spacy/mastering_spacy/chapter06_ATIS_dataset_exploration.ipynb)ã€‚

```{.python .input  n=43}
import spacy 
import pandas as pd 
import utils

from collections import Counter 
from IPython.display import display
from spacy.matcher import Matcher

# auto load the changes of referenced codes
%load_ext autoreload
%autoreload 2

# ebablbe auto-completion
%config Completer.use_jedi = False
```

```{.json .output n=43}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
 }
]
```

## è·å–ATISæ•°æ®

```{.python .input  n=7}
df_atis = pd.read_csv("./Chapter06/data/atis_intents.csv", header=None)
df_atis.columns = ['intent', 'text']
```

```{.python .input  n=35}
print('-'*50)
print(len(df_atis))

print('-'*50)
display(df_atis.head(5))

print('-'*50)
display(df_atis.describe().T)

print('-'*50)
df_atis.intent.unique()

print('-'*50)
df_atis.info()
```

![image-20210913155626645](images/image-20210913155626645.png)

æŸ¥çœ‹æ•°æ®å†…å®¹ã€‚å¯ä»¥çœ‹åˆ°ï¼š

- ç¬¬ä¸€ä¸ªç”¨æˆ·è¯´æƒ³å®šæŸä¸ªæ—¶é—´ï¼Œä»æŸåœ°åˆ°æŸåœ°ideèˆªç­ï¼Ÿ
- ç¬¬äºŒä¸ªç”¨æˆ·åŒç¬¬ä¸€ä¸ªç”¨æˆ·
- ç¬¬ä¸‰ä¸ªç”¨æˆ·æƒ³é—®èˆªç­çš„åˆ°è¾¾æ—¶é—´ã€‚
- ç¬¬ä¸‰æ ¹ç”¨æˆ·æƒ³é—®ä»æŸåœ°åˆ°æŸåœ°æœ€ä¾¿å®œçš„æœºç¥¨ã€‚
- ç¬¬äº”ä¸ªç”¨æˆ·é—®ä»æŸåœ°åˆ°æŸåœ°ä½äº1000ç¾å…ƒçš„èˆªç­ã€‚

```{.python .input  n=13}
print(*list(df_atis.head().text), sep='\n')
```

![image-20210913155649267](images/image-20210913155649267.png)

æŸ¥çœ‹æ„å›¾çš„åˆ†å¸ƒã€‚

```{.python .input  n=36}
df_grouped = df_atis.groupby(['intent']).size()
df_grouped
```

![image-20210913155710322](images/image-20210913155710322.png)

åœ¨atis_abbreviationè¿™ç§æ„å›¾ä¸­ï¼Œç”¨æˆ·è¯¢é—®å…³äºå„ç§ç¼©å†™çš„å«ä¹‰ã€‚

```{.python .input  n=24}
df_filter = df_atis.loc[(df_atis.intent=='atis_abbreviation')]
print(*list(df_filter.sample(10).text), sep='\n')
```

![image-20210913155726368](images/image-20210913155726368.png)

## æŠ½å–å‘½åå®ä½“

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ä¹¦ä¸­ä½¿ç”¨å¦‚ä¸‹ä»£ç è·å–æ–‡æœ¬ï¼Œä»£ç å¦‚ä¸‹ã€‚æˆ‘ä»¬å®é™…é‡‡ç”¨è¯»å–df_atisçš„textåˆ—æ¥è·å–æ–‡æœ¬ã€‚

~~~shell
!awk -F ',' '{print $2}' ./Chapter06/data/atis_intents.csv  > .Chapter06/data/atis_utterances.txt
~~~

ç„¶åè¯»å–æ•°æ®ã€‚

```{.python .input  n=27}
nlp = spacy.load("en_core_web_md") 

corpus = open("Chapter06/data/atis_utterances.txt", "r").read().split("\n") 

all_ent_labels = [] 
for sentence in corpus: 
    doc = nlp(sentence.strip()) 
    ents = doc.ents 
    all_ent_labels += [ent.label_ for ent in ents] 

c = Counter(all_ent_labels) 
print(c) 
```

![image-20210913160003966](images/image-20210913160003966.png)

ä¸Šé¢çš„ä»£ç è¿è¡Œèµ·æ¥æœ‰äº›æ…¢ã€‚é‡‡ç”¨å¦‚ä¸‹ä»£ç å¿«å¥½å‡ å€ã€‚

```{.python .input  n=39}
texts = list(df_atis.text)
texts = [text.strip() for text in texts]
docs = nlp.pipe(texts)

all_ent_labels = [] 
for doc in docs: 
    ents = doc.ents 
    all_ent_labels += [ent.label_ for ent in ents] 

c = Counter(all_ent_labels) 
print(c) 
```

![image-20210913160013089](images/image-20210913160013089.png)

### ä½¿ç”¨Matcheræ¥æŠ½å–
æŠ½å–çš„å†…å®¹å¦‚ä¸‹ã€‚

```{.python .input  n=63}
def show_matches(nlp, doc, matches):
    for match_id, start, end in matches:
        pattern_name = nlp.vocab.strings[match_id]
        m_span = doc[start:end]  
        print(pattern_name, start, end, m_span.text)  
        
# èµ·å§‹åœ°å€
matcher = Matcher(nlp.vocab)
pattern = [{"POS": "ADP"}, {"ENT_TYPE": "GPE"}]
matcher.add("prepositionLocation", [pattern])

# èˆªç­ä¿¡æ¯
pattern = [{"ENT_TYPE": "ORG", "OP": "+"}]
matcher.add("AirlineName", [pattern])

# æ—¥æœŸå’Œæ—¶é—´
pattern = [{"ENT_TYPE":  {"IN": ["DATE", "TIME"]}}]
matcher.add("Datetime", [pattern])

# ç¼©å†™abbreviation
pattern1 = [{"TEXT": {"REGEX": "\w{1,2}\d{1,2}"}}]
pattern2 = [{"SHAPE": { "IN": ["x", "xx"]}}, {"SHAPE": {"IN": ["d", "dd"]}}]
pattern3 = [{"TEXT": {"IN": ["class", "code", "abbrev", "abbreviation"]}}, {"SHAPE": { "IN": ["x", "xx"]}}]
pattern4 = [{"POS": "NOUN", "SHAPE": { "IN": ["x", "xx"]}}]
matcher.add("abbrevEntities", [pattern1, pattern2, pattern3, pattern4])

texts = ["show me flights from denver to boston on tuesday",
         "i'm looking for a flight that goes from ontario to westchester and stops in chicago",
         "what flights arrive in chicago on sunday on continental",
         "yes i'd like a flight from long beach to st. louis by way of dallas",
         "what are the evening flights flying out of dalas",
         "what is the earliest united airlines flight flying from denver",
         'what does restriction ap 57 mean',
         'what does the abbreviation co mean',
         'what does fare code qo mean',
         'what is the abbreviation d10',
         'what does code y mean',
         'what does the fare code f and fn mean',
         'what is booking class c']
docs = nlp.pipe(texts)

for doc in docs:
    print('-'*50)
    spacy.displacy.render(doc, style='ent')
#     print([(ent.text, ent.label_) for ent in doc.ents])
    matches = matcher(doc)
    show_matches(nlp, doc, matches)
```

![image-20210913160100727](images/image-20210913160100727.png)

![image-20210913160613474](images/image-20210913160613474.png)

### ä½¿ç”¨dependency treesæ¥æŠ½å–

*I want to fly to Munich tomorrow.*

å¯¹äºä¸Šé¢çš„è¯­å¥ï¼Œä¸ŠèŠ‚çš„æŠ½å–æ–¹æ³•å¯ä»¥æŠ½å–åˆ°destination cityï¼Œä½†æ˜¯å¦‚æœæ˜¯ä¸‹é¢çš„è¯­å¥ï¼Œå°±æ— èƒ½ä¸ºäº†ã€‚

*I'm going to a conference in Munich. I need an air ticket.*
*My sister's wedding will be held in Munich. I'd like to book a flight*
*I want to book a flight to my conference without stopping at Berlin.*

æœ¬èŠ‚å°†å°è¯•ä½¿ç”¨dependency treesæ¥è§£æä»–ä»¬ä¹‹é—´çš„å…³ç³»ã€‚

```{.python .input  n=67}
texts = ["I'm going to a conference in Munich. I need an air ticket.",
         "My sister's wedding will be held in Munich. I'd like to book a flight",
         "I want to book a flight to my conference without stopping at Berlin."]

def reach_parent(source_token, dest_token):
    source_token = source_token.head
    while source_token != dest_token:
        if source_token.head == source_token:
            return None
        source_token = source_token.head
    return source_token

doc = nlp("I'm going to a conference in Munich.")
source_token = reach_parent(doc[-2], doc[3])
print(doc[-2], doc[3], source_token)
```

![image-20210913160916315](images/image-20210913160916315.png)

## ä½¿ç”¨dependency relationsæ¥è¯†åˆ«intent
### è¯­è¨€å­¦å…¥é—¨ï¼ˆLinguistic primerï¼‰

ATISæ•°æ®ä¸­åŒ…å«äº†å¤šä¸ªæ„å›¾ï¼ˆintentï¼‰ï¼Œæ¯”å¦‚ï¼š

- book a flight
- purchase a meal on their already booked flight
- cancel their flight

å¯ä»¥å‘ç°è¿™äº›æ„å›¾å¯ä»¥è¡¨ç¤ºä¸ºï¼šåŠ¨è¯ï¼ˆverbï¼‰ + å¯¹è±¡ï¼ˆobjectï¼‰ã€‚åœ¨æœ¬èŠ‚ä¸­å°†ä½¿ç”¨è¿™ä¸ªè§„åˆ™ï¼Œå°†æŠ½å–ï¼š
- åŠç‰©åŠ¨è¯/éåŠç‰©åŠ¨è¯ï¼ˆtransitive/intransitive verbsï¼‰
- ç›´æ¥å®¾è¯­/é—´æ¥å®¾è¯­ï¼ˆdirect/indirect objectsï¼‰

ä¸‹é¢æ˜¯é¦–å…ˆæ¥ç†è§£åŠç‰©åŠ¨è¯/éåŠç‰©åŠ¨è¯ã€‚

- åŠç‰©åŠ¨è¯
    ~~~
    I bought flowers.
    He loved his cat.
    He borrowed my book.
    ~~~

- éåŠç‰©åŠ¨è¯
    ~~~
    Yesterday I slept for 8 hours.
    The cat ran towards me.
    When I went out, the sun was shining.
    Her cat died 3 days ago.
    ~~~

å†æ¥çœ‹ç›´æ¥å®¾è¯­/é—´æ¥å®¾è¯­ã€‚

- ç›´æ¥å®¾è¯­
    ~~~
    I bought flowers.  I bought what? - flowers
    He loved his cat.  He loved who?  - his cat
    He borrowed my book. He borrowed what? - my book
    ~~~

- é—´æ¥å®¾è¯­
    ~~~
    He gave me his book.  He gave his book to whom?  - me
    He gave his book to me. He gave his book to whom? -me
    ~~~

ä¸‹é¢ä»£ç å°†å±•ç¤ºç›´æ¥å®¾è¯­å’Œé—´æ¥å®¾è¯­ã€‚æ›´å¤šç›¸å…³çŸ¥è¯†ï¼Œå¯ä»¥å‚è§è¿™æœ¬ä¹¦[Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax](https://dl.acm.org/doi/book/10.5555/2534456)ã€‚

- ç›´æ¥å®¾è¯­ï¼š ç”¨dobjè¡¨ç¤ºã€‚
- é—´æ¥å®¾è¯­ï¼š ç”¨dativeè¡¨ç¤ºã€‚

```{.python .input  n=69}
doc = nlp("He gave me his book.")
spacy.displacy.render(doc, style='dep')

doc = nlp("He gave his book to me. ")
spacy.displacy.render(doc, style='dep')
```

![image-20210913161556216](images/image-20210913161556216.png)

### æŠ½å–åŠç‰©åŠ¨è¯å’Œç›´æ¥å®¾è¯­

é€šè¿‡æŠ½å–transitive verbså’Œdirect objectsï¼Œæˆ‘ä»¬å¯ä»¥è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼ˆintentï¼‰ã€‚æ¯”å¦‚:

```{.python .input  n=72}
doc = nlp("find a flight from washington to sf")
spacy.displacy.render(doc, style='dep')
for token in doc:
    if token.dep_ == "dobj":
        print(token.head.text + token.text.capitalize())
```

![image-20210913161635424](images/image-20210913161635424.png)

### ä½¿ç”¨conjunction relationæŠ½å–å¤šä¸ªæ„å›¾

æœ‰äº›ç”¨æˆ·åœºæ™¯ï¼Œè¦†ç›–äº†å¤šä¸ªç”¨æˆ·æ„å›¾ã€‚æ¯”å¦‚ï¼š

show all flights and fares from denver to san francisco

ä½¿ç”¨token.conjunctså¯ä»¥è¿”å›coordinated tokensã€‚è¿™æ ·æˆ‘ä»¬ä¸éš¾å‘ç°ç”¨æˆ·çš„æ„å›¾æ˜¯showFlightså’ŒshowFares.

```{.python .input  n=75}
doc = nlp("show all flights and fares from denver to san francisco")
spacy.displacy.render(doc, style='dep')
for token in doc:
    if token.dep_ == "dobj":
        dobj = token.text
        conj = [t.text for t in token.conjuncts]
        verb = token.head
print(verb, dobj, conj)
```

![image-20210913161713932](images/image-20210913161713932.png)

### ä½¿ç”¨åŒä¹‰è¯åˆ—è¡¨è¯†åˆ«æ„å›¾

ä½¿ç”¨åŒä¹‰è¯åˆ—è¡¨æ¥è®¤è¯†æ„å›¾ã€‚

```{.python .input  n=91}
doc = nlp("i want to make a reservation for a flight")
spacy.displacy.render(doc, style='dep')

dObj =None
tVerb = None

# Extract the direct object and its transitive verb
for token in doc:
    if token.dep_ == "dobj":
        dObj = token
        tVerb = token.head 

print(f'dObj: {dObj}')
print(f'tVerb: {tVerb}')
        
# Extract the helper verb
intentVerb = None
verbList = ["want", "like", "need", "order"]
if tVerb.text in verbList:
    intentVerb = tVerb
else:
    if tVerb.head.dep_ == "ROOT":
        intentVerb = tVerb.head
        
# Extract the object of the intent
intentObj = None
objList = ["flight", "meal", "booking"]
if dObj.text in objList:
    intentObj = dObj
else:
    for child in tVerb.children:
        if child.dep_ == "prep":
            intentObj = list(child.children)[0]
            break
        elif child.dep_ == "compound":
            intentObj = child
            break
            
print(intentVerb.text + intentObj.text.capitalize())            
```

![image-20210913161741671](images/image-20210913161741671.png)

## è¯­æ³•çš„ç›¸ä¼¼æ€§

ä¸€èˆ¬æ¥è¯´ï¼Œæœ‰ä¸¤ç§æ–¹å¼æ¥è¯†åˆ«è¯­æ³•çš„ç›¸ä¼¼æ€§ã€‚

- ä½¿ç”¨åŒä¹‰è¯å­—å…¸ï¼ˆsynonyms dictionaryï¼‰
- ä½¿ç”¨åŸºäºè¯å‘é‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦

æœ¬èŠ‚å°†å°è¯•è¿™ä¸¤ç§æ–¹æ³•ã€‚

### ä½¿ç”¨åŒä¹‰è¯å­—å…¸

```{.python .input  n=102}

verbSynsets = {
    "show": ["list"],
    "book": ["make a reservation", "buy", "reserve"]
} 

objSynsets = {
    "meal": ["food"],
    "plane": ["aircraft", "airplane"]
}    


def get_vert_object(doc):
    for token in doc:
        if token.dep_ == "dobj":
            obj = token.lemma_
            verb = token.head.lemma_
            break    
    return verb, obj

def synonym_replace(verb, obj, 
                    verbSynsets=verbSynsets, 
                    objSynsets=objSynsets):
    for key, synonyms in verbSynsets.items():
        if verb in synonyms:
            verb = key
            break
                
    for key, synonyms in objSynsets.items():
        if obj in synonyms:
            obj = key
            break                
        
    return verb, obj
    


doc1 = nlp("show me all aircrafts that cp uses")
doc2 = nlp("list all meals on my flight")

verb1, obj1 = get_vert_object(doc1)
verb2, obj2 = get_vert_object(doc2)

print(verb1+obj1.capitalize())
print(verb2+obj2.capitalize())

synonym_verb1, synonym_obj1 = synonym_replace(verb1, obj1)
synonym_verb2, synonym_obj2 = synonym_replace(verb2, obj2)

print('-'*50)
print(verb1+obj1.capitalize())
print(verb2+obj2.capitalize())
```

![image-20210913162408016](images/image-20210913162408016.png)

### ä½¿ç”¨è¯å‘é‡

ä»ä¸‹é¢çš„ç›¸ä¼¼åº¦ï¼Œç»“æœå¯ä»¥è®¤å®šï¼Œè¿™ä¸¤ä¸ªè¯­å¥æ˜¯ç»Ÿä¸€åœºæ™¯çš„å¯èƒ½æ€§éå¸¸ä½ã€‚

```{.python .input  n=108}
def get_vert_object(doc):
    for token in doc:
        if token.dep_ == "dobj":
            obj = token
            verb = token.head
            break    
    return verb, obj

verb1, obj1 = get_vert_object(doc1)
verb2, obj2 = get_vert_object(doc2)

print(obj1.similarity(obj2))
print(verb1.similarity(verb2))
```

```{.json .output n=108}
[
 {
  "name": "stdout",
  "output_type": "stream",
  "text": "0.15025872\n0.33161193\n"
 }
]
```

## æ€»ç»“

æŠŠä¸Šé¢æ‰€æœ‰çš„å†…å®¹æ•´åˆåœ¨ä¸€èµ·ã€‚

```{.python .input  n=117}
matcher = Matcher(nlp.vocab)

doc = nlp("show me flights from denver to philadelphia on tuesday")
ents = doc.ents
print(f'ents = {ents}')

# ä»‹è¯ + åœ°ç‚¹
print('-'*50)
pattern = [{"POS": "ADP"}, {"ENT_TYPE": "GPE"}]
matcher.add("prepositionLocation", [pattern])
matches = matcher(doc)
show_matches(nlp, doc, matches)

# ç›´æ¥å®¾è¯­
print('-'*50)
for token in doc:
    if token.dep_ == "dobj":
        print(token.head.lemma_ + token.lemma_.capitalize())
```

![image-20210913162513718](images/image-20210913162513718.png)

# è‡ªå®šä¹‰sPacyæ¨¡å‹

## æ•°æ®å‡†å¤‡

åœ¨æ•°æ®å‡†å¤‡é˜¶æ®µï¼Œè¦è€ƒè™‘æˆ‘ä»¬æ˜¯å¦è¦è‡ªå®šä¹‰æ¨¡å‹ï¼Œè¿™éœ€è¦å›ç­”ä¸¤ä¸ªé—®é¢˜ã€‚

- ç°æœ‰æ¨¡å‹æ˜¯å¦åŒ…å«äº†æ‰€æœ‰è¦è¯†åˆ«çš„entitiyæˆ–label

  å¦‚æœå¤§éƒ¨åˆ†å·²ç»æœ‰äº†ï¼Œåªæ˜¯ç¼ºå°‘1-2ä¸ªï¼Œæˆ–è®¸æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‰æ–‡æ‰€è¿°çš„ä¸€äº›æ–¹æ³•å®Œæˆè¯†åˆ«ï¼Œè¿™æ ·å°±ä¸éœ€è¦è‡ªå®šä¹‰æ¨¡å‹ã€‚å¦‚æœå¤§éƒ¨åˆ†ç¼ºå°‘ï¼Œå¾ˆæ˜æ˜¾ï¼Œéœ€è¦è‡ªå®šä¹‰æ¨¡å‹ã€‚

- ç°æœ‰æ¨¡å‹çš„æ€§èƒ½æ˜¯å¦è¶³å¤Ÿå¥½ï¼Ÿ

  åœ¨ç°æœ‰æ¨¡å‹ä¸­ï¼Œè¯†åˆ«çš„å‡†ç¡®ç‡è¶³å¤Ÿå‘¢ï¼Ÿå¦‚æœä¸å¤Ÿå‡†ç¡®ï¼Œåˆ™éœ€è¦è‡ªå®šä¹‰æ¨¡å‹ã€‚

è‡ªå®šä¹‰æ¨¡å‹çš„æ­¥éª¤å¦‚ä¸‹ï¼š

- æ”¶é›†æ•°æ®

- æ ‡è®°ï¼ˆAnnotateï¼‰æ•°æ®

- å†³å®šä½¿ç”¨ç°æœ‰æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œè¿˜æ˜¯å®Œå…¨é‡æ–°è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹ã€‚

  ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœæ‰€æœ‰çš„entitiyæˆ–è€…labelåœ¨ç°æœ‰æ¨¡å‹ä¸­å·²ç»æœ‰äº†ï¼Œä¸€èˆ¬è€ƒè™‘é‡ç°æœ‰æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚

## æ ‡è®°ï¼ˆAnnotateï¼‰æ•°æ®

### ä½¿ç”¨Prodigyæ ‡è®°æ•°æ®

æ‰“å¼€https://prodi.gy/demoï¼Œå¯ä»¥çœ‹åˆ°æ¼”ç¤ºã€‚å¯ä»¥é€šè¿‡ç‚¹å‡»å’Œæ‹–æ‹‰è®¾å®šEntityæˆ–è€…Labelã€‚

![image-20210923131627251](images/image-20210923131627251.png)

### ä½¿ç”¨Bratæ ‡è®°æ•°æ®

æ‰“å¼€https://brat.nlplab.org/examples.htmlã€‚Bratå¯ä»¥äº§ç”Ÿjsonæ–‡ä»¶ã€‚

![annotation example](images/PMID-20300060-small.png)

> ç½‘ç«™ä¸­çš„å¤šä¸ªdemoæ‰“ä¸å¼€ï¼Œéš¾é“æ˜¯è¿‡æ—¶äº†å—ï¼Ÿ

> è¿˜æœ‰ä¸€äº›å¼€æºå·¥å…·ä¹Ÿå¯ä»¥ç”¨æ¥æ ‡è®°æ•°æ®ã€‚æ¯”å¦‚ï¼š[Labeling Studio](https://labelstud.io/)ã€‚

### spaCyè®­ç»ƒæ•°æ®æ ¼å¼

spaCy 2.0æ”¯æŒjsonæ ¼å¼ã€‚ç¤ºä¾‹å¦‚ä¸‹

~~~
[
  ('I will visit you in Munich.', {'entities': [(20, 26, 'GPE')]}),
  ("I'm going to Victoria's house.", {'entities': [(13, 23, 'PERSON'), (24, 29, 'GPE')]}),
  ('I go there.', {'entities': []})
]
~~~

spaCy 3.0ä¸­éœ€è¦æŠŠä¸Šè¯‰jsonæ ¼å¼è½¬åŒ–ä¸ºExampleã€‚

~~~python
import spacy
from spacy.training import Example

def json_to_example(data, nlp):  
    examples = []
    for text, annotations in data:    
        doc = nlp.make_doc(text)
        example = Example.from_dict(doc, annotations)            
        examples.append(example)
    return examples
        
nlp = spacy.load("en_core_web_md")  

training_data = [
  ('I will visit you in Munich.', {'entities': [(20, 26, 'GPE')]}),
  ("I'm going to Victoria's house.", {'entities': [(13, 23, 'PERSON'), (24, 29, 'GPE')]}),
  ('I go there.', {'entities': []})
]

traing_examples = json_to_example(training_data, nlp)
print(traing_examples[0], type(traing_examples[0]))
~~~

![image-20210923144108414](images/image-20210923144108414.png)

Exampleä¸­åŒ…å«äº†ä¸¤ä¸ªdocï¼špredictedå’Œreferenceã€‚

~~~python
def show_example(example):
    attributes = [
     'alignment',
     'predicted',
     'reference',
     'text',
     'x',
     'y'
    ]
    for attribute in attributes:
        print('-'*25, attribute, '-'*25)
        value = getattr(example, attribute)
        print(value)
        
show_example(traing_examples[0])
~~~

![image-20210923144047324](images/image-20210923144047324.png)

## æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°

### æ•°æ®å’Œæ¨¡å‹å‡†å¤‡

~~~python
import json
import spacy
import random
from spacy.training import Example
from spacy.scorer import Scorer
from sklearn.model_selection import train_test_split

def get_model():
    nlp = spacy.blank("en")
    print(f'nlp.pipe_names={nlp.pipe_names}')

    ner = nlp.add_pipe("ner")
    print(f'nlp.pipe_names={nlp.pipe_names}')
    print(f'nlp.meta={nlp.meta}')

    # ç—…èŒ, èº«ä½“çŠ¶å†µ, è¯å“
    labels = ['Pathogen', 'MedicalCondition', 'Medicine']
    for ent in labels:
        ner.add_label(ent)
    print(f'ner.labels={ner.labels}')   
    return nlp

print('-'*25, 'åˆ›å»ºåˆå§‹æ¨¡å‹', '-'*25)
nlp = get_model()
options = {'colors': {'Pathogen':"#56D7C4", 
                      'MedicalCondition':"#92E0AA", 
                      'Medicine':"lightgreen"} 
          }

print('-'*25, 'æ•°æ®å‡†å¤‡', '-'*25)
with open("data/corona.json") as f:
    data = json.loads(f.read())

data_ = []

for (text, annot) in data:
    new_anno = []
    for st, end, label in annot["entities"]:
        new_anno.append((st, end, label))
    data_.append((text, {"entities": new_anno}))

print(data_[0])

# ç”Ÿæˆè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
print('-'*25, 'ç”Ÿæˆè®­ç»ƒå’Œæµ‹è¯•æ•°æ®', '-'*25)
examples = []
for text, annots in data_:    
    examples.append(Example.from_dict(nlp.make_doc(text), annots))
    
train_examples, test_examples = train_test_split(examples,  
                                                 test_size=0.2, 
                                                 random_state=202109)

print(f'len(examples)={len(examples)}')  
print(f'len(train_examples)={len(train_examples)}')
print(f'len(test_examples)={len(test_examples)}')
~~~

![image-20210927073104088](images/image-20210927073104088.png)

### è®­ç»ƒ

~~~python
def get_scores(nlp, examples):
    scores = nlp.evaluate(examples)
    # ä»¥ä¸‹ä»£ç ï¼Œæ•ˆæœå®Œå…¨ç›¸åŒ
    # scorer = Scorer()
    # predicted_examples = [Example(nlp(example.predicted.text), example.reference)  
    #                       for example in examples]        
    # scores = scorer.score(predicted_examples)
    
    auc = round(scores['cats_score'], 3)
    p = round(scores['cats_macro_p'], 3)
    r = round(scores['cats_macro_r'], 3)
    f1 = round(scores['cats_macro_f'], 3)
    return auc, p, r, f1

def get_score_text(scores):
    return f'auc:{scores[0]:>5}, p:{scores[1]:>5}, r:{scores[2]:>5}, f:{scores[3]:>5})'

def train(nlp, textcat, train_examples, test_examples, epochs, best_model_path):
    textcat.initialize(lambda: train_examples, nlp=nlp)
    with nlp.select_pipes(enable="textcat_multilabel"):  
        optimizer = nlp.resume_training()
        best_auc = None    
        for itn in range(epochs):
            losses = {}        
            random.shuffle(train_examples)
            for batch in spacy.util.minibatch(train_examples, size=8):
                nlp.update(batch, losses=losses) 

            train_scores = get_scores(nlp, train_examples)
            test_scores = get_scores(nlp, test_examples)
            auc = test_scores[0]

            loss = losses['textcat_multilabel']    
            print((f"{itn+1:>2}/{epochs} "  
                   f"train=(loss:{round(loss, 3):>5}, {get_score_text(train_scores)}"
                   f" test=({get_score_text(test_scores)}"))
            if best_auc is None or auc > best_auc:
                best_auc = auc        
                nlp.to_disk(best_model_path)

print('-'*25, 'è®­ç»ƒ', '-'*25)
# ! mkdir output/first_classification                
best_model_path='output/first_classification/best'                
train(nlp, textcat, train_examples, test_examples, epochs=10, best_model_path=best_model_path)
~~~

![image-20210927073424369](images/image-20210927073424369.png)

### è¯„ä¼°å’Œé¢„æµ‹

ç”±äºè®­ç»ƒæ•°æ®éå¸¸å°‘ï¼ˆåªæœ‰16æ¡ï¼‰ï¼Œæ‰€ä»¥æ¨¡å‹æ•ˆæœä¸ä½³ï¼Œè¿‡æ‹Ÿåˆä¸¥é‡ã€‚

~~~python
nlp = spacy.load(best_model_path)
# ç­‰ä»·äºä¸‹é¢è¿™æ¡è¯­å¥
# nlp = get_model().from_disk(best_model_path)

train_scores = get_scores(nlp, train_examples)
test_scores = get_scores(nlp, test_examples)

print(f"train=(p:{train_scores[0]:>5}, r:{train_scores[1]:>5}, f:{train_scores[2]:>5})")
print(f" test=(p:{test_scores[0]:>5}, r:{test_scores[1]:>5}, f:{test_scores[2]:>5})")
~~~

![image-20210927073454495](images/image-20210927073454495.png)

æ¥ä¸‹æ¥çœ‹çœ‹æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

~~~python
def show_results(nlp, texts):
    docs = list(nlp.pipe(texts))
    for doc in docs:
        print('-'*80)
        spacy.displacy.render(doc, style='ent', options=options)

texts = [
    "One of the bacterial diseases with the highest disease burden is tuberculosis, caused by Mycobacterium tuberculosis bacteria, which kills about 2 million people a year.",
    "Pathogenic bacteria contribute to other globally important diseases, such as pneumonia, which can be caused by bacteria such as Streptococcus and Pseudomonas, and foodborne illnesses, which can be caused by bacteria such as Shigella, Campylobacter, and Salmonella. Pathogenic bacteria also cause infections such as tetanus, typhoid fever, diphtheria, syphilis, and leprosy. Pathogenic bacteria are also the cause of high infant mortality rates in developing countries."
]   
   
show_results(nlp1, texts) 
~~~

![image-20210927073530205](images/image-20210927073530205.png)

æœ€åæ£€æŸ¥ä»¥ä¸‹test_examplesä¸­çš„é¢„æµ‹ç»“æœã€‚å¯ä»¥çœ‹åˆ°æ¨¡å‹é¢„æµ‹çš„è¯¯å·®è¿˜æ˜¯å¾ˆå¤§çš„ã€‚

~~~python
for example in test_examples[0:1]:
    print('='*80)
    doc = nlp(example.predicted.text)
    if len(doc.ents)>0:
        print('-'*30, 'predicted', '-'*30)
        spacy.displacy.render(doc, style='ent', options=options)
        print('-'*30, 'reference', '-'*30)
        spacy.displacy.render(example.reference, style='ent', options=options)            
    else:
        print(doc.text)
~~~

![image-20210927073638656](images/image-20210927073638656.png)

# æ–‡æœ¬åˆ†ç±»

![image-20210926143908023](images/image-20210926143908023.png)

ä½¿ç”¨[TextCategorizer](https://spacy.io/api/textcategorizer)ç»„ä»¶æ¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚TextCategorizerå¯ä»¥åˆ†ä¸ºä¸¤å¤§ç±»ï¼š

- TextCategorizerï¼šä¸€ä¸ªæ ·æœ¬ä»…ä»…å±äºä¸€ä¸ªlabelã€‚è¯¦è§[textcat.py](https://github.com/explosion/spaCy/blob/master/spacy/pipeline/textcat.py)

  ~~~python
  from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL
  config = {
     "threshold": 0.5,  
     "model": DEFAULT_SINGLE_TEXTCAT_MODEL
  }
  nlp = spacy.blank("en")
  textcat = nlp.add_pipe("textcat", config=config)
  textcat
  ~~~

  ![image-20210926152207940](images/image-20210926152207940.png)

- MultiLabel_TextCategorizerï¼šä¸€ä¸ªæ ·æœ¬å±äºä¸€ä¸ªæˆ–å¤šä¸ªlabelã€‚ç»§æ‰¿è‡ªTextCategorizerã€‚è¯¦è§[textcat_multilabel.py](https://github.com/explosion/spaCy/blob/master/spacy/pipeline/textcat_multilabel.py)

  ~~~python
  from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL
  config = {
     "threshold": 0.5,
     "model": DEFAULT_MULTI_TEXTCAT_MODEL
  }
  nlp = spacy.blank("en")
  textcat = nlp.add_pipe("textcat_multilabel", config=config)
  textcat
  ~~~

  ![image-20210926152219305](images/image-20210926152219305.png)

## æƒ…æ„Ÿåˆ†ç±» - spaCy

æƒ…æ„Ÿåˆ†ç±»ï¼ˆSentiment Classificationï¼‰æ˜¯ä¸€ç§äºŒå…ƒåˆ†ç±»ï¼ˆbinary classificationï¼‰ã€‚è¯¦è§[reviews_spacy_0927](http://15.15.175.163:18888/notebooks/eipi10/xuxiangwen.github.io/_notes/05-ai/45-nlp/spacy/mastering_spacy/reviews_spacy_0927.ipynb)ã€‚

### æ•°æ®å’Œæ¨¡å‹å‡†å¤‡

æ•°æ®ä¸­æ¥è‡ª[Amazon Fine Food Reviews dataset](https://www.kaggle.com/snap/amazon-fine-food-reviews)ã€‚

~~~python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np 
import spacy
import random

from IPython.core.display import display
from spacy.training import Example
from spacy.pipeline.textcat_multilabel import DEFAULT_MULTI_TEXTCAT_MODEL
from sklearn.model_selection import train_test_split

reviews_df=pd.read_csv('data/Reviews.csv')
print(reviews_df.shape)
display(reviews_df.head(5))

# è¯„åˆ†åˆ†å¸ƒ
ax=reviews_df.Score.value_counts().plot(kind='bar', colormap='Paired')
plt.show()
~~~

![image-20210926222112181](images/image-20210926222112181.png)

ä¸‹é¢è¿›è¡Œæ•°æ®æ¸…ç†ã€‚

~~~python
reviews_df = reviews_df[['Text','Score']].dropna()
print(reviews_df.shape)
display(reviews_df.head(5))

# è¯„åˆ†åˆ†å¸ƒ
ax=reviews_df.Score.value_counts().plot(kind='bar', colormap='Paired')
plt.show()
~~~

![image-20210926222220166](images/image-20210926222220166.png)

å¯ä»¥çœ‹åˆ°æ•°æ®æ˜¯éå¸¸ä¸å‡è¡¡çš„ï¼Œä¸‹é¢æŠŠè¯„åˆ†å½’ä¸ºä¸¤å¤§ç±»ã€‚

~~~python
reviews_df.loc[reviews_df.Score<=3, 'Score']=0
reviews_df.loc[reviews_df.Score>3, 'Score']=1
display(reviews_df.head(5))

ax=reviews_df.Score.value_counts().plot(kind='bar',
colormap='Paired')
plt.show()
~~~

![image-20210926222344423](images/image-20210926222344423.png)

åŠ ä¸‹æ¥ï¼Œåˆå§‹åŒ–æ¨¡å‹ï¼Œç„¶åç”Ÿæˆè®­ç»ƒæµ‹è¯•æ•°æ®ã€‚

~~~python
print('-'*25, 'åˆ›å»ºåˆå§‹æ¨¡å‹', '-'*25)
spacy.prefer_gpu()  # è¿™å¥ä»£ç å¿…é¡»åœ¨loadä¹‹å‰è°ƒç”¨
nlp = spacy.load("en_core_web_md")
config = {
    "threshold": 0.5,
    "model": DEFAULT_MULTI_TEXTCAT_MODEL
}
textcat = nlp.add_pipe("textcat_multilabel", config=config)
textcat.add_label("POS")
textcat.add_label("NEG")


print('-'*25, 'ç”Ÿæˆè®­ç»ƒæµ‹è¯•æ•°æ®', '-'*25)
examples = []
for index, row in reviews_df.iterrows():
    text = row["Text"]
    rating = row["Score"]
    label = {"POS": True, "NEG": False} if rating == 1 else {"NEG": True, "POS": False}
    examples.append(Example.from_dict(nlp.make_doc(text), {"cats": label})) 
    
print(f'len(examples)={len(examples)}')
    
train_examples, test_examples = train_test_split(examples,  
                                                 test_size=0.2, 
                                                 random_state=202109)    
print(f'len(train_examples)={len(train_examples)}')
print(f'len(test_examples)={len(test_examples)}')
~~~

![image-20210926224153147](images/image-20210926224153147.png)

### è®­ç»ƒ

~~~python
def get_scores(nlp, examples, metrics=['cats_macro_auc', 'cats_macro_p', 'cats_macro_r', 'cats_macro_f']):
    scores = nlp.evaluate(examples)
    # ä»¥ä¸‹ä»£ç ï¼Œæ•ˆæœå®Œå…¨ç›¸åŒ
    # scorer = Scorer()
    # predicted_examples = [Example(nlp(example.predicted.text), example.reference)  
    #                       for example in examples]        
    # scores = scorer.score(predicted_examples)
    if isinstance(metrics[0], tuple):
        metrics = [metric for metric, _ in metrics]     
    scores = {metric:value  for metric, value in scores.items() if metric in metrics}
    return scores

def get_score_text(metrics=['cats_macro_auc', 'cats_macro_p', 'cats_macro_r', 'cats_macro_f']):    
    def get_text(scores):
        score_texts = [f'{display}:{round(scores[metric], 3):>5}' for metric, display in metrics]
        return ', '.join(score_texts) 
    
    if isinstance(metrics[0], str):
        metrics = [(metric, metric) for metric in metrics] 
    return get_text

def train(nlp, textcat, train_examples, test_examples, epochs, best_model_path, 
          metrics, best_metric, dropout=0.0, ):
    textcat.initialize(lambda: train_examples, nlp=nlp)
    score_text_fun = get_score_text(metrics)

    with nlp.select_pipes(enable="textcat_multilabel"):  
        optimizer = nlp.resume_training()
        best_metric_value = None    
        for itn in range(epochs):
            losses = {}        
            random.shuffle(train_examples)
            for batch in spacy.util.minibatch(train_examples, size=8):
                nlp.update(batch, losses=losses, drop=dropout) 

            train_scores = get_scores(nlp, train_examples, metrics)
            test_scores = get_scores(nlp, test_examples, metrics)
            metric_value = test_scores[best_metric]

            loss = losses['textcat_multilabel']    
            print((f"{itn+1:>2}/{epochs} "  
                   f"train=(loss:{round(loss, 3):>5}, {score_text_fun(train_scores)})"
                   f" test=({score_text_fun(test_scores)})"))
            if best_metric_value is None or metric_value > best_metric_value:
                best_metric_value = metric_value        
                nlp.to_disk(best_model_path)

print('-'*25, 'è®­ç»ƒ', '-'*25)
# ! mkdir output/first_classification                
best_model_path='output/first_classification/best'  
metrics = [('cats_macro_auc', 'auc'), ('cats_macro_p', 'p'), 
           ('cats_macro_r', 'r'), ('cats_macro_f', 'f')]
train(nlp, textcat, train_examples, test_examples, epochs=10, best_model_path=best_model_path, 
      metrics=metrics, best_metric='cats_macro_auc', dropout=0.5)
~~~

![image-20210927092626214](images/image-20210927092626214.png)

> è¿™é‡Œç©¿æ’ä¸€ç‚¹ï¼Œå¯¹äºmicro, macroå’Œweightedçš„è§£é‡Šã€‚
>
> ~~~
>               precision    recall  f1-score   support
> 
>            1       0.50      0.67      0.57         3
>            2       0.33      1.00      0.50         1
>            3       1.00      0.25      0.40         4
> 
>    micro avg       0.50      0.50      0.50         8
>    macro avg       0.61      0.64      0.49         8
> weighted avg       0.73      0.50      0.48         8
> ~~~
>
> - microï¼šä¸åŒºåˆ«åˆ†ç±»åˆ«è®¡ç®—æ€»ä½“çš„å€¼ã€‚ä»¥precisionä¸ºä¾‹
>   $$
>   0.50 = \frac {0.67*3 + 1.00*1 + 0.25*4} 8
>   $$
>
> - macroï¼š å„ä¸ªåˆ†ç±»çš„å€¼è¿›è¡Œç®—æ•°å¹³å‡ã€‚ä»¥precisionä¸ºä¾‹
>   $$
>   0.61 = \frac {0.50 + 0.33+ 1.00} 3
>   $$
>
> - weightedï¼š å„ä¸ªåˆ†ç±»çš„å€¼è¿›è¡Œæƒé‡å¹³å‡ã€‚ä»¥precisionä¸ºä¾‹
>   $$
>   0.73 = \frac {0.50*3 + 0.33*1+ 1.00*4} 8
>   $$

### è¯„ä¼°å’Œé¢„æµ‹

æ¨¡å‹æ•ˆæœè¿˜è¡Œï¼Œæœ‰ä¸€äº›è¿‡æ‹Ÿåˆã€‚

~~~python
spacy.prefer_gpu()
nlp = spacy.load(best_model_path)

metrics=['cats_macro_auc', 'cats_micro_p', 'cats_macro_p', 
         'cats_micro_r', 'cats_macro_r', 'cats_micro_f', 'cats_macro_f']
metrics = [(metric, metric[5:]) for metric in metrics]
train_scores = get_scores(nlp, train_examples, metrics)
test_scores = get_scores(nlp, test_examples, metrics)
score_text_fun=get_score_text(metrics)

print(f"train=({score_text_fun(train_scores)}")
print(f" test=({score_text_fun(test_scores)}")
~~~

![image-20210927092819377](images/image-20210927092819377.png)

> ä¸Šè¿°æ¨¡å‹ä½¿ç”¨çš„æ•°æ®é›†æ˜¯3999æ¡ï¼Œä¹Ÿå°è¯•äº†ä½¿ç”¨å®Œæ•´çš„reivewæ•°æ®ï¼ˆ568454æ¡ï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½†æ˜¯æ˜¯å°†å¾ˆé•¿ä¹Ÿæ— æ³•å®Œæˆè®­ç»ƒã€‚

æ¥ä¸‹æ¥çœ‹çœ‹æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

~~~python
def show_results(nlp, texts):
    docs = list(nlp.pipe(texts))
    for doc in docs:
        print('-'*25, doc.cats,'-'*25)
        print(doc.text)

texts = [
    "This is the best food I ever ate",
    "This food is so bad"
]   
   
show_results(nlp, texts) 
~~~

![image-20210927093213034](images/image-20210927093213034.png)

æœ€åæ£€æŸ¥ä»¥ä¸‹test_examplesä¸­çš„é¢„æµ‹ç»“æœï¼Œçœ‹èµ·æ¥è¿˜ä¸é”™å•Šã€‚

~~~python
sample_index = np.random.randint(0, len(test_examples), 3)
for i in sample_index:
    example = test_examples[i] 
    print('='*40, i, '='*40)
    doc = nlp(example.predicted.text)
    print(example.predicted.text)
    print('-'*25, f'predicted:{doc.cats}', '-'*25)
    print('-'*25, f'reference: {example.reference.cats}', '-'*25)
~~~

![image-20210927093804526](images/image-20210927093804526.png)

> çš„æ•°æ®é›†æ˜¯3999æ¡ï¼Œä¹Ÿå°è¯•äº†ä½¿ç”¨å®Œæ•´çš„reivewæ•°æ®ï¼ˆ568454æ¡ï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½†é€Ÿåº¦å¥‡æ…¢æ— æ¯”ï¼Œçœ‹æ¥è¿™æˆ–è®¸æ˜¯spaCy 3.0å»ºè®®é‡‡ç”¨cfgé…ç½®æ–‡ä»¶æ¥è®­ç»ƒçš„åŸå› å§ã€‚
>
> äºæ˜¯æŠŠæ•°æ®ä»…ä»…å¢åŠ åˆ°40000ï¼ˆåŸæ¥10å€ï¼‰ï¼Œæ€§èƒ½ä¹Ÿæœ‰æå¤§çš„æå‡ã€‚
>
> ![image-20210928121809927](images/image-20210928121809927.png)

## æƒ…æ„Ÿåˆ†ç±» - TensorFlow

ä¸‹é¢ä½¿ç”¨Tensorflowæ¥å®ç°åˆ†ç±»æ¨¡å‹ã€‚è¯¦è§[](http://15.15.175.163:18888/notebooks/eipi10/xuxiangwen.github.io/_notes/05-ai/45-nlp/spacy/mastering_spacy/reviews_0927.ipynb#%E6%A8%A1%E5%9E%8B)

![image-20210927144055712](images/image-20210927144055712.png)

![image-20210927143742734](images/image-20210927143742734.png)

å’ŒspaCyçš„ç»“æœæ¯”è¾ƒï¼ŒTFæ¨¡å‹åˆ†ç±»çš„æ•ˆæœæ›´å¥½ã€‚

![image-20210927092819377](images/image-20210927092819377.png)

> ä¸Šè¿°æ¨¡å‹ä½¿ç”¨çš„æ•°æ®é›†æ˜¯3999æ¡ï¼Œä¹Ÿå°è¯•äº†ä½¿ç”¨å®Œæ•´çš„reivewæ•°æ®ï¼ˆ568454æ¡ï¼‰è¿›è¡Œè®­ç»ƒï¼Œæ€§èƒ½æœ‰å¾ˆå¤§æé«˜ã€‚
>
> ![image-20210928003550569](images/image-20210928003550569.png)
>
> ![image-20210928004230411](images/image-20210928004230411.png)

# spaCyå’ŒTransformers

## ç†è§£BERT

BERTä¸­çš„ä¸€äº›ç‰¹åˆ«tokenã€‚

- CLSï¼šæ¯ä¸ªè¾“å…¥sequenceçš„ç¬¬ä¸€ä¸ªtokenã€‚
- SEP: å¥å­ä¹‹é—´çš„åˆ†éš”ç¬¦ã€‚
- PADï¼š è¡¨ç¤ºpaddingã€‚

## Transformerå’ŒTensorflow

Transformersæ˜¯[Hugging Face](https://huggingface.co/)æ¨å‡ºçš„æ·±åº¦å­¦ä¹ åŒ…ï¼Œå®ƒçš„å‰èº«æ˜¯pytorch-transformerså’Œpytorch-pretrained-bertï¼Œä¸»è¦æä¾›äº†è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰å’Œè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰çš„é€šç”¨ä½“ç³»ç»“æ„ï¼ˆBERTï¼ŒGPT-2ï¼ŒRoBERTaï¼ŒXLMï¼ŒDistilBertï¼ŒXLNetç­‰ï¼‰ ï¼‰åŒ…å«è¶…è¿‡32ç§ä»¥100å¤šç§è¯­è¨€ç¼–å†™çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠTensorFlow 2.0å’ŒPyTorchä¹‹é—´çš„æ·±åº¦äº’æ“ä½œæ€§ã€‚

![image-20210927163316663](images/image-20210927163316663.png)

### ä½¿ç”¨BERT tokenizer

~~~python
from transformers import BertTokenizer

btokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
sentence = "He lived characteristically idle and romantic."
tokens = btokenizer.tokenize("[CLS] " + sentence + " [SEP]")
print(tokens)

ids = btokenizer.convert_tokens_to_ids(tokens)
print(ids)

ids = btokenizer.encode(sentence)
print(ids)

tokens = btokenizer.convert_ids_to_tokens(ids)
print(tokens)

sentence = btokenizer.decode(ids)
print(sentence)
~~~

![image-20210927185555451](images/image-20210927185555451.png)

æ¥ä¸‹æ¥åŠ ä¸Špaddingçš„é€»è¾‘ã€‚

~~~python
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel

physical_devices = tf.config.list_physical_devices('GPU')
for device in physical_devices:
    tf.config.experimental.set_memory_growth(device, True)

btokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
sentence = "He lived characteristically idle and romantic."

encoded = btokenizer.encode_plus(
        text=sentence,
        add_special_tokens=True,
        max_length=12,
        padding='max_length',  #'longest', 'max_length'
        return_tensors="tf"
)

token_ids = encoded["input_ids"]
print(token_ids)

tokens = btokenizer.convert_ids_to_tokens(tf.squeeze(token_ids))
print(tokens)

sentence = btokenizer.decode(tf.squeeze(token_ids))
print(sentence)
~~~

![image-20210927185618658](images/image-20210927185618658.png)

### è·å–BERTè¯å‘é‡

ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œå¯¹äºåœ¨ä¸åŒä½ç½®çš„Bankï¼ŒBERTç»™å‡ºäº†ä¸åŒçš„è¯å‘é‡ã€‚

~~~python
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel

physical_devices = tf.config.list_physical_devices('GPU')
for device in physical_devices:
    tf.config.experimental.set_memory_growth(device, True)
    
def cosine_distance(tensor1, tensor2):
    # æ±‚æ¨¡é•¿
    tensor1_norm = tf.sqrt(tf.reduce_sum(tf.square(tensor1)))
    tensor2_norm = tf.sqrt(tf.reduce_sum(tf.square(tensor2)))
    
    # å†…ç§¯
    tensor1_tensor2 = tf.reduce_sum(tf.multiply(tensor1,tensor2))
    cosin = tensor1_tensor2/(tensor1_norm*tensor2_norm)
    
    return cosin

def euclidean_distance(tensor1, tensor2):
     return tf.sqrt(tf.reduce_sum(tf.square(tensor2-tensor1)))

btokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bmodel = TFBertModel.from_pretrained("bert-base-uncased")

sentence = "open a bank account. sit on a bank."
encoded = btokenizer.encode_plus(
    text=sentence,
    add_special_tokens=True,
    max_length=15,
    padding='max_length',
    return_attention_mask=True,
    return_tensors="tf"
)

inputs = encoded["input_ids"]
outputs = bmodel(inputs)
print(type(outputs), len(outputs))
print(outputs[0].shape)
print(outputs[1].shape)    

tokens = btokenizer.convert_ids_to_tokens(tf.squeeze(inputs))
bank1_vector, bank2_vector = [vector for token, vector in zip(tokens, outputs[0][0]) if token=='bank']
print(f'cosine_distance={cosine_distance(bank1_vector, bank2_vector)}')
print(f'euclidean_distance={euclidean_distance(bank1_vector, bank2_vector)}')
~~~

![image-20210927194208779](images/image-20210927194208779.png)

ä¸Šé¢ä¸¤ä¸ªbankåœ¨ä¸€ä¸ªå¥å­é‡Œï¼Œå¦‚æœåœ¨ä¸åŒå¥å­ã€‚å·®å¼‚æ›´å¤§ã€‚

~~~python
sentences = ["open a bank account.", "sit on a bank."]
encoded = btokenizer.batch_encode_plus(
        batch_text_or_text_pairs=sentences,
        add_special_tokens=True,
        max_length=10,
        padding='max_length',
        return_attention_mask=True,
        return_tensors="tf"
)

inputs = encoded["input_ids"]
outputs = bmodel(inputs)

tokens_list = [btokenizer.convert_ids_to_tokens(tf.squeeze(inputs_)) for inputs_ in inputs]
bank_vectors = []
for i, tokens in enumerate(tokens_list):
    for token, vector in zip(tokens, outputs[i][0]):
        if token=='bank':
            bank_vectors.append(vector)
        
bank1_vector, bank2_vector = bank_vectors
print(f'cosine_distance={cosine_distance(bank1_vector, bank2_vector)}')
print(f'euclidean_distance={euclidean_distance(bank1_vector, bank2_vector)}')
~~~

![image-20210927195048758](images/image-20210927195048758.png)

### ä½¿ç”¨BERTè¿›è¡Œæ–‡æœ¬åˆ†ç±»

æŠŠTransformerç»“åˆTensorflowæ¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚

å‚è§

### ä½¿ç”¨Transformer pipelines

HuggingFace Transformersæä¾›äº†pipelinesåŠŸèƒ½ï¼Œæˆ‘ä»¬æ— é¡»è®­ç»ƒå°±å¯ä»¥å®ç°å¾ˆå¤šåŠŸèƒ½ã€‚

- Sentiment analysis
- Question answering 
- NER 
- Text summarization
- Translation

ä¸‹é¢æ˜¯æƒ…æ„Ÿåˆ†ç±»ã€‚

~~~python
from transformers import pipeline
nlp = pipeline("sentiment-analysis")

sentences = ["I hate you so much right now.", "I love fresh air and exercising."]
results = [nlp(sentence) for sentence in sentences] 
for result in results:
    print(result) 
~~~

![image-20211001220654601](images/image-20211001220654601.png)

å†çœ‹question answering.

~~~pythong
from transformers import pipeline

def answer(nlp, question_context):
    res = nlp(question_context)
    print(question_context['question'], res) 

def answers(nlp, question_contexts):
    for question_context in question_contexts:
        answer(nlp, question_context)

nlp = pipeline("question-answering")

question_contexts = [
    {
        'question': "What is the name of this book?",
        'context': "I'll publish my new book Mastering spaCy soon."
    },
    {
        'question': "Who will publish a new book?",
        'context': "I'll publish my new book Mastering spaCy soon."
    },   
    {
        'question': "What will I do?",
        'context': "I'll publish my new book Mastering spaCy soon."
    },    
    {
        'question': "Who lived in Zurich?",
        'context': "Einstein lived in Zurich."
    },
    {
        'question': "Where did Einstein live?",
        'context': "Einstein lived in Zurich."
    },     
    {
        'question': "Where am I?",
        'context': "I want to find a flight from washington to sf"
    },  
    {
        'question': "What do I want to do?",
        'context': "I want to find a flight from washington to sf"
    },                          
]

answers(nlp, question_contexts)
~~~

![image-20211001223239649](images/image-20211001223239649.png)

ä»ä¸Šé¢ç»“æœæ¥çœ‹ï¼Œæ•ˆæœè¿˜ä¸é”™ã€‚

### Transformers and spaCy

![image-20211001224903767](images/image-20211001224903767.png)

åœ¨spaCy v3.0å¼•å…¥äº†transformer based pipelinesã€‚é¦–å…ˆå¼•å…¥æ¨¡å‹ã€‚

~~~
python3 -m spacy download en_core_web_trf
~~~

ä¸‹é¢åŠ è½½æ¨¡å‹ã€‚

~~~python
import spacy
nlp = spacy.load("en_core_web_trf")
doc = nlp("It went there unwillingly.")

print(doc._.trf_data.wordpieces)
print(doc._.trf_data.tensors[0].shape)
print(doc._.trf_data.tensors[1].shape)
~~~

![image-20211001230218664](images/image-20211001230218664.png)

å¯ä»¥çœ‹åˆ°ï¼Œéå¸¸æ˜æ˜¾spaCyå¼•å…¥äº†Transformersã€‚

# ä½¿ç”¨spaCyè®¾è®¡èŠå¤©æœºå™¨äºº

æœ¬ç« ä¸­ï¼Œå°†æ•´åˆå‰æ–‡æ‰€æœ‰çš„å†…å®¹ï¼Œåˆ›å»ºä¸€ä¸ªèŠå¤©æœºå™¨äººã€‚

## ä»‹ç»conversational AI

ä¸‹åˆ—æ˜¯ä¸€äº›éå¸¸è‘—åçš„ä¸€äº›è™šæ‹ŸåŠ©ç†AIã€‚

- Amazon Alexa 
- AllGenie from Alibaba Group
- Bixby from Samsung
- Celia from Huawei
- Duer from Baidu
- Google Assistant
- Microsoft Cortana
- Siri from Apple
- Xiaowei from Tencent

### conversational AIçš„ç»„æˆ

- Speech-to-text component

- Conversational NLU component

  æ‰§è¡Œintent recognitionå’Œentity extraction

- Dialog manager

  å¯¹è¯ç®¡ç†å™¨ä¸­æœ‰ dialog memoryï¼Œå®ƒä¿å­˜äº†dialog state.

- Answer generator

- Text-to-speech

æœ¬ç« å°†é›†ä¸­è®¨è®ºConversational NLU componentã€‚

### ç†è§£æ•°æ®é›†

æˆ‘ä»¬å°†ä½¿ç”¨æ•°æ®é›†[Schema-Guided Dialogue dataset (SGD) ](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue)ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…å«20000ä¸ªä»»åŠ¡å¯¼å‘çš„å¯¹è¯ã€‚è¿™äº›å¯¹è¯è¦†ç›–äº†20ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬é“¶è¡Œï¼Œåª’ä½“æ—¶é—´ï¼Œæ—¥ç¨‹å®‰æ’ï¼Œæ—…è¡Œå’Œå¤©æ°”ã€‚

æœ¬ç« ä¸­ï¼Œä½¿ç”¨äº†æˆªå–çš„éƒ¨åˆ†æ•°æ®[resaurants.json](https://github.com/PacktPublishing/Mastering-spaCy/blob/
main/Chapter10/data/restaurants.json).ã€‚

![image-20211002114604635](images/image-20211002114604635.png)

## å®ä½“æŠ½å–ï¼ˆEntity extractionï¼‰

### æŠ½å–æ—¥æœŸå’Œæ—¶é—´

ä¸‹é¢æ˜¯å¯¹äºæ—¥æœŸå’Œæ—¶é—´çš„è¯†åˆ«ã€‚

~~~python
import spacy
nlp = spacy.load("en_core_web_md")
sentences = [
   "I will be eating there at 11:30 am so make it for then.",
   "I'll reach there at 1:30 pm.",
   "No, change it on next friday",
   "Sure. Please confirm that the date is now next Friday and for 1 person.",
   "I need to make it on Monday next week at half past 12 in the afternoon.",
   "A quarter past 5 in the evening, please."
]

for sent in sentences:
   doc = nlp(sent)
   ents = doc.ents
   print([(ent.text, ent.label_) for ent in ents])
~~~

![image-20211002121537155](images/image-20211002121537155.png)

> éœ€è¦æç¤ºçš„æ˜¯ï¼Œå¯¹äºç¬¬äº”è¡Œï¼Œæ•°ä¸­ç¤ºä¾‹æŠŠ`half past 12`è¯†åˆ«æˆäº†`DATE`ï¼Œè€Œä¸Šé¢è¯†åˆ«æˆ`CARDINAL`.

ä¸Šé¢çš„è¯†åˆ«æ•ˆæœå¥½åƒä¸é”™ï¼Œå†çœ‹çœ‹ä¸‹é¢çš„ä¾‹å­ã€‚

~~~python
sentences = [
   "Have a great day.",
   "Have a nice day.",
   "Have a good day",
   "Have a wonderful day.",
   "Have a sunny and nice day"
]
for sent in sentences:
   doc = nlp(sent)
   ents = doc.ents
   print([(ent.text, ent.label_) for ent in ents])
~~~

![image-20211002122019337](images/image-20211002122019337.png)

ä¸Šé¢æ˜¯æ˜æ˜¾çš„é”™è¯¯ï¼Œè¿™æ˜¯ç¬¦åˆdeterminer adjective dayæ¨¡å¼çš„ï¼Œåº”è¯¥å¯ä»¥æ‰¾ä¸€äº›åŠæ³•æ¥å»é™¤ã€‚

### æŠ½å–ç”µè¯å·ç 



