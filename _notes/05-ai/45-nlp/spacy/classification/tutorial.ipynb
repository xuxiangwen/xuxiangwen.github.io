{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d5d157",
   "metadata": {},
   "source": [
    "来自[Building a Text Classifier with Spacy 3.0](https://medium.com/analytics-vidhya/building-a-text-classifier-with-spacy-3-0-dd16e9979a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d235e658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T07:12:49.264845Z",
     "start_time": "2021-08-29T07:12:49.248372Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# tqdm is a great progress bar for python\n",
    "# tqdm.auto automatically selects a text based progress \n",
    "# for the console \n",
    "# and html based output in jupyter notebooks\n",
    "from tqdm.auto import tqdm\n",
    "# DocBin is spacys new way to store Docs in a \n",
    "# binary format for training later\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# !pip install ml-datasets\n",
    "# 公开的用于机器学习、深度学习的各种数据集的集合。\n",
    "import ml_datasets\n",
    "from ml_datasets import imdb\n",
    "\n",
    "# ebablbe auto-completion\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728b5e1",
   "metadata": {},
   "source": [
    "[ml_datasets](https://pypi.org/project/ml-datasets/)中有如下数据集\n",
    "\n",
    "| More ActionsID / Function | Description                                  | NLP task                                  | From URL |\n",
    "| :------------------------ | :------------------------------------------- | :---------------------------------------- | :------: |\n",
    "| `imdb`                    | IMDB sentiment dataset                       | Binary classification: sentiment analysis |    ✓     |\n",
    "| `dbpedia`                 | DBPedia ontology dataset                     | Multi-class single-label classification   |    ✓     |\n",
    "| `cmu`                     | CMU movie genres dataset                     | Multi-class, multi-label classification   |    ✓     |\n",
    "| `quora_questions`         | Duplicate Quora questions dataset            | Detecting duplicate questions             |    ✓     |\n",
    "| `reuters`                 | Reuters dataset (texts not included)         | Multi-class multi-label classification    |    ✓     |\n",
    "| `snli`                    | Stanford Natural Language Inference corpus   | Recognizing textual entailment            |    ✓     |\n",
    "| `stack_exchange`          | Stack Exchange dataset                       | Question Answering                        |    ✓      |\n",
    "| `ud_ancora_pos_tags`      | Universal Dependencies Spanish AnCora corpus | POS tagging                               |    ✓     |\n",
    "| `ud_ewtb_pos_tags`        | Universal Dependencies English EWT corpus    | POS tagging                               |    ✓     |\n",
    "| `wikiner`                 | WikiNER data                                 | Named entity recognition                  |    ✓     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5e6e1db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T07:12:56.373188Z",
     "start_time": "2021-08-29T07:12:56.368486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_registry',\n",
       " 'cmu',\n",
       " 'dbpedia',\n",
       " 'imdb',\n",
       " 'loaders',\n",
       " 'mnist',\n",
       " 'quora_questions',\n",
       " 'reuters',\n",
       " 'snli',\n",
       " 'stack_exchange',\n",
       " 'ud_ancora_pos_tags',\n",
       " 'ud_ewtb_pos_tags',\n",
       " 'util']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ml_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3869b08d",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d283803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T07:17:41.148883Z",
     "start_time": "2021-08-29T07:17:37.563481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 <class 'list'>\n",
      "25000 <class 'list'>\n",
      "('- A film crew is shooting a horror movie in an old, supposedly cursed house where over the years, seven people have mysteriously died. One of the crew finds an old book of spells and it looks like it would be perfect to use in some of the ritual scenes in their movie. It is reasoned that the spells in the book are better written than the script they are using. But as the book is read, the graveyard outside suddenly comes to life. Now the cast and crew are faced with real danger .\\n\\n\\n\\n- IMDb lists a running time of 90 minutes. For the first 60 of those minutes, nothing happens. Far too much time is spent on the movie within a movie. Are we supposed to be frightened by the horror movie that they are shooting? We already know that their movie isn\\'t \"real\". These scares just don\\'t work.\\n\\n\\n\\n- There are very few things to enjoy about The House of Seven Corpses. The acting is atrocious. Most of these \"actors\" would have trouble making a elementary school play. The score is terrible. It is very reminiscent of a 70s television series and provides no atmosphere. Speaking of atmosphere, other than a few moments at the end of the movie, there is none to speak of. Character logic is all but non-existent. Even in a movie, you expect characters to behave in a certain way. Here, I don\\'t think I remember one scene where a character didn\\'t choose the most illogical avenue available to them. And finally, there\\'s those first 60 minutes of the movie that I\\'ve already mentioned. Can you say BORING? \\n\\n\\n\\n- I haven\\'t rated The House of Seven Corpses any lower because of instances where the movie (probably by accident) actually works. My two favorite are the beginning and ending. The opening title sequence presents the deaths of the seven previous owners and may be the highlight of the movie. And, the ending scenes on the massive staircase as the zombie menaces the film crew are somewhat effective (what a ringing endorsement). Overall though, these moments aren\\'t enough to make this a good movie.', 'neg')\n"
     ]
    }
   ],
   "source": [
    "# We want to classify movie reviews as positive or negative\n",
    "# http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# load movie reviews as a tuple (text, label)\n",
    "train_data, valid_data = imdb()\n",
    "\n",
    "print(len(train_data), type(train_data))\n",
    "print(len(valid_data), type(valid_data))\n",
    "\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e458b462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T07:19:26.388631Z",
     "start_time": "2021-08-29T07:19:25.123369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "doc.cats = {}\n",
      "doc.ents = (english,)\n"
     ]
    }
   ],
   "source": [
    "# load a medium sized english language model in spacy\n",
    "# ！python -m spacy download en_core_web_md\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp('load a medium sized english language model in spacy')\n",
    "print(f'doc.cats = {doc.cats}')\n",
    "print(f'doc.ents = {doc.ents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e5e5176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T07:53:50.410481Z",
     "start_time": "2021-08-29T07:24:34.989820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a8b4631fa34e26892931856beee438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070a957e91874680964c25dda597e2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_docs(data):\n",
    "    \"\"\"\n",
    "    this will take a list of texts and labels \n",
    "    and transform them in spacy documents\n",
    "    \n",
    "    data: list(tuple(text, label))\n",
    "    \n",
    "    returns: List(spacy.Doc.doc)\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = []\n",
    "    # nlp.pipe([texts]) is way faster than running \n",
    "    # nlp(text) for each text\n",
    "    # as_tuples allows us to pass in a tuple, \n",
    "    # the first one is treated as text\n",
    "    # the second one will get returned as it is.\n",
    "    \n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        \n",
    "        # we need to set the (text)cat(egory) for each document\n",
    "        doc.cats[\"positive\"] = label\n",
    "        \n",
    "        # put them into a nice list\n",
    "        docs.append(doc)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# we are so far only interested in the first 5000 reviews\n",
    "# this will keep the training time short.\n",
    "# In practice take as much data as you can get.\n",
    "# you can always reduce it to make the script even faster.\n",
    "num_texts = len(train_data)\n",
    "# first we need to transform all the training data\n",
    "train_docs = make_docs(train_data[:num_texts])\n",
    "# then we save it in a binary file to disc\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./data/train.spacy\")\n",
    "\n",
    "num_texts = len(valid_data)\n",
    "# repeat for validation data\n",
    "valid_docs = make_docs(valid_data[:num_texts])\n",
    "doc_bin = DocBin(docs=valid_docs)\n",
    "doc_bin.to_disk(\"./data/valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d10798c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T08:59:45.212989Z",
     "start_time": "2021-08-29T08:59:45.202104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len([label for text, label in train_data if label=='neg']))\n",
    "print(len([label for text, label in train_data if label=='pos'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c916a",
   "metadata": {},
   "source": [
    "## 生成配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b6e92f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T08:51:29.452206Z",
     "start_time": "2021-08-29T08:51:23.770363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-29 08:51:25.712934: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "#!python -m spacy init fill-config ./config/base_config.cfg ./config/config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0bee9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T08:52:15.564896Z",
     "start_time": "2021-08-29T08:52:15.561522Z"
    }
   },
   "source": [
    "## 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2a9b7d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T08:57:07.498049Z",
     "start_time": "2021-08-29T08:54:20.195956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-29 08:54:22.230562: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-08-29 08:54:25,339] [INFO] Set up nlp object from config\n",
      "[2021-08-29 08:54:25,355] [INFO] Pipeline: ['transformer', 'textcat']\n",
      "[2021-08-29 08:54:25,362] [INFO] Created vocabulary\n",
      "[2021-08-29 08:54:25,363] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/cli/_util.py\", line 69, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/typer/main.py\", line 497, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/cli/train.py\", line 56, in train_cli\n",
      "    nlp = init_nlp(config, use_gpu=use_gpu)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/training/initialize.py\", line 82, in init_nlp\n",
      "    nlp.initialize(lambda: train_corpus(nlp), sgd=optimizer)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/language.py\", line 1273, in initialize\n",
      "    proc.initialize(get_examples, nlp=self, **p_settings)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/pipeline/textcat.py\", line 340, in initialize\n",
      "    raise ValueError(Errors.E867)\n",
      "ValueError: [E867] The 'textcat' component requires at least two labels because it uses mutually exclusive classes where exactly one label is True for each doc. For binary classification tasks, you can use two labels with 'textcat' (LABEL / NOT_LABEL) or alternatively, you can use the 'textcat_multilabel' component with one label.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./config/config.cfg --output ./output --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c7cfe52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-29T09:07:05.510731Z",
     "start_time": "2021-08-29T09:04:18.879635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-29 09:04:20.809378: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2021-08-29 09:04:23,898] [INFO] Set up nlp object from config\n",
      "[2021-08-29 09:04:23,915] [INFO] Pipeline: ['transformer', 'textcat']\n",
      "[2021-08-29 09:04:23,921] [INFO] Created vocabulary\n",
      "[2021-08-29 09:04:23,922] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/cli/_util.py\", line 69, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/typer/main.py\", line 497, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/cli/train.py\", line 56, in train_cli\n",
      "    nlp = init_nlp(config, use_gpu=use_gpu)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/training/initialize.py\", line 82, in init_nlp\n",
      "    nlp.initialize(lambda: train_corpus(nlp), sgd=optimizer)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/language.py\", line 1273, in initialize\n",
      "    proc.initialize(get_examples, nlp=self, **p_settings)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/spacy/pipeline/textcat.py\", line 340, in initialize\n",
      "    raise ValueError(Errors.E867)\n",
      "ValueError: [E867] The 'textcat' component requires at least two labels because it uses mutually exclusive classes where exactly one label is True for each doc. For binary classification tasks, you can use two labels with 'textcat' (LABEL / NOT_LABEL) or alternatively, you can use the 'textcat_multilabel' component with one label.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train ./config/config.cfg --output ./output --gpu-id 0 --paths.train ./data/train.spacy --paths.dev ./data/valid.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff787c",
   "metadata": {},
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy evaluate output/gpu_aug/model-best ./data/test_aug.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# load thebest model from training\n",
    "nlp = spacy.load(\"output/model-best\")\n",
    "text = \"\"\n",
    "print(\"type : ‘quit’ to exit\")\n",
    "\n",
    "# # predict the sentiment until someone writes quit\n",
    "# while text != \"quit\":\n",
    "#     text = input(\"Please enter example input: \")\n",
    "#     doc = nlp(text)\n",
    "#     if doc.cats['positive'] >.5:\n",
    "#         print(f\"the sentiment is positive\")\n",
    "#     else:\n",
    "#         print(f\"the sentiment is negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
