# 第一部分 表格解决方法

## 5 蒙特卡洛方法

Monte Carlo Methods

在本章中，我们首次介绍一种学习方法，它用于估计价值函数和发现最优策略。与上一章不同，这里我们不假设对环境有完全的了解。蒙特卡洛方法需要的仅仅是经验——与环境进行真实或模拟交互所得到的状态，动作，奖励的样本序列。从 *真实* 的经验中学习是非常吸引人的，因为不需要环境动态的先验知识，它仍然能够选择最佳行为。而从*模拟* 的经验中学习也是很强大的。虽然需要一个模型，但是模型仅仅用来生成样本转换（ sample transitions），而不是动态规划（DP）所需的所有可能转换的完整概率分布。令人惊讶的是，根据所期望的概率分布生成经验样本很容易，但是要获取分布的具体形式却很难。

蒙特卡洛方法是基于对平均样本回报来解决强化学习的问题的。为了保证能够得到良好定义的回报，这里我们定义蒙特卡洛方法仅适用于 episode 任务。为了确保有明确定义的回报可用，我们只针对 episode 任务定义蒙特卡洛方法。也就是说，我们把经历划分为 episode ，而且无论选择什么行为，最终所有的 episode 都会终止。只有当 episode 结束以后，价值估计和策略才会改变。因此，蒙特卡洛方法可以在 episode-by-episode 之间增量，但不能在逐步（在线）（step-by-step (online) ）的意义上增量。术语 “蒙特卡洛（Monte Carlo）” 被广泛的用于任何的在操作中引入了随机成分的估计方法。我们在这里特指基于平均完整回报的方法（区别于那些从部分回报中学习的方法，我们将在下一章讨论）。

蒙特卡洛方法对每个状态-动作对进行采样和平均回报，就像我们在第2章中探索的 bandit 方法对每个动作进行采样和平均奖励一样。主要区别是，现在有多个状态，每个状态都像一个不同的 bandit 问题（像关联搜索或情境bandit），而且不同的 bandit 问题是相互关联的。也就是说，在一个状态下采取一个行为后得到的回报取决于同一 episode 中后续状态下采取的行为。因为所有的行为选择都在进行学习，所以从早期状态的角度来看，这是一个非平稳的（nonstationary）问题。

为了处理非平稳性，我们采用第4章 DP方法中的广义策略迭代（GPI）思想。在那一章，我们根据MDP的知识计算价值函数，而在本章，我们从MDP的样本回报中学习价值函数。价值函数和相应的策略仍然以基本相同的（GPI）方式进行相互，从而达到最优。与 DP 章节一样，首先我们考虑预测问题（对任意固定的策略 $\pi$ 计算  $v_\pi$ 和 $q_\pi$），然后进行策略提升，最后由 GPI 进行控制并求解。从DP中得到的每一个想法都扩展到了蒙特卡洛方法中，只是它仅仅只有样本经验可用的。

### 5.1 蒙特卡洛预测

Monte Carlo Prediction

我们首先考虑用蒙特卡洛方法来学习给定策略下的状态价值函数。 回顾一下，一个状态的价值是从该状态往后的期望回报——即期望的累积未来折扣奖励。一个显而易见从经验中估计价值的方法是：简单地对访问该状态后观察到的回报求平均。随着观察到的回报越来越多，平均值应该收敛到期望值。这个想法是所有蒙特卡洛方法的基础。

特别地，假设我们想要估计 $v_\pi(s)$，即在策略 $\pi$ 下状态 $s$ 的价值，给定一组通过遵循 $\pi$ 并经过 s 获得的 episode 。 episode 中每次出现状态 $s$ 的情况都称为访问 $s$。当然，同一个 episode 中 $s$ 可能被访问多次。我们称 episode 中第一次访问 s 的时间为*首次访问（first visit）* 。有两种蒙特卡洛方法：

- *首次访问 MC 方法（ first-visit MC method）*将 $v_\pi(s) $ 估计为在第一次访问 $s$ 之后的回报的平均值。
- *每次访问 MC 方法（ every-visit MC method）*则将在所有访问 $s$ 之后的回报的平均值。

这两种方法非常的相似，但具有略微不同的理论性质。第一种方法是得到最广泛研究的，可以追溯到 20 世纪 40 年代，也是我们在本章重点关注的。第二种方法更自然地扩展到函数逼近（function approximation）和资格迹（eligibility traces），在第 9 章和第 12 章中讨论。首次访问 MC 方法如下框所示。而每次访问 MC 除了不检查 $s_t$ 是否在 episode 中早期出现过之外，其他都相同。

> **首次访问 MC 预测，估计 $V \approx v_\pi$,**
> $$
> \begin{flalign}
> &\text {输入（Input）：用来评估的策略} \pi   \\
> &\text {初始化（Initialize）：}   \\
> & \quad V(s)\in\mathbb{R} \\
> & \quad   Returns(s) \leftarrow \text {一个空的列表（List）}    \\
> &  \text {Loop forever (for each episode): }   \\
> &  \quad  \text {根据 } \pi \text { 生成情节：}  S_0, A_0, R_1, S_1, A_1, R_2, \dots , S_{T-1}, A_{T-1}, R_{T}  \\
> &   \quad G \leftarrow 0    \\
> &   \quad \text {Loop for each step of episode, }  t = T-1, T-2, \dots, 0 :  \\
> &   \quad \quad G \leftarrow \gamma{G} + R_{t+1}  \\
> &   \quad \quad \text {Unless } S_t \text { appears in }S_0, S_1,...,S_{t-1}:   \\
> &  \quad \quad  \quad \text {Append } G \text { to } Returns(S_t)    \\
> &   \quad \quad  \quad V(s) \leftarrow average(Returns(s))   \\
> \end{flalign}
> $$

不论是首次访问 MC 方法还是每次访问 MC 方法，都会随着访问 （或首次访问）$s$ 的次数趋于无穷而收敛于 $v_\pi(s)$。对于首次访问MC方法，这是显而易见的。这种情况下，每个回报都是对于 $$v_\pi(s)$$ 的有限方差的独立同分布估计。根据大数定理，这些估计的平均数会收敛于期望价值。每个平均值本身就是一个无偏估计，其误差的标准差降为 $1 / \sqrt{n}$，其中 n 是回报的次数。每次访问 MC 方法没有那么直观，不过也会以二次方的速度收敛于 $v_\pi(s)$ （Singh and Sutton, 1996）。

蒙特卡洛方法的使用最好通过一个例子来说明。

**例5.1：21点（Blackjack）** *21点* 是一种流行的赌场纸牌游戏，其目标是是获得牌的数字值之和尽可能大，但不超过21。 所有的J，Q，K记为10点，A可以算作1或11。我们考虑每个玩家独立地与庄家竞争的版本。具体规则如下：

1. 游戏开始时，庄家和玩家各发两张牌。庄家的其中一张牌是正面朝上的（明牌），另一张是正面朝下的。

2. 如果玩家起手就是 21 点（一张A和一张10点牌），则称为 *natural*。

     - 庄家也是 *natural* ，则称为 *draw*，也就是平局。

     - 庄家不是 *natural* ，则玩家赢，游戏结束。

3. 玩家不是21点，可以有三个行为：

     - hit：可以继续要一张牌。

       - 超过21点（goes bust）：玩家输，游戏结束。
       - 没有超过 21 点，回到第 3 步。
     
     -  stick：停止要牌。进入庄家轮次，即第 4 步。

4. 庄家按照如下固定策略来打牌

   - 当点数大于或等于 17 点，停止要牌（stick）。
     - 比较玩家和庄家点数。
       - 玩家大：玩家赢。
       - 庄家大：庄家赢。
       - 相同：平局（draw）
   - 当点数小于 17 点，要牌（hit）
     - 超过21点（goes bust）：玩家赢，游戏结束。
     - 没有超过 21点，回到第 4 步。

很自然，21 点游戏可看成是一个 episode 有限 MDP 问题。每一局都是一个 episode 。赢得比赛，输掉比赛和平局分别获得+1、-1和0的奖励。游戏过程中的奖励为0，且没有衰减（即 $\gamma=1$）。玩家的行为只有要牌或者停止要牌两种。游戏的状态取决于玩家是什么牌以及庄家手上的明牌。 假设牌是从一个无限的牌堆中发出的（即，有替换），因此跟踪已经发出的牌没有什么用。如果玩家拥有一张 A 且当做11点来算还不超过21点的话，则称该 A 牌为 *可用（usable）*。在这种情况下，它总是被计为11，因为将其计为1会使总和小于或等于11，所以此时，显然玩家的选择是要牌。因此，玩家做出的决定依赖于三个变量：当前的牌的点数和（12-21）、庄家的明牌的点数（A-10）以及玩家是否有可用的 A。 这样的话，总共有200个不同的状态。

考虑这样的策略：一直要牌，直到点数和等于20或21时停止。为了通过蒙特卡洛方法来找到这个策略的状态价值函数，我们可以使用该策略模拟玩许多次游戏，然后计算每个状态后的平均回报。通过这种方法求得的价值函数如图5.1所示。 可以看到： 有可用 A 的状态的估计值不太确定且不太规则，但无论是否有可用 A，在500,000局游戏之后，价值函数都被近似的很好。

![图5.1：遵循一直要牌直到点数和等于20或21的策略，使用蒙特卡洛策略估计求得的估计的状态价值函数。](images/figure-5.1.png)
$$
\text {图5.1：遵循上文的策略，使用蒙特卡洛策略评估近似计算状态价值函数。}
$$
在这二十一点任务中，虽然我们对环境有完全的了解，但是仍然难以用 DP 方法来计算价值函数。 DP 方法需要下个事件的分布，特别是它需要由四个参数函数p给出的环境动态，而对于二十一点来说，这并不容易确定。例如，假设玩家的点数为14，其玩家选择停止要牌。获得+1的奖励的概率是庄展示的牌的一个函数。在运用 DP 之前，必须计算所有的概率，这样的计算通常是复杂和容易出错的。相比之下，使用蒙特卡洛方法仅仅只需要产生样本就好了，这要简单许多。 这种情况经常令人惊讶，蒙特卡洛方法仅使用样本 episode 的能力是一个重要的优势，即使在完全了解环境动态的情况下也是如此。

我们能否将 Backup 图的思想推广到蒙特卡洛算法中？Backup 图（下面右图）的一般思想是在顶部显示要更新的根节点，并在下面显示所有转换和叶节点，它们奖励和估计价值用于更新根节点的价值。对于 $v_\pi$ 的蒙特卡洛估计来说，根节点是一个状态节点，在它下面是沿着特定 episode 的转换轨迹，并以终止状态结束，如下面左图所示。DP 算法和蒙特卡洛方法之间的基本差异是：

- DP 图显示了所有可能的转换，蒙特卡洛图仅显示了在一个episode 中采样的转换。
- DP 图仅包括一步转换，而蒙特卡洛图一直延伸到 episode 的结束。这些图中的差异准确地反映了算法之间的基本差异。

![image-20230318161535894](images/image-20230318161535894.png)

蒙特卡洛方法的一个重要事实是，每个状态的估计值是独立的。一个状态的估计值不建立在任何其他状态的估计值之上，就像DP中的情况一样。换句话说，蒙特卡洛方法不会进行 bootstrap（在上一章定义的）。

特别要注意的是，单个状态价值计算的开销与状态数无关。 当我们只需要一个或者一小部分状态价值时，蒙特卡洛方法特别有吸引力。可以从感兴趣的状态开始生成许多样本 episode，仅计算这些状态的平均回报，而忽略所有其他的状态。 这是蒙特卡洛方法相对 DP方法的第三个优势。

**例5.2：肥皂泡（Soap Bubble）** 

![../../_images/bubble.png](images/bubble.png)
$$
\begin{align}
\text {线圈上的肥皂泡, 来自 Hersh 和 Griego（1969）} \\ 
\end{align}
$$
假设一根线围成一个闭环线框，在肥皂水中浸泡后，表面形成了一个肥皂薄膜或者泡泡。 如果线框是已知的不规则形状，如何计算肥皂泡表面的形状呢？泡泡形状具有这样的特性，即相邻点施加的每个点的总力为零（否则形状将发生变化）。 这意味着表面在任何点的高度都是其周围小圆中的点的高度的平均值。此外，泡泡表面的边界和线框一致。解决这个问题的常规办法是，用网格分格这个区域，并通过迭代计算求解每个网格点的高度。边界上的点的高度和线框一致，其他所有网格点则根据其四个最近邻居的平均高度调整。 这个过程不断的迭代，就像DP的迭代策略评估一样，最终收敛到期望泡泡表面的近似值。

这类问题与最初设计蒙特卡罗方法所涉及的问题类似。 与上述迭代计算不同，想象站在泡泡表面上并进行随机游走，从网格点等概率随机向相邻网格点移动，直到到达边界。结果是，这些边界点的高度求得的期望值即是我们随机漫步起始点的高度（事实上，它恰好等于之前的迭代方法计算得到的值）。 因此，我们能够很好地得到表面上任意一点的高度值。只需要从该点开始，进行许多次随机漫步，然后将所有得到的边界高度值求平均。 如果我们仅仅对某一点或者某一小块区域的高度感兴趣，这个蒙特卡洛（MC）方法要比之前的迭代方法高效的多。

> 上面这一段还不是很理解。原文如下：
>
> This is similar to the kind of problem for which Monte Carlo methods were originally designed. Instead of the iterative computation described above, imagine standing on the surface and taking a random walk, stepping randomly from grid point to neighboring grid point, with equal probability, until you reach the boundary. It turns out that the expected value of the height at the boundary is a close approximation to the height of the desired surface at the starting point (in fact, it is exactly the value computed by the iterative method described above). Thus, one can closely approximate the height of the surface at a point by simply averaging the boundary heights of many walks started at the point. If one is interested in only the value at one point, or any fixed small set of points, then this Monte Carlo method can be far more efficient than the iterative method based on local consistency.

#### 练习5.1-5.2

- 练习5.1 参见图5.1右侧的图。为什么估计的价值函数在尾部的最后两行会突然变大？ 为什么最左边的整个一行价值会有下降？对于最前面的（frontmost）价值，为什么上面图中的值要大于下面的图？

  答：分别答案如下。

  - 两个原因：
    - 玩家点数20点或者21点时，赢面非常大。
    - 策略是当点数小于20，还会要牌，这样就有超过21点的可能，尤其是点数是18-19时。
  - 因为庄家的明牌是 A 的时候，有较大概率可以赢，而且超过 21 点概率变小。
  - 当玩家有可用 A 时，减少了超过 21 点概率，赢面较大。

- 练习5.2 假设在 21 点任务中使用每次访问 MC 方法而不是首次访问 MC 方法。结果会有很大差异吗？为什么？

  答：差异不大。分两种情况分析：

  - 没有可用 A

    每一次要牌后，点数总是增加，所以之前的状态无法在后面重现。

  - 有可用 A

    每一次要牌后，

    - 如果还是处于有可用 A的状态，总的点数是增加地，所以之前的状态无法在后面重现。如果
    - 如果处于无可用 A的状态，之前的（有可用 A）状态也无法在后面重现。

### 5.2 行为价值的蒙特卡洛估计

Monte Carlo Estimation of Action Values

如果模型不可用，那么估计 *行为* 价值（即状态-行为对的价值）比估计 *状态* 价值更有用。 如果模型可用，状态价值就足以确定策略；决定下一步只需要看哪个行为导致的奖励和下一状态组合最佳，就像我们在DP章节中所做的那样。然而，如果没有模型，仅有状态价值是不够的。而如果模型不可用，仅使用状态价值是不够的。 我们必须清楚地估计每个行为的价值，以使它们在建议策略时有用。 所以，蒙特卡洛方法主要用来估计 $q_*$。

我们首先考虑行为价值的策略评估问题，即估计 $q_\pi{(s,a)}$。本节的蒙特卡洛的方法与上节基本相同，不同是本节谈论的是状态-行为对，而上节是状态。 在每次访问MC方法中，每次访问状态-行为对都会计算，最后求平均； 而首次访问 MC 方法每个 episode 只计算最多一次。当访问次数趋近于无穷时，这两种方法（指每次访问 MC 和首次访问 MC）都会以二次方收敛到期望值。

唯一的问题是，可能会有许多状态-行为对从未被访问到。如果 $\pi$ 是一个确定性的策略， 那么遵循策略 $\pi$，每个状态将会仅仅观察到一个行为的回报。 对于未被访问的行为，没有回报，所以它们的蒙特卡洛的估计就不能随着经验的增加而提高。 这是一个严重的问题，因为我们学习行为价值，就是为了在每个状态选择合适的行为。 为了比较所有的可能，我们需要估计每个状态 *所有* 行为的价值，而不仅仅是当前行为。

这是一个很普遍的问题，即 *保持探索（maintaining exploration）*，我们在第二章 k-armed bandit 问题中提到过。 要使策略评估能够工作，我们必须保证持续的探索。一个方法是， 从一个的*状态-行为对* 开始 ，每种行为都有大于零的概率被选择到。 这能够保证经历无限个 episode 后，所有的状态-行为对都会被访问到无限次。我们称这种假设为 *探索开端（exploring starts）*。

这个 exploring starts 的假设有时是很有用的。但是它不具普遍意义，特别是当我们直接从与真实环境的交互中学习时，这种方法就不太适用了。 在这种情况下，起始状态不是很有用。 为了让所有状态-行为对都能访问到的，最常用的替代方法是采用随机策略，即每个状态下，选择任意行为的概率都不为零。 我们将会在后面的小节里讨论这种方法的两个变种。现在，我们保留 *exploring starts* 的假设，完整地介绍蒙特卡罗控制方法。

#### 练习5.3

- 练习 5.3 画出蒙特卡洛估计 $q_\pi$  的 backup 图？
  答：如下图所示。

  ![image-20230319211517772](images/image-20230319211517772.png)

### 5.3 蒙特卡洛控制

Monte Carlo Control

现在，我们开始考虑蒙特卡洛估计来解决控制问题，即求解近似最优的策略。 整个的过程和上一章动态规划的模式相同，我们依照广义策略迭代（GPI）的思想。 广义策略迭代（GPI）中，我们同时维持一个近似的策略和一个近似的价值函数。价值函数会被反复修改，以更接近当前策略的价值函数，而策略也会根据当前价值函数不断改进，如下的图所示。这两种变化在一定程度上相互作用，每种变化都为另一种变化指明了移动的方向，但是总的来说，它们共同使得策略和价值函数都趋向于最优。

<img src="images/GPI_chp5.3.png" alt="../../_images/GPI_chp5.3.png" style="zoom:67%;" />

首先，我们考虑经典的策略迭代的蒙特卡洛（MC）版本。我们交替执行策略迭代和策略提升的完整步骤。 从一个随机的策略 $\pi_0$ 开始，以最优策略和最优的动作-价值函数结束：
$$
\pi_0 \overset{E}{\rightarrow} q_{\pi_0} \overset{I}{\rightarrow} \pi_1 \overset{E}{\rightarrow} q_{\pi_1} \overset{I}{\rightarrow} \pi_2 \overset{E}{\rightarrow} \cdots \overset{I}{\rightarrow} \pi_{*} \overset{E}{\rightarrow} q_{*}
$$
其中，$\overset{E}{\rightarrow}$ 表示一个完整的策略评估，$\overset{I}{\rightarrow}$ 表示一个完整的策略提升。随着经历越来越多的 episode，近似的行为-价值函数逐渐趋近于真实的函数。 此时，我们假设观察到了无限的 episode，而且这些 episode 都是以 exploring starts 的方式生成的。 在上述假设下，对应于任意 $\pi_k$，蒙特卡洛方法会精确地计算每个 $q_{\pi_k}$。

策略提升的方法是，对于当前的价值函数，使用 Greedy 方式来选择行为。而对于蒙特卡洛方法，因为我们有行为-价值函数，所以不需要模型来构建 Greedy 策略。 对于任何的行为-价值函数 $q$，它对应的贪婪策略是：对每个 $s \in\mathcal{S}$， 选择行为-价值函数最大的那个动作：
$$
\pi(s) \dot{=} arg \space \underset{a}{max} \space q(s,a)  \tag {5.1}
$$
之后我们可以做策略提升，使用基于 $q_{\pi_k}$ 的贪婪策略构建每个 $\pi_{k+1}$  。 策略提升理论（见4.2节）可以应用到 $\pi_k$ 和 $ \pi_{k+1}$ 上， 因为对于所有 $s \in\mathcal{S}$，
$$
\begin{split}\begin{aligned}
q_{\pi_{k}}\left(s, \pi_{k+1}(s)\right) &=q_{\pi_{k}}(s, \arg \max _{a} q_{\pi_{k}}(s, a)) \\
&=\max _{a} q_{\pi_{k}}(s, a) \\
& \geq q_{\pi_{k}}\left(s, \pi_{k}(s)\right) \\
& \geq v_{\pi_{k}}(s)
\end{aligned}\end{split}
$$
通过这种方法，可以在不知道环境动态的情况下，仅靠样本 episode，使用蒙特卡洛（MC）方法）来找到最优策略。

为了保证蒙特卡洛方法的收敛性，我们有两个不太可能的假设：

- episode 采用 exploring starts 方式。
- 策略评估需要无限次 episode 

为了得到一个实际可用的算法，我们将不得不去掉这两个假设。 我们将在这一章的稍后部分讨论怎么去掉第一个假设。

现在，我们先考虑第二个假设，即策略评估需要无限次 episode 。这个假设相对容易去掉。事实上，相同的问题曾在上一章的经典 DP 方法中出现过。例如迭代策略评估只会渐进地收敛到真实价值函数。 无论是DP还是MC，有两种方法解决这个问题。

- 让每次策略评估都尽让接近 $q_{\pi_k}$。我们会使用一些方法和一些假设来获得误差的边界和概率，然后经过足够多的步骤后， 策略评估能够保证这些边界足够的小。这种方法可能可以在某种程度上保证正确的收敛，但是在实践中除了最小的问题之外，它也可能需要太多的 episode。
- 在进入到策略提升前，放弃完成策略评估。 在每次评估步骤中，价值函数向 $q_{\pi_k}$ 移动，经过很多步的移动，就能移动到期望的值。在第4.6节中首次介绍 GPI 的想法时，我们介绍了这个想法。它的一个极端形式是价值迭代，在策略提升之间执行一次迭代策略评估。价值迭代的 in-place 版本甚至更加极端；在那里，对单个状态，策略提升和策略评估是交替进行的。

对于蒙特卡洛策略评估而言，按照每个 episode 交替进行评估和提升是很自然的。在每个 episode 之后，观察到的回报用于策略评估，然后对每个经历的状态做策略提升。完整的简化算法如下所示，我们称它为 *探索开端的蒙特卡洛算法* （Monte Carlo ES，即 Monte Carlo with Exploring Starts）。

> **Monte Carlo ES (Exploring Starts)**， 用于估算 $\pi \approx \pi_*$
> $$
> \begin{flalign}
> & \text {Initialize}:
> \\ &  \quad  \pi(s) \in  \mathcal A(s) \text { (arbitrarily), } \text {for all } s \in \mathcal S
> \\ &  \quad Q(s, a) \in  \mathcal  R  \text { (arbitrarily), for all } s \in  \mathcal S, a \in  \mathcal A(s)
> \\ &  \quad Returns(s, a) \leftarrow \text {empty list, for all } s \in  \mathcal S, a \in  \mathcal A(s)
> \\ &   \text {Loop forever (for each episode): }
> \\ &  \quad \text {Choose } S_0 \in  \mathcal S, A_0 \in  \mathcal A(S_0)  \text  { randomly such that all pairs have probability > 0}
> \\ &  \quad \text {Generate an episode from } S_0, A_0, \text {following } \pi: S_0, A_0, R_1,...,S_{T-1}, A_{T-1}, R_T
> \\ &  \quad G \leftarrow 0
> \\ &   \quad\text {Loop for each step of episode, } t = T -1, T -2, \cdots, 0:
> \\ &  \quad \quad  G \leftarrow \gamma G + R_{t+1}
> \\ &  \quad \quad \text {Unless the pair } S_t, A_t \text { appears in }S_0, A_0, S_1, A_1 ...,S_{t-1}, A_{t-1}:
> \\ &  \quad \quad \quad \text {Append } G \text { to }Returns(S_t, A_t)
> \\ &  \quad \quad \quad Q(S_t, A_t) \leftarrow  average(Returns(S_t, A_t))
> \\ &  \quad \quad \quad \pi(S_t) \leftarrow \arg\max_a Q(S_t, a)
> \end{flalign}
> $$

在Monte Carlo ES中，每个状态-动作对的所有回报都被累积并平均，无论观察时使用的是什么策略。很容易看出，Monte Carlo ES无法收敛到任何次优策略。如果这样做，那么价值函数最终将收敛到该策略的价值函数，这反过来会导致策略发生变化。只有当策略和价值函数都是最优的时，才能实现稳定性。随着动作-价值函数的变化随着时间的推移而减少，收敛到这个最优固定点似乎是不可避免的，但尚未得到正式证明。在我们看来，这是强化学习中最基本的开放性理论问题之一（有关部分解决方案，请参见Tsitsiklis，2002）。

**例 5.3：解决 21 点（Blackjack）问题** 将Monte Carlo ES应用于 21 点游戏非常简单。由于所有 episode 都是模拟的游戏，因此很容易获取探索开端（ exploring starts ）的所有可能性。在这种情况下，等概率的随机选取庄家的牌，玩家的牌面和，以及玩家是否有可用的 A。初始策略使用我们之前讨论时使用的，在20或21时停止要牌，其余情况均要牌。初始的各个状态的行为-价值函数均为零。图5.2展示了使用探索开端（ exploring starts）的蒙特卡洛算法得到的最优策略。 这个策略和 Thorp在 1966 提出的”基本”策略是一样的。唯一的例外是可用 A 策略中最左边的凹口，它在Thorp的策略中不存在。我们不确定这种差异的原因，但确信这里显示的确实是我们所说 21 点游戏版本的最优策略。

![图 5.3： 使用探索开端的蒙特卡洛算法（Monte Carlo ES），21点的最优策略和状态-价值函数。状态-价值函数是从算法得到的动作-价值函数计算而来的](images/figure-5.2.png)
$$
\text {图5.2 使用探索开端的蒙特卡洛算法下21点的最优策略和状态-价值函数。}
$$
上图中的状态-价值函数是从行为-价值函数计算而来的。

#### 练习5.4

- 练习5.4 Monte Carlo ES的伪代码效率低下，这是因为对于每个状态-行为对，它维护了所有回报的列表并重复计算它们的平均值。更有效的方式是：使用类似于第2.4节中的方法，仅仅维护平均值和计数（对于每个状态-动作对）并且逐步更新。请描述如何更改伪代码以实现此目的。

  答：
  $$
  \begin{flalign}
  & \text {Initialize}:
  \\ &  \quad  \pi(s) \in  \mathcal A(s) \text { (arbitrarily), } \text {for all } s \in \mathcal S
  \\ &  \quad Q(s, a) \leftarrow 0,  \text { (arbitrarily), for all } s \in  \mathcal S, a \in  \mathcal A(s)
  \\ &  \quad N(s, a) \leftarrow \text {0, for all } s \in  \mathcal S, a \in  \mathcal A(s)
  \\ &   \text {Loop forever (for each episode): }
  \\ &  \quad \text {Choose } S_0 \in  \mathcal S, A_0 \in  \mathcal A(S_0)  \text  { randomly such that all pairs have probability > 0}
  \\ &  \quad \text {Generate an episode from } S_0, A_0, \text {following } \pi: S_0, A_0, R_1,...,S_{T-1}, A_{T-1}, R_T
  \\ &  \quad G \leftarrow 0
  \\ &   \quad\text {Loop for each step of episode, } t = T -1, T -2, \cdots, 0:
  \\ &  \quad \quad  G \leftarrow \gamma G + R_{t+1}
  \\ &  \quad \quad \text {Unless the pair } S_t, A_t \text { appears in }S_0, A_0, S_1, A_1 ...,S_{t-1}, A_{t-1}:
  \\ &  \quad \quad \quad N(s, a) \leftarrow N(s, a) +1
  \\ &  \quad \quad \quad Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \frac 1 {N(s, a)} (G - Q(S_t, A_t))
  \\ &  \quad \quad \quad \pi(S_t) \leftarrow \arg\max_a Q(S_t, a)
  \end{flalign}
  $$

### 5.4 非探索开端的蒙特卡洛控制

Monte Carlo Control without Exploring Starts

如何避免探索开始（exploring starts）这个不太可能的假设呢？确保所有行为都被无限选择的唯一通用方法是让个体继续选择它们。有两种方法可以确保这一点，我们称之为 on-policy 方法和 off-policy 方法。

- on-policy 方法：评估或改进用于做出决策的策略。
- off-policy 方法：评估或改进一个策略，这个策略不同于生成数据所使用的那个策略。

上一节所谈到的蒙特卡洛 ES 方法是一种 on-policy 方法。 在本节里，我们将学习如何设计另一个 on-policy 蒙特卡洛控制方法，它不使用探索开端（exploring starts）这个不太切换实际的假设。 而 off-policy 方法将在下一节讨论。 

在 on-policy 控制方法中，策略通常是*soft*，这意味着对于所有 $s \in \mathcal S$ 和 $a \in \mathcal A(s)$，满足 $\pi(a | s)> 0$，随着时间的推移，策略逐渐向确定性最优策略靠近。第二章中我们讨论的许多方法便是采用这一机制。本节中，我们使用 $\varepsilon \text - greedy$ 方法，即大多数时间选择行为价值最大的那个行为，仅有 $\varepsilon$ 的概率选择随机的动作。也就是说，选择所有 nongreedy 的动作的概率是 $\frac{\varepsilon}{|\mathcal{A}(s)|}$，而选择 greedy 的动作的概率是 $1-\varepsilon+\frac{\varepsilon}{|\mathcal{A}(s)|}$。 $\varepsilon \text - greedy$ 策略是 $\varepsilon \text - soft$ 策略的一个例子，对所有的状态和行为，$\pi(a|s)\geq\frac{\varepsilon}{|A(s)|}$。在所有 $\varepsilon \text - soft$ 策略中， $\varepsilon \text - greedy$ 策略在某种意义上是最接近 greedy 的。

on-policy 蒙特卡洛控制的思想仍然是一种广义策略迭代（GPI）。和蒙特卡洛 ES 方法一样，我们使用first-visit 蒙特卡洛方法来估计当前策略的行为-价值函数。 由于没有探索开端（exploring starts）这个假设，不能简单地对当前价值函数使用 greedy 方法来提升当前策略， 因为这样会阻止对 nongreedy 行为的进一步探索。 幸运的是，广义策略迭代（GPI）并不需要我们的策略一直保持 greedy，只是要求不断向 greedy 策略 *靠近*。 我们的在策略方法会不断的趋向于 $\varepsilon \text - greedy$ 策略。 对任意的 $\varepsilon \text - soft$  策略 $\pi$ 来说， $q_{\pi}$ 对应的任意的  $\varepsilon \text - greedy$ 策略都好于或等于策略 $\pi$。 完整的算法如下。

> **Monte Carlo ES (Exploring Starts)**， 用于估算 $\pi \approx \pi_*$
> $$
> \begin{flalign}
> & \text {Initialize}:
> \\ &  \quad  \pi(s)  \leftarrow \text { an arbitrary } \varepsilon\text -soft \text { policy}
> \\ &  \quad Q(s, a) \in  \mathcal  R  \text { (arbitrarily), for all } s \in  \mathcal S, a \in  \mathcal A(s)
> \\ &  \quad Returns(s, a) \leftarrow \text {empty list, for all } s \in  \mathcal S, a \in  \mathcal A(s)
> \\ &   \text {Loop forever (for each episode): }
> \\ &  \quad \text {Choose } S_0 \in  \mathcal S, A_0 \in  \mathcal A(S_0)  \text  { randomly such that all pairs have probability > 0}
> \\ &  \quad \text {Generate an episode from } S_0, A_0, \text {following } \pi: S_0, A_0, R_1,...,S_{T-1}, A_{T-1}, R_T
> \\ &  \quad G \leftarrow 0
> \\ &   \quad\text {Loop for each step of episode, } t = T -1, T -2, \cdots, 0:
> \\ &  \quad \quad  G \leftarrow \gamma G + R_{t+1}
> \\ &  \quad \quad \text {Unless the pair } S_t, A_t \text { appears in }S_0, A_0, S_1, A_1 ...,S_{t-1}, A_{t-1}:
> \\ &  \quad \quad \quad \text {Append } G \text { to }Returns(S_t, A_t)
> \\ &  \quad \quad \quad Q(S_t, A_t) \leftarrow  average(Returns(S_t, A_t))
> \\ &  \quad \quad \quad A^* \leftarrow \mathop{argmax} \limits_{a} Q(S_t, a)  \quad \quad \quad \quad \text {(with ties broken arbitrarily)}
> \\ &  \quad \quad \quad \text {For all } a \in \mathcal A(S_t):
> \\ &  \quad \quad \quad \quad \begin{split}\pi\left(a|S_{t}\right) \leftarrow\left\{\begin{array}{ll}
> 1-\varepsilon+\varepsilon /\left|\mathcal{A}\left(S_{t}\right)\right| & \text { if } a=A^{*} \\
> \varepsilon /\left|\mathcal{A}\left(S_{t}\right)\right| & \text { if } a \neq A^{*}
> \end{array}\right.\end{split}
> \end{flalign}
> $$

设 $\pi'$ 为 $\varepsilon \text - greedy$ 策略，应用策略提升理论可得如下推导：
$$
\begin{split}\begin{aligned}
q_{\pi}\left(s, \pi^{\prime}(s)\right) &=\sum_{a} \pi^{\prime}(a | s) q_{\pi}(s, a) \\
&=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \max _{a} q_{\pi}(s, a) \\
& \geq \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+(1-\varepsilon) \sum_{a} \frac{\pi(a | s)-\frac{\varepsilon}{|\mathcal{A}(s)|}}{1-\varepsilon} q_{\pi}(s, a) \\


\end{aligned}\end{split} \tag {5.2}
$$
注意，由于 $\sum_{a} \frac{\pi(a | s)-\frac{\varepsilon}{|\mathcal{A}(s)|}}{1-\varepsilon} q_{\pi}(s, a)$是一个加权平均值，所以 $\max _{a} q_{\pi}(s, a)  \geq \sum_{a} \frac{\pi(a | s)-\frac{\varepsilon}{|\mathcal{A}(s)|}}{1-\varepsilon} q_{\pi}(s, a)$。继续推导可得。
$$
\begin{split}\begin{aligned}
\quad\quad\quad\quad\quad\quad\quad    &=\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)-\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a)+\sum_{a} \pi(a | s) q_{\pi}(s, a) \\
&=v_{\pi}(s)
\end{aligned}\end{split}
$$
由此，根据应用策略提升理论，可得 $\pi^{'} \geq \pi$ （即对所有 $s \in \mathcal S，v_{\pi^{'}}(s) \geq v_\pi(s)$）。

考虑一个新环境，除了采用 $\varepsilon \text - soft$ 策略，它与原始环境完全相同。新环境与原始环境具有相同的行为集和状态集。如果在状态 s 中采取行动 a ，则新环境的行为有 $1-\varepsilon$ 的概率与原始环境完全相同，有 $\varepsilon $ 的概率随机选择一个行为。 假设 $\tilde{v}_*$ 和 $\tilde{q}_*$ 表示新环境的最优的价值函数。 则当且仅当 $v_\pi = \tilde{v}_*$，策略 $\pi$ 是 $\varepsilon \text - soft $ 策略中最优的哪一个。 $\tilde{v}_*$ 是贝尔曼方程（3.19）的唯一解。
$$
\begin{split}\begin{aligned}
\widetilde{v}_{*}(s)=&(1-\varepsilon) \max _{a} \widetilde{q}_{*}(s, a)+\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} \widetilde{q}_{*}(s, a) \\
=&(1-\varepsilon) \max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma \widetilde{v}_{*}\left(s^{\prime}\right)\right] \\
&+\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma \widetilde{v}_{*}\left(s^{\prime}\right)\right]
\end{aligned}\end{split}
$$
当 $\varepsilon \text - soft $ 的策略 $\pi$ 不在提升的时候，根据公式（5.2）可得：
$$
\begin{split}\begin{aligned}
v_{\pi}(s)=&(1-\varepsilon) \max _{a} q_{\pi}(s, a)+\frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} q_{\pi}(s, a) \\
=&(1-\varepsilon) \max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right] \\
+\quad & \frac{\varepsilon}{|\mathcal{A}(s)|} \sum_{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]
\end{aligned}\end{split}
$$
这个方程与上面的方程相比，除了把 $$\tilde{v}_*$$ 换成了 $v_\pi$ ，其他的都相同。 由于 $$\tilde{v}_*$$ 是唯一的解，所以必定有 $$v_\pi = \tilde{v}_*$$。

本质上，上面几页已经说明了策略迭代适用于 $\varepsilon \text - soft $ 策略，对 $\varepsilon \text - soft $ 策略使用 greedy 策略，能够保证每一步都有提升， 直到找到最优的策略为止。

### 5.5 通过重要性采样的离策略预测

 Off-policy Prediction via Importance Sampling

所有的学习控制方法都面临着一个困境：它们试图学习在后续的最优行为条件下的行动价值，但是为了探索所有的行动（以找到最优行动），它们需要执行非最优的行为。如何在按照探索性策略行动的同时学习最优策略呢？在前一节中，基于策略（on-policy）的方法实际上是一种妥协——它学习的不是最优策略，而是一个仍然进行探索的近似最优策略。更直接的方法是使用两个策略：

- 进行学习，生成最优策略。称之为*目标策略（target policy）*
- 进行更多的探索，用于生成行为。称之为*行为策略（behavior policy）*

在这种情况下，我们说学习是从目标策略 “off” 数据中进行的，整个过程称为 *off-policy学习*。

我们将在本书后续内容同时探讨 on-policy 和  off-policy 两种方法。由于一般来说 on-policy 方法更简单一些，所以我们先讨论了它。off-policy 方法需要额外的概念和符号，且由于数据归因于不同的策略，off-policy 方法通常具有更大的方差并且收敛速度较慢。而另一方面，off-policy 方法更加强大且适用更广。对于off-policy 方法来说，off-policy 方法可看做其特殊情况，此时其目标和行为策略相同。off-policy 方法在应用程序中也有各种其他用途。例如，它们能够从传统非学习控制器或人类专家生成的数据中学习。 off-policy 学习也被一些人视为学习多步预测环境动态模型的关键。[1](https://www.sciencedirect.com/science/article/pii/S0306261919314424)2。（参见17.2章节; Sutton, 2009; Sutton et al., 2011）。

本节我们开始学习 off-policy 方法。从考虑 *预测* 问题开始，其目标策略和行为策略都是固定的。也就是说，假设我们想要估计 $v_\pi$ 或者 $q_\pi$， 但我们所有的 episode 都遵循另一个策略 b ，且 $b \neq \pi$。 这种情况下，$\pi$ 是目标策略，$b$ 是行为策略，这两种策略都认为是已知且固定的。



