## 5 蒙特卡洛方法

Monte Carlo Methods

在本章中，我们首次介绍一种学习方法，它用于估计价值函数和发现最优策略。与上一章不同，这里我们不假设对环境有完全的了解。蒙特卡罗方法需要的仅仅是经验——与环境进行真实或模拟交互所得到的状态，动作，奖励的样本序列。从 *真实* 的经验中学习是非常吸引人的，因为在不需要关于环境动态的先验知识的情况，它仍然能够选择最佳行为。而从*模拟* 的经验中学习也是很强大的。虽然需要一个模型，但是模型仅仅用来生成样本转换（ sample transitions），而不是动态规划（DP）所需的所有可能转换的完整概率分布。令人惊讶的是，根据所期望的概率分布生成经验样本很容易，但是要获取分布的具体形式却很难。

蒙特卡洛方法是基于对平均样本回报来解决强化学习的问题的。为了保证能够得到良好定义的回报，这里我们定义蒙特卡洛方法仅适用于回合制任务。为了确保有明确定义的回报可用，我们只针对情节性任务定义蒙特卡罗方法。也就是说，我们把经验划分为情节，而且无论选择什么行为，最终所有的情节都会终止。只有当情节结束以后，价值估计和策略才会改变。因此，蒙特卡罗方法可以在情节与情节（episode-by-episode）之间增量，但不能在逐步（在线）（step-by-step (online) ）的意义上增量。术语 “蒙特卡洛（Monte Carlo）” 被广泛的用于任何的在操作中引入了随机成分的估计方法。我们在这里特指基于平均完整回报的方法（区别于那些从部分回报中学习的方法，我们将在下一章讨论）。

蒙特卡罗方法对每个状态-动作对进行采样和平均回报，就像我们在第2章中探索的 bandit 方法对每个动作进行采样和平均奖励一样。主要区别是，现在有多个状态，每个状态都像一个不同的bandit问题（像关联搜索或情境bandit），而且不同的 bandit 问题是相互关联的。也就是说，在一个状态下采取一个行为后得到的回报取决于同一情节中后续状态下采取的行为。因为所有的行为选择都在进行学习，所以从早期状态的角度来看，这是一个非平稳的（nonstationary）问题。

为了处理非平稳性，我们采用第4章 DP方法中的广义策略迭代（GPI）思想。在那一章，我们根据MDP的知识计算价值函数，而在本章，我们从MDP的样本回报中学习价值函数。价值函数和相应的策略仍然以基本相同的（GPI）方式进行相互，从而达到最优。与DP章节一样，首先我们考虑预测问题（对任意固定的策略 $\pi$ 计算  $v_\pi$ 和 $q_\pi$），然后进行策略改进，最后由 GPI 进行控制并求解。从DP中得到的每一个想法都扩展到了蒙特卡罗方法中，只是它仅仅只有样本经验可用的。

### 5.1 蒙特卡洛预测

Monte Carlo Prediction

