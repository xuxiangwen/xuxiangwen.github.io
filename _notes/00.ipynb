{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T00:28:34.734410Z",
     "start_time": "2021-08-22T00:28:34.722949Z"
    }
   },
   "outputs": [],
   "source": [
    "# ebablbe auto-completion\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:26.362000Z",
     "start_time": "2021-07-05T08:45:22.374042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "console.log('Starting front end url_querystring_target comm target');\n",
       "const comm = Jupyter.notebook.kernel.comm_manager.new_comm('url_querystring_target', {'init': 1});\n",
       "comm.send({'ipyparams_browser_url': window.location.href});\n",
       "console.log('Sent window.location.href on url_querystring_target comm target');\n",
       "\n",
       "comm.on_msg(function(msg) {\n",
       "    console.log(msg.content.data);\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-05 08:45:26,327: INFO: deployment = dev\n",
      "2021-07-05 08:45:26,329: INFO: dataset_name = snps_18510_c8\n",
      "2021-07-05 08:45:26,330: INFO: working_path = /tf/eipi10/jian-xu3/snps-classification\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ipyparams\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string  \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from collections import Counter\n",
    "# from google_trans_new import google_translator\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model, models, layers, regularizers, preprocessing, datasets, metrics, losses, optimizers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorboard.plugins.hparams import api as hp \n",
    "\n",
    "base_path = os.path.abspath('/tf/eipi10/jian-xu3/snps-classification')\n",
    "sys.path.append(base_path)\n",
    "\n",
    "import snps \n",
    "\n",
    "from snps.core import text_predictor \n",
    "from snps.core import util \n",
    "from snps.utils import data\n",
    "from snps.core.text_classification import lr_schedule\n",
    "from snps.core.text_classification import plot_distribution, plot_length_distribution, plot_frequency_distribution\n",
    "from snps.core.text_classification import Params, TextClassificationHelper\n",
    "from snps.core.text_classification import SimpleTextDatasets, SequenceTextDatasets, RawTextDatasets,TransferTextDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:26.583909Z",
     "start_time": "2021-07-05T08:45:26.364475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>pid</th>\n",
       "      <th>memory</th>\n",
       "      <th>cpu</th>\n",
       "      <th>num_threads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eipi10/xuxiangwen.github.io/_notes/00.ipynb</td>\n",
       "      <td>47</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          path  pid  memory  cpu  num_threads\n",
       "1  eipi10/xuxiangwen.github.io/_notes/00.ipynb   47   0.377  0.0            8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import posixpath\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import requests\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def show_notebooks_table(host, port):\n",
    "    \"\"\"Show table with info about running jupyter notebooks.\n",
    "\n",
    "    Args:\n",
    "        host: host of the jupyter server.\n",
    "        port: port of the jupyter server.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with rows corresponding to running notebooks and following columns:\n",
    "            * index: notebook kernel id.\n",
    "            * path: path to notebook file.\n",
    "            * pid: pid of the notebook process.\n",
    "            * memory: notebook memory consumption in percentage.\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_colwidth', 300)\n",
    "    \n",
    "    notebooks = get_running_notebooks(host, port)\n",
    "    prefix = long_substr([notebook['path'] for notebook in notebooks])\n",
    "    df = pd.DataFrame(notebooks)\n",
    "    df = df.set_index('kernel_id')\n",
    "    df.index.name = prefix\n",
    "    df.path = df.path.apply(lambda x: x[len(prefix):])\n",
    "    df['pid'] = df.apply(lambda row: get_process_id(row.name), axis=1)\n",
    "    # same notebook can be run in multiple processes\n",
    "    df = expand_column(df, 'pid')\n",
    "    df['memory'] = df.pid.apply(memory_usage_psutil)\n",
    "    df['cpu'] = df.pid.apply(cpu_usage_psutil)\n",
    "    df['num_threads'] = df.pid.apply(num_threads_psutil)    \n",
    "    df = df.sort_values('memory', ascending=False)\n",
    "    df.index = range(1, len(df)+1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_running_notebooks(host, port, password='xxw'):\n",
    "    \"\"\"\n",
    "    Get kernel ids and paths of the running notebooks.\n",
    "\n",
    "    Args:\n",
    "        host: host at which the notebook server is listening. E.g. 'localhost'.\n",
    "        port: port at which the notebook server is listening. E.g. 8888.\n",
    "            \n",
    "    Returns:\n",
    "        list of dicts {kernel_id: notebook kernel id, path: path to notebook file}.\n",
    "    \"\"\"\n",
    "        \n",
    "    BASE_URL = 'http://{0}:{1}/'.format(host, port)\n",
    "        \n",
    "    # Get the cookie data\n",
    "    s = requests.Session()\n",
    "    url = BASE_URL + 'login?next=%2F'\n",
    "    resp = s.get(url)\n",
    "    xsrf_cookie = resp.cookies['_xsrf']\n",
    "\n",
    "    # Login with the password\n",
    "    params = {'_xsrf': xsrf_cookie, 'password': password}\n",
    "    res = s.post(url, data=params)\n",
    "\n",
    "    # Find which kernel corresponds to which notebook\n",
    "    # by querying the notebook server api for sessions\n",
    "    url = posixpath.join(BASE_URL, 'api', 'sessions')\n",
    "    ret = s.get(url)\n",
    "    #print('Status code:', ret.status_code)\n",
    "\n",
    "    # Get the notebook list\n",
    "    res = json.loads(ret.text)\n",
    "    notebooks = [{'kernel_id': notebook['kernel']['id'],\n",
    "                  'path': notebook['notebook']['path']} for notebook in res]\n",
    "    return notebooks\n",
    "\n",
    "\n",
    "def get_process_id(name):\n",
    "    \"\"\"Return process ids found by (partial) name or regex.\n",
    "\n",
    "    Source: https://stackoverflow.com/a/44712205/304209.\n",
    "    >>> get_process_id('kthreadd')\n",
    "    [2]\n",
    "    >>> get_process_id('watchdog')\n",
    "    [10, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56, 61]  # ymmv\n",
    "    >>> get_process_id('non-existent process')\n",
    "    []\n",
    "    \"\"\"\n",
    "    child = subprocess.Popen(['pgrep', '-f', name], stdout=subprocess.PIPE, shell=False)\n",
    "    response = child.communicate()[0]\n",
    "    return [int(pid) for pid in response.split()]\n",
    "\n",
    "\n",
    "def memory_usage_psutil(pid=None):\n",
    "    \"\"\"Get memory usage percentage by current process or by process specified by id, like in top.\n",
    "\n",
    "    Source: https://stackoverflow.com/a/30014612/304209.\n",
    "\n",
    "    Args:\n",
    "        pid: pid of the process to analyze. If None, analyze the current process.\n",
    "\n",
    "    Returns:\n",
    "        memory usage of the process, in percentage like in top, values in [0, 100].\n",
    "    \"\"\"\n",
    "    if pid is None:\n",
    "        pid = os.getpid()\n",
    "    process = psutil.Process(pid)\n",
    "    return round(process.memory_percent(),3)\n",
    "\n",
    "def cpu_usage_psutil(pid=None):\n",
    "    \"\"\"Get cpu usage percentage by current process or by process specified by id, like in top.\n",
    "\n",
    "    Source: https://stackoverflow.com/a/30014612/304209.\n",
    "\n",
    "    Args:\n",
    "        pid: pid of the process to analyze. If None, analyze the current process.\n",
    "\n",
    "    Returns:\n",
    "        cpu usage of the process, in percentage like in top, values in [0, 100].\n",
    "    \"\"\"\n",
    "    if pid is None:\n",
    "        pid = os.getpid()\n",
    "    process = psutil.Process(pid)\n",
    "    return round(process.cpu_percent(),3)\n",
    "\n",
    "\n",
    "def num_threads_psutil(pid=None):\n",
    "    \"\"\"Get num_threads by current process or by process specified by id, like in top.\n",
    "\n",
    "    Source: https://stackoverflow.com/a/30014612/304209.\n",
    "\n",
    "    Args:\n",
    "        pid: pid of the process to analyze. If None, analyze the current process.\n",
    "\n",
    "    Returns:\n",
    "        num_threads of the process\n",
    "    \"\"\"\n",
    "    if pid is None:\n",
    "        pid = os.getpid()\n",
    "    process = psutil.Process(pid)\n",
    "    return process.num_threads()\n",
    "\n",
    "\n",
    "def long_substr(strings):\n",
    "    \"\"\"Find longest common substring in a list of strings.\n",
    "\n",
    "    Source: https://stackoverflow.com/a/2894073/304209.\n",
    "\n",
    "    Args:\n",
    "        strings: list of strings.\n",
    "\n",
    "    Returns:\n",
    "        longest substring which is found in all of the strings.\n",
    "    \"\"\"\n",
    "    substr = ''\n",
    "    if len(strings) > 1 and len(strings[0]) > 0:\n",
    "        for i in range(len(strings[0])):\n",
    "            for j in range(len(strings[0])-i+1):\n",
    "                if j > len(substr) and all(strings[0][i:i+j] in x for x in strings):\n",
    "                    substr = strings[0][i:i+j]\n",
    "    return substr\n",
    "\n",
    "\n",
    "def expand_column(dataframe, column):\n",
    "    \"\"\"Transform iterable column values into multiple rows.\n",
    "\n",
    "    Source: https://stackoverflow.com/a/27266225/304209.\n",
    "\n",
    "    Args:\n",
    "        dataframe: DataFrame to process.\n",
    "        column: name of the column to expand.\n",
    "\n",
    "    Returns:\n",
    "        copy of the DataFrame with the following updates:\n",
    "            * for rows where column contains only 1 value, keep them as is.\n",
    "            * for rows where column contains a list of values, transform them\n",
    "                into multiple rows, each of which contains one value from the list in column.\n",
    "    \"\"\"\n",
    "    tmp_df = dataframe.apply(\n",
    "        lambda row: pd.Series(row[column]), axis=1).stack().reset_index(level=1, drop=True)\n",
    "    tmp_df.name = column\n",
    "    return dataframe.drop(column, axis=1).join(tmp_df)\n",
    "\n",
    "df = show_notebooks_table('15.15.166.35', 18888)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:26.590781Z",
     "start_time": "2021-07-05T08:45:26.586654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import types\n",
    "import astunparse\n",
    "\n",
    "func_def = \\\n",
    "\"\"\"\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "print(add(3, 5))\n",
    "\"\"\"\n",
    "\n",
    "cm = compile(func_def, '<string>', 'exec')\n",
    "assert(isinstance(cm, types.CodeType))\n",
    "\n",
    "exec(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:26.600523Z",
     "start_time": "2021-07-05T08:45:26.593073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(body=[\n",
      "  FunctionDef(\n",
      "    name='add',\n",
      "    args=arguments(\n",
      "      args=[\n",
      "        arg(\n",
      "          arg='x',\n",
      "          annotation=None),\n",
      "        arg(\n",
      "          arg='y',\n",
      "          annotation=None)],\n",
      "      vararg=None,\n",
      "      kwonlyargs=[],\n",
      "      kw_defaults=[],\n",
      "      kwarg=None,\n",
      "      defaults=[]),\n",
      "    body=[Return(value=BinOp(\n",
      "      left=Name(\n",
      "        id='x',\n",
      "        ctx=Load()),\n",
      "      op=Add(),\n",
      "      right=Name(\n",
      "        id='y',\n",
      "        ctx=Load())))],\n",
      "    decorator_list=[],\n",
      "    returns=None),\n",
      "  Expr(value=Call(\n",
      "    func=Name(\n",
      "      id='print',\n",
      "      ctx=Load()),\n",
      "    args=[Call(\n",
      "      func=Name(\n",
      "        id='add',\n",
      "        ctx=Load()),\n",
      "      args=[\n",
      "        Num(n=3),\n",
      "        Num(n=5)],\n",
      "      keywords=[])],\n",
      "    keywords=[]))])\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "def add(x, y):\n",
      "    return (x + y)\n",
      "print(add(3, 5))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成AST\n",
    "r_node = ast.parse(func_def)\n",
    "# 也可调用ast.dump(r_node)，但它不能进行缩进输出\n",
    "ast_output = astunparse.dump(r_node)  \n",
    "print(ast_output)\n",
    "\n",
    "# 从AST得到源代码\n",
    "print('-'*50)\n",
    "print(astunparse.unparse(r_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.017517Z",
     "start_time": "2021-07-05T08:45:26.602648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Name:add\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'ast' has no attribute 'print'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8b8b43d27792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mr_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mvisitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCodeVisitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mvisitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mastunparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ast.py\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'visit_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mvisitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_visit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgeneric_visit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ast.py\u001b[0m in \u001b[0;36mgeneric_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/ast.py\u001b[0m in \u001b[0;36mvisit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'visit_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mvisitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_visit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgeneric_visit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8b8b43d27792>\u001b[0m in \u001b[0;36mvisit_FunctionDef\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Function Name:%s'\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_visit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         func_log_stmt = ast.print(\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mdest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'calling func: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ast' has no attribute 'print'"
     ]
    }
   ],
   "source": [
    "class CodeVisitor(ast.NodeVisitor):\n",
    "    def visit_BinOp(self, node):\n",
    "        if isinstance(node.op, ast.Add):\n",
    "            node.op = ast.Sub()\n",
    "        self.generic_visit(node)\n",
    "\n",
    "    def visit_FunctionDef(self, node):\n",
    "        print('Function Name:%s'% node.name)\n",
    "        self.generic_visit(node)\n",
    "        func_log_stmt = ast.print(\n",
    "            dest = None,\n",
    "            values = [ast.Str(s = 'calling func: %s' % node.name, lineno = 0, col_offset = 0)],\n",
    "            nl = True,\n",
    "            lineno = 0,\n",
    "            col_offset = 0,\n",
    "        )\n",
    "        node.body.insert(0, func_log_stmt)\n",
    "\n",
    "r_node = ast.parse(func_def)\n",
    "visitor = CodeVisitor()\n",
    "visitor.visit(r_node)\n",
    "\n",
    "print(astunparse.unparse(r_node))\n",
    "exec(compile(r_node, '<string>', 'exec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.021085Z",
     "start_time": "2021-07-05T08:45:22.403Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.022525Z",
     "start_time": "2021-07-05T08:45:22.406Z"
    }
   },
   "outputs": [],
   "source": [
    "dir(ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.024026Z",
     "start_time": "2021-07-05T08:45:22.407Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections, re\n",
    "pairs = collections.defaultdict(int)\n",
    "pairs['\\u2581 3', '4']=5\n",
    "pairs['4', '5']=3\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.025540Z",
     "start_time": "2021-07-05T08:45:22.410Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_vocab(pair):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "\n",
    "    w_out = p.sub(''.join(pair), word)\n",
    "    return w_out\n",
    "\n",
    "bigrams = [re.escape(' '.join(pair)) for pair in pairs]\n",
    "print(f'bigrams={bigrams}')\n",
    "ps = [re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') for bigram in bigrams] \n",
    "print(f'ps={ps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.027084Z",
     "start_time": "2021-07-05T08:45:22.411Z"
    }
   },
   "outputs": [],
   "source": [
    "pair = list(pairs.keys())[0]\n",
    "bigram = re.escape(' '.join(pair))\n",
    "p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "print(p)\n",
    "word=r'3\\\\ 4'\n",
    "p.sub(''.join(pair), word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.028635Z",
     "start_time": "2021-07-05T08:45:22.413Z"
    }
   },
   "outputs": [],
   "source": [
    "''.join(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.030053Z",
     "start_time": "2021-07-05T08:45:22.414Z"
    }
   },
   "outputs": [],
   "source": [
    "word='3\\\\ 4'\n",
    "word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.031467Z",
     "start_time": "2021-07-05T08:45:22.415Z"
    }
   },
   "outputs": [],
   "source": [
    "word='3\\\\ 4'\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.032880Z",
     "start_time": "2021-07-05T08:45:22.417Z"
    }
   },
   "outputs": [],
   "source": [
    "def sub(pair, word):\n",
    "    '''在word中替换pair为pair去空格'''\n",
    "    print(f'word = {word}')\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "\n",
    "    # 匹配，前后都为空的pair\n",
    "    # (?<!\\S)\t匹配前面不是非空的位置，即匹配前面是空\n",
    "    # (?!\\S)\t匹配后面跟的不是非空的位置，即匹配后面是空      \n",
    "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    print(f'pattern = {pattern}')\n",
    "    new_word = pattern.sub(''.join(pair), word)\n",
    "    print(f'new_word = {new_word}') \n",
    "\n",
    "word = 'b c ab c b c c bd b c'\n",
    "\n",
    "pair = ('b', 'c')\n",
    "# 从结果可以看到，第一个,第三个，第五个b c被替换成了bc\n",
    "sub(pair, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.034300Z",
     "start_time": "2021-07-05T08:45:22.420Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# word = re.escape('\\u2581abcd') \n",
    "# print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.035698Z",
     "start_time": "2021-07-05T08:45:22.421Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = '\\S'\n",
    "string = ' 344-Hello--World!'\n",
    "result = re.search(pattern, string)\n",
    "print(result)\n",
    "print(result.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.037089Z",
     "start_time": "2021-07-05T08:45:22.422Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern = '[a-z]+'\n",
    "string = '-----2344-HELLO--WORLD!'\n",
    "result = re.search(pattern, string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.038610Z",
     "start_time": "2021-07-05T08:45:22.424Z"
    }
   },
   "outputs": [],
   "source": [
    "pair = ('4', '5')\n",
    "bigram = re.escape(' '.join(pair))\n",
    "p = re.compile(bigram)\n",
    "print(p)\n",
    "word = re.escape('1234 567') \n",
    "print(word)\n",
    "print(''.join(pair))\n",
    "print(p.sub(''.join(pair), word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.040069Z",
     "start_time": "2021-07-05T08:45:22.425Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1 = np.array([0.2, 0.3, 0.1, 0.4])\n",
    "scores = np.array([2, 3, 4, -1])\n",
    "order = scores.argsort()[::-1] \n",
    "print(order)\n",
    "i = order[0]\n",
    "np.maximum(x1[i], x1[order[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.041801Z",
     "start_time": "2021-07-05T08:45:22.426Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def py_cpu_nms(dets, thresh):\n",
    "    \"\"\"\n",
    "    dets是shape为 Nx5 的ndarray,如:\n",
    "    dets = np.array([[5,5,20,20,0.7],\n",
    "                     [0,0,10,10,0.3]])\n",
    "    \n",
    "    \"\"\"\n",
    "    x1 = dets[:, 0] #所有的x1\n",
    "    y1 = dets[:, 1] #所有的y1\n",
    "    x2 = dets[:, 2] #所有的x2\n",
    "    y2 = dets[:, 3] #所有的y2\n",
    "    scores = dets[:, 4] #所有的得分\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1) #所有框的面积\n",
    "    order = scores.argsort()[::-1] #面积排序\n",
    "\n",
    "    keep = [] #\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        \n",
    "        print(x1[order[1:]])\n",
    "        print(xx1,yy1,xx2,yy2)\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= thresh)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.043348Z",
     "start_time": "2021-07-05T08:45:22.427Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.044809Z",
     "start_time": "2021-07-05T08:45:22.428Z"
    }
   },
   "outputs": [],
   "source": [
    "a =1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.046320Z",
     "start_time": "2021-07-05T08:45:22.430Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install  jupyter_contrib_nbextensions\n",
    "!pip install jupyter_nbextensions_configurator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.047827Z",
     "start_time": "2021-07-05T08:45:22.433Z"
    }
   },
   "outputs": [],
   "source": [
    "!jupyter contrib nbextension install --user \n",
    "!jupyter nbextensions_configurator enable --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.049309Z",
     "start_time": "2021-07-05T08:45:22.434Z"
    }
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.050712Z",
     "start_time": "2021-07-05T08:45:22.435Z"
    }
   },
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.052294Z",
     "start_time": "2021-07-05T08:45:22.437Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-05T08:45:27.053695Z",
     "start_time": "2021-07-05T08:45:22.439Z"
    }
   },
   "outputs": [],
   "source": [
    "np.testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-06T08:59:44.568952Z",
     "start_time": "2021-07-06T08:59:44.562463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2042040"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "561*728*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T01:47:54.989138Z",
     "start_time": "2021-07-09T01:47:54.975609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "汪汪叫\n",
      "汪汪叫\n",
      "神一样的叫唤...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Dog:\n",
    " \n",
    "    def bark(self):\n",
    "        print(\"汪汪叫\")\n",
    " \n",
    "# 子类  继承\n",
    "class XiaoTianQuan(Dog):\n",
    " \n",
    "    def fly(self):\n",
    "        print(\"我会飞\")\n",
    " \n",
    "    # 可以重写父类中的同名方法\n",
    "    def bark(self):\n",
    " \n",
    "        # super().父类方法名 调用父类中的方法 (第一种方式)(推荐)\n",
    "        super().bark()\n",
    " \n",
    "        # 父类名.方法(self) 调用父类中的方法 (第二种方式,python2.x)(不推荐,父类名修改后,此处也得改)\n",
    "        Dog.bark(self)\n",
    " \n",
    "        # 注意：如果使用子类名调用方法，可能会出现递归调用 -- 死循环！\n",
    "        # XiaoTianQuan.bark(self)  # 会产生死循环\n",
    " \n",
    "        # 针对子类特有的需求，进行扩展\n",
    "        print(\"神一样的叫唤...\")\n",
    " \n",
    " \n",
    "xtq = XiaoTianQuan()\n",
    "xtq.bark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T08:42:53.191722Z",
     "start_time": "2021-07-12T08:42:53.169975Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-835a6f7471a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "a = lambda x, y: x+y, x-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-12T08:42:46.289246Z",
     "start_time": "2021-07-12T08:42:46.282387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, -1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T07:37:43.299150Z",
     "start_time": "2021-07-13T07:37:43.291767Z"
    }
   },
   "outputs": [],
   "source": [
    "r= np.array([[1, 2, 3], [3, 4, 2], [0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T07:37:52.262629Z",
     "start_time": "2021-07-13T07:37:52.254435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[np.all(r == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-13T07:37:54.996094Z",
     "start_time": "2021-07-13T07:37:54.987513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 4, 2]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[~np.all(r == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T09:40:30.201516Z",
     "start_time": "2021-07-15T09:40:29.238993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/eipi10/xuxiangwen.github.io/_notes\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T09:42:47.565472Z",
     "start_time": "2021-07-15T09:42:47.556295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_notes'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dataset_path='/tf/eipi10/xuxiangwen.github.io/_notes/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T01:40:46.069562Z",
     "start_time": "2021-07-17T01:40:46.065305Z"
    }
   },
   "outputs": [],
   "source": [
    "def f2(a, b=2, c=3):\n",
    "    return a + b + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T07:25:12.017398Z",
     "start_time": "2021-07-17T07:25:12.013448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['1', '2', '3']\n",
    "a[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextReader:\n",
    "    \"\"\"Print and number lines in a text file.\"\"\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.file = open(filename)\n",
    "        self.lineno = 0\n",
    "\n",
    "    def readline(self):\n",
    "        self.lineno += 1\n",
    "        line = self.file.readline()\n",
    "        if not line:\n",
    "            return None\n",
    "        if line.endswith('\\n'):\n",
    "            line = line[:-1]\n",
    "        return \"%i: %s\" % (self.lineno, line)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Copy the object's state from self.__dict__ which contains\n",
    "        # all our instance attributes. Always use the dict.copy()\n",
    "        # method to avoid modifying the original state.\n",
    "        state = self.__dict__.copy()\n",
    "        # Remove the unpicklable entries.\n",
    "        del state['file']\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Restore instance attributes (i.e., filename and lineno).\n",
    "        self.__dict__.update(state)\n",
    "        # Restore the previously opened file's state. To do so, we need to\n",
    "        # reopen it and read from it until the line count is restored.\n",
    "        file = open(self.filename)\n",
    "        for _ in range(self.lineno):\n",
    "            file.readline()\n",
    "        # Finally, save the file.\n",
    "        self.file = file\n",
    "        \n",
    "reader = TextReader(\"hello.txt\")\n",
    "print(reader.readline())\n",
    "print(reader.readline())\n",
    "s = pickle.dumps(reader)\n",
    "#print(s)\n",
    "new_reader = pickle.loads(s)\n",
    "print(new_reader.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:25:09.034163Z",
     "start_time": "2021-07-17T09:25:08.228278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/eipi10/xuxiangwen.github.io/_notes\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T09:25:20.079646Z",
     "start_time": "2021-07-17T09:25:20.072645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tf/eipi10/xuxiangwen.github.io'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.dirname('/tf/eipi10/xuxiangwen.github.io/_notes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T09:32:46.304267Z",
     "start_time": "2021-07-19T09:32:46.293444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 3, 'b': 2, 'e': 2}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':1, 'b':2}\n",
    "class B:\n",
    "    def __init__(self):\n",
    "        self.c = {'e':2, 'a':3}\n",
    "\n",
    "dict(a, **B().c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T09:31:52.618661Z",
     "start_time": "2021-07-19T09:31:52.610656Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4592cda7d891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'c'"
     ]
    }
   ],
   "source": [
    "b.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T03:34:56.826730Z",
     "start_time": "2021-08-10T03:34:56.819961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60000/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T09:38:29.175615Z",
     "start_time": "2021-08-10T09:38:29.171116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-08-10 09:38'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "now.strftime(\"%Y-%m-%d %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-10T09:39:54.757838Z",
     "start_time": "2021-08-10T09:39:54.753862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-08-10 09:39'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:07:10.636514Z",
     "start_time": "2021-08-17T02:07:09.965611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 12, 8)]           0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 12, 10)            90        \n",
      "=================================================================\n",
      "Total params: 90\n",
      "Trainable params: 90\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,Conv2D,TimeDistributed, Input\n",
    " \n",
    "input_ = Input(shape=(12,8))\n",
    "out = TimeDistributed(Dense(units=10))(input_)\n",
    "#out = Dense(units=10)(input_)\n",
    "model = Model(inputs=input_,outputs=out)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:08:28.909572Z",
     "start_time": "2021-08-17T02:08:28.864113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 12, 8)]           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12, 10)            90        \n",
      "=================================================================\n",
      "Total params: 90\n",
      "Trainable params: 90\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ = Input(shape=(12,8))\n",
    "# out = TimeDistributed(Dense(units=10))(input_)\n",
    "out = Dense(units=10)(input_)\n",
    "model = Model(inputs=input_,outputs=out)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:25:31.728689Z",
     "start_time": "2021-08-17T02:25:31.705627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 12, 32, 32, 2)]   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 12, 32, 32, 7)     133       \n",
      "=================================================================\n",
      "Total params: 133\n",
      "Trainable params: 133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ = Input(shape=(12,32,32, 2))\n",
    "out = TimeDistributed(Conv2D(filters=7,kernel_size=(3,3),padding='same'))(input_)\n",
    "# out = Conv2D(filters=7,kernel_size=(3,3),padding='same')(input_)\n",
    "model = Model(inputs=input_,outputs=out)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:27:11.534227Z",
     "start_time": "2021-08-17T02:27:11.488190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 12, 32, 32, 2)]   0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 32, 32, 7)     133       \n",
      "=================================================================\n",
      "Total params: 133\n",
      "Trainable params: 133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ = Input(shape=(12,32,32, 2))\n",
    "# out = TimeDistributed(Conv2D(filters=7,kernel_size=(3,3),padding='same'))(input_)\n",
    "out = Conv2D(filters=7,kernel_size=(3,3),padding='same')(input_)\n",
    "model = Model(inputs=input_,outputs=out)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T00:22:23.847307Z",
     "start_time": "2021-08-18T00:21:55.001664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (21.1.3)\n",
      "Collecting pip\n",
      "  Downloading pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 230 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (56.2.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-57.4.0-py3-none-any.whl (819 kB)\n",
      "\u001b[K     |████████████████████████████████| 819 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (0.36.2)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.36.2\n",
      "    Uninstalling wheel-0.36.2:\n",
      "      Successfully uninstalled wheel-0.36.2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 56.2.0\n",
      "    Uninstalling setuptools-56.2.0:\n",
      "      Successfully uninstalled setuptools-56.2.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.3\n",
      "    Uninstalling pip-21.1.3:\n",
      "      Successfully uninstalled pip-21.1.3\n",
      "Successfully installed pip-21.2.4 setuptools-57.4.0 wheel-0.37.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (3.1.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.61.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (8.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy) (5.1.0)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy) (0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.8->spacy) (2.4)\n",
      "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.8->spacy) (0.16)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy) (2.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "2021-08-18 00:22:13.421465: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.6 MB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from en-core-web-sm==3.1.0) (3.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.61.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2020.12.5)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.8->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4)\n",
      "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.8->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.16)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T00:42:31.922293Z",
     "start_time": "2021-08-18T00:42:28.749655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T00:42:07.007426Z",
     "start_time": "2021-08-18T00:41:49.489036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy[cuda112] in /usr/local/lib/python3.6/dist-packages (3.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (20.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (1.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (0.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (2.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (0.8.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (3.7.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (2.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (8.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (4.61.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (3.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (57.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (0.7.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (1.19.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy[cuda112]) (3.0.8)\n",
      "Collecting cupy-cuda112<10.0.0,>=5.0.0b4\n",
      "  Downloading cupy_cuda112-9.3.0-cp36-cp36m-manylinux1_x86_64.whl (76.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 76.0 MB 13 kB/s s eta 0:00:01     |████▌                           | 10.6 MB 22.6 MB/s eta 0:00:03     |████▉                           | 11.6 MB 22.6 MB/s eta 0:00:03     |█████▊                          | 13.5 MB 22.6 MB/s eta 0:00:03     |███████████▊                    | 27.8 MB 25.4 MB/s eta 0:00:02     |█████████████                   | 30.6 MB 25.4 MB/s eta 0:00:02     |██████████████                  | 33.5 MB 2.4 MB/s eta 0:00:18     |███████████████████████▎        | 55.3 MB 14.5 MB/s eta 0:00:02     |████████████████████████▏       | 57.3 MB 14.5 MB/s eta 0:00:02     |█████████████████████████▊      | 61.1 MB 14.5 MB/s eta 0:00:02     |███████████████████████████▊    | 65.9 MB 19.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from catalogue<2.1.0,>=2.0.4->spacy[cuda112]) (3.4.1)\n",
      "Collecting fastrlock>=0.5\n",
      "  Downloading fastrlock-0.6-cp36-cp36m-manylinux1_x86_64.whl (39 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->spacy[cuda112]) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy[cuda112]) (5.1.0)\n",
      "Requirement already satisfied: dataclasses<1.0,>=0.6 in /usr/local/lib/python3.6/dist-packages (from pathy>=0.3.5->spacy[cuda112]) (0.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda112]) (2.6)\n",
      "Requirement already satisfied: contextvars<3,>=2.4 in /usr/local/lib/python3.6/dist-packages (from thinc<8.1.0,>=8.0.8->spacy[cuda112]) (2.4)\n",
      "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.6/dist-packages (from contextvars<3,>=2.4->thinc<8.1.0,>=8.0.8->spacy[cuda112]) (0.16)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.6/dist-packages (from typer<0.4.0,>=0.3.0->spacy[cuda112]) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /usr/local/lib/python3.6/dist-packages (from jinja2->spacy[cuda112]) (2.0.0)\n",
      "Installing collected packages: fastrlock, cupy-cuda112\n",
      "Successfully installed cupy-cuda112-9.3.0 fastrlock-0.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy[cuda112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T11:52:19.385749Z",
     "start_time": "2021-08-20T11:52:18.602551Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-20T08:03:49.694887Z",
     "start_time": "2021-08-20T08:03:49.673604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple isn't looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T06:44:19.874086Z",
     "start_time": "2021-08-21T06:44:18.513167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "内战 --> 萨尔瓦多\n",
      "内战\n",
      "任务 --> 纳米比亚\n",
      "任务\n",
      "隔离 --> 南非\n",
      "隔离\n",
      "统治 --> 柬埔寨\n",
      "统治\n",
      "授权 --> 美国\n",
      "授权\n",
      "入侵 --> 伊拉克\n",
      "入侵\n",
      "入侵 --> 科威特\n",
      "入侵\n",
      "莫桑比克 --> 索马里\n",
      "莫桑比克\n",
      "莫桑比克 --> 海地\n",
      "莫桑比克\n",
      "南斯拉夫 --> 莫桑比克\n",
      "南斯拉夫\n",
      "包括 --> 南斯拉夫\n",
      "包括\n",
      "惨重 --> 美国\n",
      "惨重\n",
      "行动 --> 索马里\n",
      "行动\n",
      "援助团 --> 卢旺达\n",
      "援助团\n",
      "大屠杀 --> 卢旺达\n",
      "大屠杀\n",
      "欧洲 --> 美国\n",
      "欧洲\n",
      "总统 --> 美国\n",
      "总统\n",
      "新加坡 --> 英国\n",
      "新加坡\n",
      "紧随 --> 新加坡\n",
      "紧随\n",
      "撤资 --> 美国\n",
      "撤资\n",
      "任务 --> 塞拉利昂\n",
      "任务\n",
      "陆战队 --> 英国\n",
      "陆战队\n",
      "入侵 --> 阿富汗\n",
      "入侵\n",
      "入侵 --> 美国\n",
      "入侵\n",
      "入侵 --> 伊拉克\n",
      "入侵\n",
      "介入 --> 苏丹\n",
      "介入\n",
      "共和国 --> 刚果\n",
      "共和国\n",
      "内战 --> 叙利亚\n",
      "内战\n",
      "内战 --> 斯里兰卡\n",
      "内战\n",
      "地震 --> 海地\n",
      "地震\n",
      "[('萨尔瓦多', 'GPE'), ('纳米比亚', 'GPE'), ('南非', 'GPE'), ('柬埔寨', 'GPE'), ('美国', 'GPE'), ('伊拉克', 'GPE'), ('科威特', 'GPE'), ('索马里', 'GPE'), ('海地', 'GPE'), ('莫桑比克', 'GPE'), ('南斯拉夫', 'GPE'), ('美国', 'GPE'), ('索马里', 'GPE'), ('卢旺达', 'GPE'), ('卢旺达', 'GPE'), ('美国', 'GPE'), ('美国', 'GPE'), ('英国', 'GPE'), ('新加坡', 'GPE'), ('美国', 'GPE'), ('塞拉利昂', 'GPE'), ('英国', 'GPE'), ('阿富汗', 'GPE'), ('美国', 'GPE'), ('伊拉克', 'GPE'), ('苏丹', 'GPE'), ('刚果', 'GPE'), ('叙利亚', 'GPE'), ('斯里兰卡', 'GPE'), ('海地', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from spacy.lang.zh import Chinese\n",
    "\n",
    "\n",
    "COUNTRIES = ['阿富汗', '奥兰群岛', '阿尔巴尼亚', '阿尔及利亚', '美属萨摩亚', '安道尔', \n",
    "             '安哥拉', '安圭拉', '安提瓜和巴布达', '阿根廷', '亚美尼亚', '阿鲁巴', \n",
    "             '澳大利亚', '奥地利', '阿塞拜疆', '孟加拉', '巴林', '巴哈马', '巴巴多斯', \n",
    "             '白俄罗斯', '比利时', '伯利兹', '贝宁', '百慕大', '不丹', '玻利维亚', \n",
    "             '波斯尼亚和黑塞哥维那', '博茨瓦纳', '布维岛', '巴西', '文莱', '保加利亚', \n",
    "             '布基纳法索', '布隆迪', '柬埔寨', '喀麦隆', '加拿大', '佛得角', '中非', \n",
    "             '乍得', '智利', '圣诞岛', '科科斯群岛', '哥伦比亚', '科摩罗', '刚果', \n",
    "             '刚果', '库克群岛', '哥斯达黎加', '科特迪瓦', '中国', '克罗地亚', '古巴', \n",
    "             '捷克', '塞浦路斯', '丹麦', '吉布提', '多米尼加', '东帝汶', '厄瓜多尔', '埃及', \n",
    "             '赤道几内亚', '厄立特里亚', '爱沙尼亚', '埃塞俄比亚', '法罗群岛', '斐济', '法国', \n",
    "             '法国大都会', '法属圭亚那', '法属波利尼西亚', '加蓬', '冈比亚', '格鲁吉亚', '德国', \n",
    "             '加纳', '直布罗陀', '希腊', '格林纳达', '瓜德罗普岛', '关岛', '危地马拉', '根西岛', \n",
    "             '几内亚比绍', '几内亚', '圭亚那', '海地', '洪都拉斯', '匈牙利', '冰岛', '印度', \n",
    "             '印度尼西亚', '伊朗', '伊拉克', '爱尔兰', '马恩岛', '以色列', '意大利', '牙买加', \n",
    "             '日本', '泽西岛', '约旦', '哈萨克斯坦', '肯尼亚', '基里巴斯', '韩国', '朝鲜', \n",
    "             '科威特', '吉尔吉斯斯坦', '老挝', '拉脱维亚', '黎巴嫩', '莱索托', '利比里亚', \n",
    "             '利比亚', '列支敦士登', '立陶宛', '卢森堡', '马其顿', '马拉维', '马来西亚', \n",
    "             '马达加斯加', '马尔代夫', '马里', '马耳他', '马绍尔群岛', '马提尼克岛', \n",
    "             '毛里塔尼亚', '毛里求斯', '马约特', '墨西哥', '密克罗尼西亚', '摩尔多瓦', \n",
    "             '摩纳哥', '蒙古', '黑山', '蒙特塞拉特', '摩洛哥', '莫桑比克', '缅甸', \n",
    "             '纳米比亚', '瑙鲁', '尼泊尔', '荷兰', '新喀里多尼亚', '新西兰', '尼加拉瓜', \n",
    "             '尼日尔', '尼日利亚', '纽埃', '诺福克岛', '挪威', '阿曼', '巴基斯坦', '帕劳', \n",
    "             '巴勒斯坦', '巴拿马', '巴布亚新几内亚', '巴拉圭', '秘鲁', '菲律宾', '皮特凯恩群岛', \n",
    "             '波兰', '葡萄牙', '波多黎各', '卡塔尔', '留尼汪岛', '罗马尼亚', '卢旺达', '俄罗斯联邦', \n",
    "             '圣赫勒拿', '圣基茨和尼维斯', '圣卢西亚', '圣文森特和格林纳丁斯', '萨尔瓦多', '萨摩亚', \n",
    "             '圣马力诺', '圣多美和普林西比', '沙特阿拉伯', '塞内加尔', '塞舌尔', '塞拉利昂', '新加坡', \n",
    "             '塞尔维亚', '斯洛伐克', '斯洛文尼亚', '所罗门群岛', '索马里', '南非', '西班牙', '斯里兰卡', \n",
    "             '苏丹', '苏里南', '斯威士兰', '瑞典', '瑞士', '叙利亚', '塔吉克斯坦', '坦桑尼亚', '泰国', \n",
    "             '特立尼达和多巴哥', '东帝汶', '多哥', '托克劳', '汤加', '突尼斯', '土耳其', '土库曼斯坦', \n",
    "             '图瓦卢', '乌干达', '乌克兰', '阿拉伯联合酋长国', '英国', '美国', '乌拉圭', '乌兹别克斯坦', \n",
    "             '瓦努阿图', '梵蒂冈', '委内瑞拉', '越南', '瓦利斯群岛和富图纳群岛', '西撒哈拉', '也门', \n",
    "             '南斯拉夫', '赞比亚', '津巴布韦']\n",
    "\n",
    "TEXT = \"\"\"冷战结束后，联合国在维和方面有显著的扩展，在十年内的维和行动数量超过过去四十年。在1988年和2000年间，通过的安理会决议数量翻了超过一倍，维和的预算也增加超过十倍。联合国调停结束萨尔瓦多内战，成功进行纳米比亚的维和任务，并监督了南非种族隔离和柬埔寨红色高棉统治之后的民主选举。1991年，联合国授权美国领导的同盟军事行动，击退伊拉克对科威特的入侵。1971年至1985年担任副秘书长的布莱恩·厄克特后来说这些行动的成功为联合国带来一种“虚假复兴”——在之后，联合国的任务变得更为艰难。\n",
    "\n",
    "联合国宪章原本目的是阻止一个国家对另一个国家的进攻，但在90年代初联合国面临许多同时发生的严重国内危机，包括索马里、海地、莫桑比克和前南斯拉夫。在美国于摩加迪沙之战中损失惨重而撤出之后，联合国在索马里的行动被普遍认为是失败的；而联合国在波黑的行动则因为面对种族清洗显得任务不清和不果决而成为“世界的笑柄”。1994年，联合国卢旺达援助团在安理会无法决断的情况下没能对卢旺达大屠杀进行介入。\n",
    "\n",
    "在冷战的最后数十年中，联合国受到来自美国和欧洲的批评家的批评，被指管理不善和腐败。1984年，美国总统罗纳德·里根停止对联合国教科文组织的资助，英国和新加坡也紧随其后。1992年至1996年在任的联合国秘书长布特罗斯·布特罗斯-加利对秘书处进行改革，在一定程度上精简了组织的规模。他的继任者科菲·安南（1997年-2006年在任）面对美国撤资的威胁进行进一步的管理改革。\n",
    "\n",
    "20世纪90年代末和21世纪初，联合国授权的国际介入形式多样。联合国在塞拉利昂的任务得到英国皇家海军陆战队的支持，而对阿富汗的入侵则由北约领衔。2003年，在安理会决议未能通过授权的情况下，美国仍然入侵伊拉克，新一轮的对联合国是否有效的争论就此展开。在秘书长潘基文任下，联合国维和行动介入苏丹的达尔富尔冲突和刚果民主共和国的基伍冲突，并派出观察员和化学武器核查人员前往叙利亚内战。2013年，对联合国于2009年在斯里兰卡内战末期的行动的内部审查表明，该组织遭遇“系统性的失败”。在2010年海地地震中，有101名联合国人员殉职，这是联合国历史上最为惨痛的损失。\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# 创建一个doc并重置其已有的实体\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# 遍历所有的匹配结果\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # 创建一个标签为\"GPE\"的span\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "\n",
    "    # 覆盖doc.ents并添加这个span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # 获取这个span的根头词符\n",
    "    span_root_head = span.root.head \n",
    "\n",
    "    # 打印这个span的根头词符的文本及span的文本\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# 打印文档中的所有实体\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T04:45:47.465820Z",
     "start_time": "2021-08-21T04:45:46.809206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, world\n",
      "15578876784678163569 HelloWorld 4 6 Hello world\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "patterns = [\n",
    "    [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}],\n",
    "    [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n",
    "]\n",
    "matcher.add(\"HelloWorld\", patterns)\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T05:35:11.193541Z",
     "start_time": "2021-08-21T05:35:10.627785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found match: United States\n",
      "Found match: United States\n",
      "Found match: U.S.\n",
      "Found match: US\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\")\n",
    "\n",
    "expression = r\"[Uu](nited|\\.?) ?[Ss](tates|\\.?)\"\n",
    "for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T05:55:45.897363Z",
     "start_time": "2021-08-21T05:55:45.207981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found match: United States\n",
      "Found match: United States\n",
      "Found match: U.S.\n",
      "Found match: US\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\")\n",
    "\n",
    "expression = r\"[Uu](nited|\\.?) ?[Ss](tates|\\.?)\"\n",
    "for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T06:15:30.184848Z",
     "start_time": "2021-08-21T06:15:29.180461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "0 荣耀 NOUN 0\n",
      "1 发布 VERB 2\n",
      "2 锐龙 NOUN 4\n",
      "3 笔记本 NOUN 6\n",
      "4 ， PUNCT 9\n",
      "5 显然 ADV 10\n",
      "6 会 VERB 12\n",
      "7 配备 VERB 13\n",
      "8 7nm NUM 15\n",
      "9 工艺 NOUN 18\n",
      "10 、 PUNCT 20\n",
      "11 Zen NOUN 21\n",
      "12 2 NUM 24\n",
      "13 架构 NOUN 26\n",
      "14 的 PART 28\n",
      "15 全新 ADJ 29\n",
      "16 锐龙 NOUN 31\n",
      "17 4000 NUM 33\n",
      "18 系列 NUM 38\n",
      "19 ， PUNCT 40\n",
      "20 但 ADV 41\n",
      "21 具体 ADV 42\n",
      "22 采用 VERB 44\n",
      "23 低 ADJ 46\n",
      "24 功耗 NOUN 47\n",
      "25 的 PART 49\n",
      "26 锐龙 NOUN 50\n",
      "27 4000 NUM 52\n",
      "28 U NOUN 56\n",
      "29 系列 NOUN 58\n",
      "30 ， PUNCT 60\n",
      "31 还是 CCONJ 61\n",
      "32 高性能 NOUN 63\n",
      "33 的 PART 66\n",
      "34 锐龙 NOUN 67\n",
      "35 4000 NUM 69\n",
      "36 H NUM 73\n",
      "37 系列 NUM 75\n",
      "--------------------------------------------------\n",
      "PATTERN1 锐龙笔记本\n",
      "PATTERN2 锐龙4000U\n",
      "PATTERN2 锐龙4000H\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"荣耀发布锐龙笔记本，显然会配备7nm工艺、Zen2 架构的\"\n",
    "          \"全新锐龙4000 系列，但具体采用低功耗的锐龙4000U 系列，还是高性能的锐龙4000H 系列\"\n",
    "         )\n",
    "\n",
    "print('-'*50)\n",
    "for i, token in enumerate(doc):\n",
    "    print(i, token.text, token.pos_, token.idx)\n",
    "\n",
    "# 创建匹配模板\n",
    "pattern1 = [{\"POS\": \"NOUN\"},{\"TEXT\": \"笔记本\"}]\n",
    "pattern2 = [{\"TEXT\": \"锐龙\"}, {\"TEXT\": {\"REGEX\": \"\\d+\"}}, {\"TEXT\": {\"REGEX\": \"[a-z0-9A-Z_]+\"}}]\n",
    "\n",
    "# 初始化matcher并加入模板\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# 遍历匹配结果\n",
    "print('-'*50)\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # 打印匹配到的字符串名字及匹配到的span的文本\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T06:17:25.313988Z",
     "start_time": "2021-08-21T06:17:24.455941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Call', 'me', 'at', '(', '123', ')', '456', '789', 'or', '(', '123', ')', '456', '789', '!']\n",
      "(123) 456 789\n",
      "(123) 456 789\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
    "           {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"ddd\"}]\n",
    "matcher.add(\"PHONE_NUMBER\", [pattern])\n",
    "\n",
    "doc = nlp(\"Call me at (123) 456 789 or (123) 456 789!\")\n",
    "print([t.text for t in doc])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T06:45:09.973645Z",
     "start_time": "2021-08-21T06:45:09.710138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on lowercase token text: angela merkel\n",
      "Matched based on lowercase token text: barack Obama\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in [\"Angela Merkel\", \"Barack Obama\"]]\n",
    "matcher.add(\"Names\", patterns)\n",
    "\n",
    "doc = nlp(\"angela merkel and us president barack Obama\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T06:25:57.598211Z",
     "start_time": "2021-08-21T06:25:57.365831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on token shape: 192.168.1.1\n",
      "Matched based on token shape: 192.168.2.1\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "matcher.add(\"IP\", [nlp(\"127.0.0.1\"), nlp(\"127.127.0.0\")])\n",
    "\n",
    "doc = nlp(\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on token shape:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T06:46:59.771428Z",
     "start_time": "2021-08-21T06:46:58.644778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "荣耀 NOUN 发布\n",
      "发布 VERB 发布\n",
      "锐龙 NOUN 笔记本\n",
      "笔记本 NOUN 发布\n",
      "， PUNCT 配备\n",
      "显然 ADV 配备\n",
      "会 VERB 配备\n",
      "配备 VERB 发布\n",
      "7nm NUM 工艺\n",
      "工艺 NOUN 配备\n",
      "、 PUNCT Zen\n",
      "Zen NOUN 2\n",
      "2 NUM 架构\n",
      "架构 NOUN 锐龙\n",
      "的 PART 架构\n",
      "全新 ADJ 锐龙\n",
      "锐龙 NOUN 4000\n",
      "4000 NUM 配备\n",
      "系列 NUM 4000\n",
      "， PUNCT 发布\n",
      "但 ADV 采用\n",
      "具体 ADV 采用\n",
      "采用 VERB 发布\n",
      "低 ADJ 功耗\n",
      "功耗 NOUN 锐龙\n",
      "的 PART 功耗\n",
      "锐龙 NOUN U\n",
      "4000 NUM U\n",
      "U NOUN 系列\n",
      "系列 NOUN 采用\n",
      "， PUNCT 发布\n",
      "还是 CCONJ H\n",
      "高性能 NOUN 锐龙\n",
      "的 PART 高性能\n",
      "锐龙 NOUN H\n",
      "4000 NUM H\n",
      "H NUM 发布\n",
      "系列 NUM 发布\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"zh\" id=\"1c9353bffad9447990904b874bb04612-0\" class=\"displacy\" width=\"6000\" height=\"924.5\" direction=\"ltr\" style=\"max-width: none; height: 924.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">荣耀</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">发布</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">锐龙</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">笔记本，</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">显然</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">会</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">配备</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">7nm</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">工艺、</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">Zen</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">2</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">架构</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">的</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">全新</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">锐龙</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">4000</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">系列，</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">但</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">具体</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">采用</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">低</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">功耗</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">的</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">锐龙</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">4000</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">U</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4600\">系列，</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4600\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4775\">还是</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4775\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4950\">高性能</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5125\">的</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5125\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5300\">锐龙</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5300\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5475\">4000</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5475\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5650\">H</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5650\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"834.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5825\">系列</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5825\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-0\" stroke-width=\"2px\" d=\"M70,789.5 C70,702.0 185.0,702.0 185.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,791.5 L62,779.5 78,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-1\" stroke-width=\"2px\" d=\"M420,789.5 C420,702.0 535.0,702.0 535.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound:nn</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,791.5 L412,779.5 428,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-2\" stroke-width=\"2px\" d=\"M245,789.5 C245,614.5 540.0,614.5 540.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M540.0,791.5 L548.0,779.5 532.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-3\" stroke-width=\"2px\" d=\"M770,789.5 C770,614.5 1065.0,614.5 1065.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,791.5 L762,779.5 778,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-4\" stroke-width=\"2px\" d=\"M945,789.5 C945,702.0 1060.0,702.0 1060.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux:modal</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,791.5 L937,779.5 953,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-5\" stroke-width=\"2px\" d=\"M245,789.5 C245,439.5 1075.0,439.5 1075.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1075.0,791.5 L1083.0,779.5 1067.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-6\" stroke-width=\"2px\" d=\"M1295,789.5 C1295,702.0 1410.0,702.0 1410.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,791.5 L1287,779.5 1303,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-7\" stroke-width=\"2px\" d=\"M1120,789.5 C1120,614.5 1415.0,614.5 1415.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1415.0,791.5 L1423.0,779.5 1407.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-8\" stroke-width=\"2px\" d=\"M1645,789.5 C1645,702.0 1760.0,702.0 1760.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,791.5 L1637,779.5 1653,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-9\" stroke-width=\"2px\" d=\"M1820,789.5 C1820,702.0 1935.0,702.0 1935.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,791.5 L1812,779.5 1828,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-10\" stroke-width=\"2px\" d=\"M1995,789.5 C1995,527.0 2470.0,527.0 2470.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod:assmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,791.5 L1987,779.5 2003,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-11\" stroke-width=\"2px\" d=\"M1995,789.5 C1995,702.0 2110.0,702.0 2110.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2110.0,791.5 L2118.0,779.5 2102.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-12\" stroke-width=\"2px\" d=\"M2345,789.5 C2345,702.0 2460.0,702.0 2460.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,791.5 L2337,779.5 2353,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-13\" stroke-width=\"2px\" d=\"M2520,789.5 C2520,702.0 2635.0,702.0 2635.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,791.5 L2512,779.5 2528,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-14\" stroke-width=\"2px\" d=\"M1120,789.5 C1120,264.5 2660.0,264.5 2660.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod:range</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2660.0,791.5 L2668.0,779.5 2652.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-15\" stroke-width=\"2px\" d=\"M2695,789.5 C2695,702.0 2810.0,702.0 2810.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark:clf</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2810.0,791.5 L2818.0,779.5 2802.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-16\" stroke-width=\"2px\" d=\"M3045,789.5 C3045,614.5 3340.0,614.5 3340.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,791.5 L3037,779.5 3053,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-17\" stroke-width=\"2px\" d=\"M3220,789.5 C3220,702.0 3335.0,702.0 3335.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3220,791.5 L3212,779.5 3228,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-18\" stroke-width=\"2px\" d=\"M245,789.5 C245,177.0 3365.0,177.0 3365.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3365.0,791.5 L3373.0,779.5 3357.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-19\" stroke-width=\"2px\" d=\"M3570,789.5 C3570,702.0 3685.0,702.0 3685.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3570,791.5 L3562,779.5 3578,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-20\" stroke-width=\"2px\" d=\"M3745,789.5 C3745,614.5 4040.0,614.5 4040.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod:assmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3745,791.5 L3737,779.5 3753,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-21\" stroke-width=\"2px\" d=\"M3745,789.5 C3745,702.0 3860.0,702.0 3860.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3860.0,791.5 L3868.0,779.5 3852.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-22\" stroke-width=\"2px\" d=\"M4095,789.5 C4095,614.5 4390.0,614.5 4390.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound:nn</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4095,791.5 L4087,779.5 4103,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-23\" stroke-width=\"2px\" d=\"M4270,789.5 C4270,702.0 4385.0,702.0 4385.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4270,791.5 L4262,779.5 4278,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-24\" stroke-width=\"2px\" d=\"M4445,789.5 C4445,702.0 4560.0,702.0 4560.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound:nn</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4445,791.5 L4437,779.5 4453,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-25\" stroke-width=\"2px\" d=\"M3395,789.5 C3395,352.0 4580.0,352.0 4580.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4580.0,791.5 L4588.0,779.5 4572.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-26\" stroke-width=\"2px\" d=\"M4795,789.5 C4795,439.5 5625.0,439.5 5625.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-26\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4795,791.5 L4787,779.5 4803,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-27\" stroke-width=\"2px\" d=\"M4970,789.5 C4970,614.5 5265.0,614.5 5265.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-27\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod:assmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4970,791.5 L4962,779.5 4978,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-28\" stroke-width=\"2px\" d=\"M4970,789.5 C4970,702.0 5085.0,702.0 5085.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-28\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5085.0,791.5 L5093.0,779.5 5077.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-29\" stroke-width=\"2px\" d=\"M5320,789.5 C5320,614.5 5615.0,614.5 5615.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-29\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5320,791.5 L5312,779.5 5328,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-30\" stroke-width=\"2px\" d=\"M5495,789.5 C5495,702.0 5610.0,702.0 5610.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-30\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5495,791.5 L5487,779.5 5503,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-31\" stroke-width=\"2px\" d=\"M245,789.5 C245,89.5 5645.0,89.5 5645.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-31\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5645.0,791.5 L5653.0,779.5 5637.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1c9353bffad9447990904b874bb04612-0-32\" stroke-width=\"2px\" d=\"M245,789.5 C245,2.0 5825.0,2.0 5825.0,789.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1c9353bffad9447990904b874bb04612-0-32\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5825.0,791.5 L5833.0,779.5 5817.0,779.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(\"荣耀发布锐龙笔记本，显然会配备7nm工艺、Zen2 架构的\"\n",
    "          \"全新锐龙4000 系列，但具体采用低功耗的锐龙4000U 系列，还是高性能的锐龙4000H 系列\"\n",
    "         )\n",
    "\n",
    "for token in doc: \n",
    "    print(token.text,  token.pos_, token.head) \n",
    "\n",
    "spacy.displacy.render(doc, style='dep') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T06:41:40.350047Z",
     "start_time": "2021-08-21T06:41:40.346026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "us"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[3:6]\n",
    "span.root.head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T07:08:11.534552Z",
     "start_time": "2021-08-21T07:08:11.530974Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T07:18:32.424721Z",
     "start_time": "2021-08-21T07:18:32.421223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7fa45914ffc0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7fa45918eeb8>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7fa46c5e0ce0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7fa456c6d308>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7fa46c5e0a70>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-21T23:56:02.993379Z",
     "start_time": "2021-08-21T23:56:02.016023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner']\n",
      "This document is 4 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# 定义定制化组件\n",
    "@Language.component(name='length_component')\n",
    "def length_component(doc):\n",
    "    # 获取doc的长度\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # 返回这个doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# 读取小规模的中文模型\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# 将组件加入到流程的最前面，打印流程组件名\n",
    "nlp.add_pipe('length_component', first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# 处理一段文本\n",
    "doc = nlp(\"这是一个句子。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T00:19:07.834029Z",
     "start_time": "2021-08-22T00:19:06.705473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [金毛犬, 猫, 乌龟, 老鼠]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner', 'animal_component']\n",
      "[('猫', 6303828839600189595), ('金毛犬', 6303828839600189595)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "animals = [\"金毛犬\", \"猫\", \"乌龟\", \"老鼠\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# 定义定制化组件\n",
    "@Language.component(name='animal_component')\n",
    "def animal_component(doc):\n",
    "    # 把matcher应用到doc上\n",
    "    matches = matcher(doc)\n",
    "    # 为每一个匹配结果生成一个Span并赋予标签\"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label='ANIMAL') for match_id, start, end in matches]\n",
    "    # 用匹配到的span覆盖doc.ents\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# 把组件加入到流程中，紧跟在\"ner\"组件后面\n",
    "nlp.add_pipe('animal_component', after='ner')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# 处理文本，打印doc.ents的文本和标签\n",
    "doc = nlp(\"我养了一只猫和一条金毛犬。\")\n",
    "import spacyprint([(ent.text, ent.label) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T00:29:10.356817Z",
     "start_time": "2021-08-22T00:29:09.122237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - 蓝色\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.tokens import Token\n",
    "\n",
    "# 定义取值器函数\n",
    "def get_is_color(token):\n",
    "    colors = [\"红色\", \"黄色\", \"蓝色\"]\n",
    "    return token.text in colors\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# 为词符设置有取值器的扩展\n",
    "Token.set_extension(\"is_color\", getter=get_is_color)\n",
    "\n",
    "doc = nlp(\"天空是蓝色的。\")\n",
    "print(doc[2]._.is_color, \"-\", doc[2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T00:28:44.406242Z",
     "start_time": "2021-08-22T00:28:40.294554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "from spacy.tokens import Token\n",
    "\n",
    "# 为Token设置一个有默认值的扩展\n",
    "Token.set_extension(\"is_weather\", default=False)\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"今天回下雨。\")\n",
    "\n",
    "print(doc[2]._.is_weather )\n",
    "# 覆盖默认扩展特性的值\n",
    "doc[2]._.is_weather = True\n",
    "print(doc[2]._.is_weather )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T00:29:50.263686Z",
     "start_time": "2021-08-22T00:29:50.250479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - 是蓝色的\n",
      "False - 天空是\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# 定义取值器函数\n",
    "def get_has_color(span):\n",
    "    colors = [\"红色\", \"黄色\", \"蓝色\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# 为Span设置一个带有取值器getter的扩展\n",
    "Span.set_extension(\"has_color\", getter=get_has_color)\n",
    "\n",
    "doc = nlp(\"天空是蓝色的\")\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T00:31:14.304625Z",
     "start_time": "2021-08-22T00:31:14.292038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - 蓝色\n",
      "False - 云朵\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# 定义含有参数的方法\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# 在doc上设置方法扩展\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"天空是蓝色的。\")\n",
    "print(doc._.has_token(\"蓝色\"), \"- 蓝色\")\n",
    "print(doc._.has_token(\"云朵\"), \"- 云朵\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T00:47:04.984772Z",
     "start_time": "2021-08-22T00:47:03.853553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "周杰伦 https://zh.wikipedia.org/w/index.php?search=周杰伦\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # 如果span有其中一个标签则获取其维基百科URL\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://zh.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# 设置Span的扩展wikipedia_url及其取值器get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"出道这么多年，周杰伦已经成为几代年轻人共同的偶像。\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # 打印实体的文本和其维基百科URL\n",
    "    print(ent.text,  ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T01:43:18.548958Z",
     "start_time": "2021-08-22T01:43:18.482968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "[('中国', 'GPE', '北京'), ('新加坡', 'GPE', '新加坡'), ('马来西亚', 'GPE', '吉隆坡')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.zh import Chinese\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.language import Language\n",
    "\n",
    "COUNTRIES = ['阿富汗', '奥兰群岛', '阿尔巴尼亚', '阿尔及利亚', '美属萨摩亚', '安道尔', \n",
    "             '安哥拉', '安圭拉', '安提瓜和巴布达', '阿根廷', '亚美尼亚', '阿鲁巴', \n",
    "             '澳大利亚', '奥地利', '阿塞拜疆', '孟加拉', '巴林', '巴哈马', '巴巴多斯', \n",
    "             '白俄罗斯', '比利时', '伯利兹', '贝宁', '百慕大', '不丹', '玻利维亚', \n",
    "             '波斯尼亚和黑塞哥维那', '博茨瓦纳', '布维岛', '巴西', '文莱', '保加利亚', \n",
    "             '布基纳法索', '布隆迪', '柬埔寨', '喀麦隆', '加拿大', '佛得角', '中非', \n",
    "             '乍得', '智利', '圣诞岛', '科科斯群岛', '哥伦比亚', '科摩罗', '刚果', \n",
    "             '刚果', '库克群岛', '哥斯达黎加', '科特迪瓦', '中国', '克罗地亚', '古巴', \n",
    "             '捷克', '塞浦路斯', '丹麦', '吉布提', '多米尼加', '东帝汶', '厄瓜多尔', '埃及', \n",
    "             '赤道几内亚', '厄立特里亚', '爱沙尼亚', '埃塞俄比亚', '法罗群岛', '斐济', '法国', \n",
    "             '法国大都会', '法属圭亚那', '法属波利尼西亚', '加蓬', '冈比亚', '格鲁吉亚', '德国', \n",
    "             '加纳', '直布罗陀', '希腊', '格林纳达', '瓜德罗普岛', '关岛', '危地马拉', '根西岛', \n",
    "             '几内亚比绍', '几内亚', '圭亚那', '海地', '洪都拉斯', '匈牙利', '冰岛', '印度', \n",
    "             '印度尼西亚', '伊朗', '伊拉克', '爱尔兰', '马恩岛', '以色列', '意大利', '牙买加', \n",
    "             '日本', '泽西岛', '约旦', '哈萨克斯坦', '肯尼亚', '基里巴斯', '韩国', '朝鲜', \n",
    "             '科威特', '吉尔吉斯斯坦', '老挝', '拉脱维亚', '黎巴嫩', '莱索托', '利比里亚', \n",
    "             '利比亚', '列支敦士登', '立陶宛', '卢森堡', '马其顿', '马拉维', '马来西亚', \n",
    "             '马达加斯加', '马尔代夫', '马里', '马耳他', '马绍尔群岛', '马提尼克岛', \n",
    "             '毛里塔尼亚', '毛里求斯', '马约特', '墨西哥', '密克罗尼西亚', '摩尔多瓦', \n",
    "             '摩纳哥', '蒙古', '黑山', '蒙特塞拉特', '摩洛哥', '莫桑比克', '缅甸', \n",
    "             '纳米比亚', '瑙鲁', '尼泊尔', '荷兰', '新喀里多尼亚', '新西兰', '尼加拉瓜', \n",
    "             '尼日尔', '尼日利亚', '纽埃', '诺福克岛', '挪威', '阿曼', '巴基斯坦', '帕劳', \n",
    "             '巴勒斯坦', '巴拿马', '巴布亚新几内亚', '巴拉圭', '秘鲁', '菲律宾', '皮特凯恩群岛', \n",
    "             '波兰', '葡萄牙', '波多黎各', '卡塔尔', '留尼汪岛', '罗马尼亚', '卢旺达', '俄罗斯联邦', \n",
    "             '圣赫勒拿', '圣基茨和尼维斯', '圣卢西亚', '圣文森特和格林纳丁斯', '萨尔瓦多', '萨摩亚', \n",
    "             '圣马力诺', '圣多美和普林西比', '沙特阿拉伯', '塞内加尔', '塞舌尔', '塞拉利昂', '新加坡', \n",
    "             '塞尔维亚', '斯洛伐克', '斯洛文尼亚', '所罗门群岛', '索马里', '南非', '西班牙', '斯里兰卡', \n",
    "             '苏丹', '苏里南', '斯威士兰', '瑞典', '瑞士', '叙利亚', '塔吉克斯坦', '坦桑尼亚', '泰国', \n",
    "             '特立尼达和多巴哥', '东帝汶', '多哥', '托克劳', '汤加', '突尼斯', '土耳其', '土库曼斯坦', \n",
    "             '图瓦卢', '乌干达', '乌克兰', '阿拉伯联合酋长国', '英国', '美国', '乌拉圭', '乌兹别克斯坦', \n",
    "             '瓦努阿图', '梵蒂冈', '委内瑞拉', '越南', '瓦利斯群岛和富图纳群岛', '西撒哈拉', '也门', \n",
    "             '南斯拉夫', '赞比亚', '津巴布韦']\n",
    "\n",
    "CAPITALS = {'中国': '北京', '蒙古': '乌兰巴托', '朝鲜': '平壤', '韩国': '首尔', '日本': '东京', \n",
    "            '菲律宾': '马尼拉', '印度尼西亚': '雅加达', '文莱': '斯里巴加湾市', '新加坡': '新加坡', '泰国': '曼谷', \n",
    "            '马来西亚': '吉隆坡', '越南': '河内', '老挝': '万象', '柬埔寨': '金边', '缅甸': '内比都', '不丹': '廷布', \n",
    "            '东帝汶': '帝力', '尼泊尔': '加德满都', '印度': '新德里', '孟加拉国': '达卡', '斯里兰卡': '科伦坡', \n",
    "            '马尔代夫': '马累', '巴基斯坦': '伊斯兰堡', '阿富汗': '喀布尔', '塔吉克斯坦': '杜尚别', \n",
    "            '吉尔吉斯斯坦': '比什凯克', '哈萨克斯坦': '阿斯塔纳', '乌兹别克期坦': '塔什干', '土库曼斯坦': '阿什哈巴德', \n",
    "            '伊朗': '德黑兰', '伊拉克': '巴格达', '科威特': '科威特城', '卡塔尔': '多哈', '阿拉伯联合酋长国': '阿布扎比', \n",
    "            '巴林': '麦纳麦', '阿曼': '马斯喀特', '也门': '萨那', '沙特阿拉伯': '利雅得', '约旦': '安曼', \n",
    "            '巴勒斯坦': '耶路撒冷', '以色列': '耶路撒冷', '叙利亚': '大马士革', '黎巴嫩': '贝鲁特', '塞浦路斯': '尼科西亚', \n",
    "            '土耳其': '安卡拉', '阿塞拜疆': '巴库', '格鲁吉亚': '第比利斯', '亚美尼亚': '埃里温', '挪威': '奥斯路', \n",
    "            '冰岛': '雷克雅未克', '瑞典': '斯德哥尔摩', '芬兰': '赫尔辛基', '爱沙尼亚': '塔林', '拉脱维亚': '里加', \n",
    "            '立陶宛': '维尔纽斯', '白俄罗斯': '明斯克', '俄罗斯': '莫斯科', '乌克兰': '基辅', '摩尔多瓦': '基希讷乌', \n",
    "            '波兰': '华沙', '捷克': '布拉格', '斯洛伐克': '布拉提斯拉发', '匈牙利': '布达佩斯', '德国': '柏林', \n",
    "            '英国': '伦敦', '爱尔兰': '都柏林', '丹麦': '哥本哈根', '荷兰': '阿姆斯特丹', '摩纳哥': '摩纳哥', \n",
    "            '法国': '巴黎', '比利时': '布鲁塞尔', '卢森堡': '卢森堡', '奥地利': '维也纳', '瑞士': '伯尔尼', \n",
    "            '列支敦士登': '瓦杜兹', '西班牙': '马德里', '安道尔': '安道尔', '葡萄牙': '里斯本', '意大利': '罗马', \n",
    "            '马耳他': '瓦莱塔', '圣马力诺': '圣马力诺', '梵蒂冈': '梵蒂冈城', '斯洛文尼亚': '卢布尔雅那', \n",
    "            '克罗地亚': '萨格勒布', '波斯尼亚和黑塞哥维那': '萨拉热窝', '南斯拉夫': '贝尔格莱德', '马其顿': '斯科普里', \n",
    "            '阿尔巴尼亚': '地拉那', '罗马尼亚': '布加勒斯特', '保加利亚': '索非亚', '希腊': '雅典', '埃及': '开罗', \n",
    "            '苏丹': '喀土穆', '埃塞俄比亚': '亚的斯亚贝巴', '厄立特里亚': '阿斯马拉', '吉布提': '吉布提', \n",
    "            '索马里': '摩加迪沙', '利比亚': '的黎波里', '阿尔及利亚': '阿尔及尔', '突尼斯': '突尼斯', \n",
    "            '摩洛哥': '拉巴特', '佛得角': '普拉亚', '毛里塔尼亚': '努瓦克肖特', '马里': '巴马科', '塞内加尔': '达喀尔', \n",
    "            '冈比亚': '班珠尔', '几内亚比绍-': '比绍', '几内亚': '科纳克里', '塞拉利昂': '弗里敦', '利比里亚': '蒙罗维亚', \n",
    "            '科特迪瓦': '亚穆苏克罗', '布基纳法索': '瓦加杜古', '尼日尔': '尼亚美', '乍得': '恩贾梅纳', \n",
    "            '尼日利亚': '阿布贾', '加纳': '阿克拉', '多哥': '洛美', '贝宁': '波多诺伏-', '喀麦隆': '雅温得', \n",
    "            '加蓬': '利伯维尔', '赤道几内亚': '马拉博', '圣多美和普林西比': '圣多美', '中非': '班吉', \n",
    "            '刚果': '布拉柴维尔', '刚果民主共和国': '金沙萨', '乌干达': '坎帕拉', '卢旺达': '基加利', \n",
    "            '布隆迪': '布琼布拉', '坦桑尼亚': '多多马', '肯尼亚': '内罗华', '安哥拉': '罗安达', '赞比亚': '卢萨卡', \n",
    "            '马拉维': '利隆圭', '莫桑比克': '马普托', '马达加斯加': '塔那那利佛', '科摩罗': '莫罗尼', \n",
    "            '塞舌尔': '维多利亚', '毛里求斯': '路易港', '津巴布韦': '哈拉雷', '博茨瓦纳': '哈博罗内', '纳米比亚': \n",
    "            '温得和克', '斯威士兰': '姆巴巴内', '莱索托': '马塞卢', '南非': '比勒陀利亚', '澳大利亚': '堪培拉', \n",
    "            '巴布亚新几内亚': '莫尔兹比港', '所罗门群岛': '霍尼亚拉', '瓦努阿图': '维拉港-', '新西兰': '惠灵顿', \n",
    "            '斐济': '苏瓦', '汤加': '努库阿洛法', '瑙鲁': '亚伦', '基里巴斯': '塔拉瓦', '图瓦卢': '富纳富提', \n",
    "            '萨摩亚': '阿皮亚', '密克罗尼西亚联邦': '帕利基尔', '马绍尔群岛': '马朱罗', '帕劳': '科罗尔', \n",
    "            '加拿大': '渥太华', '美国': '华盛顿', '墨西哥': '墨西哥城', '危地马拉': '危地马拉', '伯利兹': '贝尔莫潘', \n",
    "            '萨尔瓦多': '圣萨尔瓦多', '洪都拉斯': '特古西加尔巴', '尼加拉瓜': '马那瓜', '哥斯达黎加': '圣何塞', \n",
    "            '巴拿马': '巴拿马城', '古巴': '哈瓦那', '巴哈马': '拿骚', '海地': '太子港--', '多米尼加共和国': '圣多明各', \n",
    "            '牙买加': '金斯敦', '圣基茨和尼维斯': '巴斯特尔', '安提瓜和巴布达': '圣约翰', '多米尼克': '罗索', \n",
    "            '圣卢西亚': '卡斯特里', '圣文森特和格林纳丁斯': '金斯敦', '格林纳达': '圣乔治', '巴巴多斯': '布里奇敦', \n",
    "            '特立尼达和多巴哥': '西班牙港--', '哥伦比亚': '波哥大', '厄瓜多尔': '基多', '委内瑞拉': '加拉加斯', \n",
    "            '圭亚那-': '乔治敦', '苏里南': '帕拉马里博', '秘鲁': '利马', '玻利维亚': '苏克雷', '巴拉圭': '亚松森', \n",
    "            '阿根廷': '布宜诺斯艾利斯', '乌拉圭': '蒙得维的亚', '巴西': '巴西利亚', '智利': '圣地亚哥'}\n",
    "\n",
    "nlp = Chinese()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "# matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "@Language.component(name='countries_component')\n",
    "def countries_component(doc):\n",
    "    # 对所有匹配结果创建一个标签为\"GPE\"的实体Span\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label='GPE') for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# 把这个组件加入到流程中\n",
    "#nlp.add_pipe(countries_component)\n",
    "nlp.add_pipe('countries_component')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# 取值器，在国家首都的字典中寻找span的文本\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# 用这个取值器注册Span的扩展属性\"capital\"\n",
    "Span.set_extension('capital', getter=get_capital, force=True) \n",
    "\n",
    "# 处理文本，打印实体文本、标签和首都属性\n",
    "doc = nlp(\"中国将邦族新加坡和马来西亚一起建造高铁。\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T01:59:37.084854Z",
     "start_time": "2021-08-22T01:59:36.068202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "['老']\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "\n",
    "TEXTS = ['我最喜欢吃性价比高的麦当劳了！', \n",
    "         '我以为麦当劳只有预处理的汉堡，现在我才发现他们家还有生的汉堡？？', \n",
    "         '为什么各位还在吃麦当劳 :(', '中国的麦当劳有老北京鸡肉卷，这也太爽了！', \n",
    "         '作为一个帅帅的男人，去麦当劳我只吃巨无霸:P', \n",
    "         '今天早上决定去吃麦当劳套餐，现在胃里涨了一整天了。']\n",
    "\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "# 处理文本，打印形容词\n",
    "for doc in docs:\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T02:54:58.107160Z",
     "start_time": "2021-08-22T02:54:57.931597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\n",
      " — 'Metamorphosis' by Franz Kafka\n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing.\n",
      " — 'Moby-Dick or, The Whale' by Herman Melville\n",
      "\n",
      "It was the best of times, it was the worst of times.\n",
      " — 'A Tale of Two Cities' by Charles Dickens\n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\n",
      " — 'On the Road' by Jack Kerouac\n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      " — '1984' by George Orwell\n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing.\n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "DATA = [['One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.', {'author': 'Franz Kafka', 'book': 'Metamorphosis'}], [\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\", {'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'}], ['It was the best of times, it was the worst of times.', {'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'}], ['The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.', {'author': 'Jack Kerouac', 'book': 'On the Road'}], ['It was a bright cold day in April, and the clocks were striking thirteen.', {'author': 'George Orwell', 'book': '1984'}], ['Nowadays people know the price of everything and the value of nothing.', {'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'}]]\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# 注册Doc的扩展\"author\"（默认值为None）\n",
    "Doc.set_extension('author', default=None, force=True) \n",
    "\n",
    "# 注册Doc的扩展\"book\"（默认值为None）\n",
    "Doc.set_extension('book', default=None, force=True) \n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # 从context中设置属性doc._.book和doc._.author\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "\n",
    "    # 打印文本和定制化的属性数据\n",
    "    print(f\"{doc.text}\\n — '{doc._.book}' by {doc._.author}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T02:57:32.345102Z",
     "start_time": "2021-08-22T02:57:31.356054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['在', '300多', '年', '的', '风雨', '历程', '中', '，', '历代', '同仁', '堂人', '始终', '恪守', '“', '炮制', '虽', '繁必', '不', '敢', '省', '人工', '，', '品味', '虽', '贵必', '不', '敢', '减物力', '”', '的', '古训', '，', '树立', '“', '修合', '无', '人', '见', '，', '存心', '有', '天知', '”', '的', '自律', '意识', '，', '造就', '了', '制药', '过程', '中', '兢兢小心', '、', '精益求精', '的', '严细', '精神', '。']\n",
      "['在', '300多', '年', '的', '风雨', '历程', '中', '，', '历代', '同仁', '堂人', '始终', '恪守', '“', '炮制', '虽', '繁必', '不', '敢', '省', '人工', '，', '品味', '虽', '贵必', '不', '敢', '减物力', '”', '的', '古训', '，', '树立', '“', '修合', '无', '人', '见', '，', '存心', '有', '天知', '”', '的', '自律', '意识', '，', '造就', '了', '制药', '过程', '中', '兢兢小心', '、', '精益求精', '的', '严细', '精神', '。']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "text = (\n",
    "    \"在300多年的风雨历程中，历代同仁堂人始终恪守“炮制虽繁必不敢省人工，品味虽贵必不敢减物力”的古训，\"\n",
    "    \"树立“修合无人见，存心有天知”的自律意识，造就了制药过程中兢兢小心、精益求精的严细精神。\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# 仅对文本做分词\n",
    "doc = nlp.make_doc(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T03:07:45.412625Z",
     "start_time": "2021-08-22T03:07:44.252188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'ner']\n",
      "(300多年,)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "text = (\n",
    "    \"在300多年的风雨历程中，历代同仁堂人始终恪守“炮制虽繁必不敢省人工，品味虽贵必不敢减物力”的古训，\"\n",
    "    \"树立“修合无人见，存心有天知”的自律意识，造就了制药过程中兢兢小心、精益求精的严细精神。\"\n",
    ")\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(text)\n",
    "print(doc.ents)\n",
    "    \n",
    "# 关闭tagger和parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\", 'ner'):\n",
    "    # 处理文本\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T03:09:01.408060Z",
     "start_time": "2021-08-22T03:09:00.255726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('在', 'ADP'), ('300多', 'NUM'), ('年', 'NUM'), ('的', 'PART'), ('风雨', 'NOUN'), ('历程', 'NOUN'), ('中', 'PART'), ('，', 'PUNCT'), ('历代', 'VERB'), ('同仁', 'NOUN'), ('堂人', 'NOUN'), ('始终', 'ADV'), ('恪守', 'VERB'), ('“', 'PUNCT'), ('炮制', 'NOUN'), ('虽', 'SCONJ'), ('繁必', 'NUM'), ('不', 'ADV'), ('敢', 'VERB'), ('省', 'VERB'), ('人工', 'VERB'), ('，', 'PUNCT'), ('品味', 'NOUN'), ('虽', 'SCONJ'), ('贵必', 'VERB'), ('不', 'ADV'), ('敢', 'VERB'), ('减物力', 'NOUN'), ('”', 'PUNCT'), ('的', 'PART'), ('古训', 'NOUN'), ('，', 'PUNCT'), ('树立', 'VERB'), ('“', 'PUNCT'), ('修合', 'VERB'), ('无', 'VERB'), ('人', 'NOUN'), ('见', 'VERB'), ('，', 'PUNCT'), ('存心', 'ADV'), ('有', 'VERB'), ('天知', 'NOUN'), ('”', 'PUNCT'), ('的', 'PART'), ('自律', 'NOUN'), ('意识', 'NOUN'), ('，', 'PUNCT'), ('造就', 'VERB'), ('了', 'PART'), ('制药', 'NOUN'), ('过程', 'NOUN'), ('中', 'PART'), ('兢兢小心', 'VERB'), ('、', 'PUNCT'), ('精益求精', 'VERB'), ('的', 'PART'), ('严细', 'NOUN'), ('精神', 'NOUN'), ('。', 'PUNCT')]\n",
      "[('在', ''), ('300多', ''), ('年', ''), ('的', ''), ('风雨', ''), ('历程', ''), ('中', ''), ('，', ''), ('历代', ''), ('同仁', ''), ('堂人', ''), ('始终', ''), ('恪守', ''), ('“', ''), ('炮制', ''), ('虽', ''), ('繁必', ''), ('不', ''), ('敢', ''), ('省', ''), ('人工', ''), ('，', ''), ('品味', ''), ('虽', ''), ('贵必', ''), ('不', ''), ('敢', ''), ('减物力', ''), ('”', ''), ('的', ''), ('古训', ''), ('，', ''), ('树立', ''), ('“', ''), ('修合', ''), ('无', ''), ('人', ''), ('见', ''), ('，', ''), ('存心', ''), ('有', ''), ('天知', ''), ('”', ''), ('的', ''), ('自律', ''), ('意识', ''), ('，', ''), ('造就', ''), ('了', ''), ('制药', ''), ('过程', ''), ('中', ''), ('兢兢小心', ''), ('、', ''), ('精益求精', ''), ('的', ''), ('严细', ''), ('精神', ''), ('。', '')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "text = (\n",
    "    \"在300多年的风雨历程中，历代同仁堂人始终恪守“炮制虽繁必不敢省人工，品味虽贵必不敢减物力”的古训，\"\n",
    "    \"树立“修合无人见，存心有天知”的自律意识，造就了制药过程中兢兢小心、精益求精的严细精神。\"\n",
    ")\n",
    "\n",
    "doc = nlp(text)\n",
    "print([(token.text, token.pos_) for token in doc])\n",
    "\n",
    "# 仅对文本做分词\n",
    "doc = nlp.make_doc(text)\n",
    "print([(token.text, token.pos_) for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T04:07:29.942361Z",
     "start_time": "2021-08-22T04:07:28.796802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('如何预定 iPhone X', {'entities': [(5, 13, 'GADGET')]})\n",
      "('iPhone X就要来了', {'entities': [(0, 8, 'GADGET')]})\n",
      "('为买一个iPhone X花上万块钱值得吗？', {'entities': [(4, 12, 'GADGET')]})\n",
      "('iPhone 8的评测出来了', {'entities': [(0, 8, 'GADGET')]})\n",
      "('iPhone 11 vs iPhone 8：有哪些升级？', {'entities': [(0, 9, 'GADGET'), (13, 21, 'GADGET')]})\n",
      "('我急需一部新手机，给点建议吧！', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "TEXTS = ['如何预定 iPhone X', \n",
    "         'iPhone X就要来了', \n",
    "         '为买一个iPhone X花上万块钱值得吗？', \n",
    "         'iPhone 8的评测出来了', \n",
    "         'iPhone 11 vs iPhone 8：有哪些升级？', \n",
    "         '我急需一部新手机，给点建议吧！']\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# 两个词符，其小写形式匹配到\"iphone\"和\"x\"上\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# 词符的小写形式匹配到\"iphone\"和一个数字上\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# 把模板加入到matcher中然后检查结果\n",
    "matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # 在doc上做匹配，创建一个匹配结果span的列表\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # 获取(start character, end character, label)这样的匹配结果元组\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # 将匹配结果的格式变为(doc.text, entities)元组\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # 把这些例子加入到训练数据中\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    \n",
    "print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 循环重复10次\n",
    "for i in range(10):\n",
    "    # 随机化训练数据的顺序\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # 创建批次并遍历\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "    # 在该批数据中分离文本和标注\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # 更新模型\n",
    "        nlp.update(texts, annotations)\n",
    "\n",
    "# 保存模型\n",
    "nlp.to_disk(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T04:51:07.241815Z",
     "start_time": "2021-08-22T04:51:07.089682Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-b0a06444d524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 训练10个循环\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 将例子切分为一系列批次\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'examples' is not defined"
     ]
    }
   ],
   "source": [
    "# 从空的中文模型开始\n",
    "nlp = spacy.blank(\"zh\")\n",
    "# 创建一个空的实体识别器加入到流程中\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe('ner')\n",
    "# 添加一个新的标签\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# 开始训练\n",
    "nlp.begin_training()\n",
    "# 训练10个循环\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    # 将例子切分为一系列批次\n",
    "    for batch in spacy.util.minibatch(examples, size=2):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # 更新模型\n",
    "        nlp.update(texts, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T07:45:32.451328Z",
     "start_time": "2021-08-22T07:45:31.793098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 {'ner': 42.49999928474426}\n",
      "2/10 {'ner': 68.98857140541077}\n",
      "3/10 {'ner': 71.35534292459488}\n",
      "4/10 {'ner': 68.32567793130875}\n",
      "5/10 {'ner': 63.826959669589996}\n",
      "6/10 {'ner': 57.02356398105621}\n",
      "7/10 {'ner': 47.38814955949783}\n",
      "8/10 {'ner': 33.9421991109848}\n",
      "9/10 {'ner': 21.177328035235405}\n",
      "10/10 {'ner': 11.303111843764782}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "from spacy.training import Example\n",
    "    \n",
    "TRAINING_DATA = [['如何预定iPhone X', {'entities': [[4, 12, 'GADGET']]}], \n",
    "                 ['iPhone X就要来了', {'entities': [[0, 8, 'GADGET']]}], \n",
    "                 ['为买一个iPhone X花上万块钱值得吗？', {'entities': [[4, 12, 'GADGET']]}], \n",
    "                 ['iPhone 8的评测出来了', {'entities': [[0, 8, 'GADGET']]}], \n",
    "                 ['最新的iPhone已经到第11代了', {'entities': [[3, 9, 'GADGET']]}], \n",
    "                 ['我急需一部新手机，给点建议吧！', {'entities': []}]]\n",
    "\n",
    "nlp = spacy.blank(\"zh\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(\"ner\")\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "examples = []\n",
    "for text, annots in TRAINING_DATA:    \n",
    "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "    \n",
    "nlp.initialize(lambda: examples)\n",
    "\n",
    "# 开始训练\n",
    "nlp.begin_training()\n",
    "\n",
    "# 迭代10个循环\n",
    "epochs = 10  \n",
    "for itn in range(epochs):\n",
    "    losses = {}\n",
    "    random.shuffle(examples)\n",
    "    for batch in spacy.util.minibatch(examples, size=8):\n",
    "        nlp.update(batch, losses=losses) \n",
    "    print(f'{itn+1}/{epochs}', losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T07:45:51.678608Z",
     "start_time": "2021-08-22T07:45:51.670209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('最新的iPhone已经到第11代了, 你好，中国 ')\n",
    "print(doc.ents)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T08:07:05.811899Z",
     "start_time": "2021-08-22T08:07:04.512623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('哔哩', 0), ('哔哩', 2), ('与', 4), ('阿里巴巴', 5), ('合作', 9), ('为', 11), ('博主们', 12), ('建立', 15), ('社群', 17)]\n",
      "[('李子柒', 0), ('打破', 3), ('了', 5), ('Youtube', 6), ('的', 13), ('记录', 14)]\n",
      "[('阿里巴巴', 0), ('的', 4), ('创始人', 5), ('马云', 8), ('提供', 10), ('了', 12), ('一千万', 13), ('购物', 16), ('优惠券', 18)]\n",
      "--------------------------------------------------\n",
      "0 4 WEBSITE\n",
      "哔哩哔哩\n",
      "5 9 WEBSITE\n",
      "阿里巴巴\n",
      "--------------------------------------------------\n",
      "6 13 WEBSITE\n",
      "Youtube\n",
      "--------------------------------------------------\n",
      "0 4 WEBSITE\n",
      "阿里巴巴\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"哔哩哔哩与阿里巴巴合作为博主们建立社群\",\n",
    "        {\"entities\": [(0, 4, \"WEBSITE\"), (5, 9, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\"李子柒打破了Youtube的记录\", {\"entities\": [(6, 13, \"WEBSITE\")]}),\n",
    "    (\n",
    "        \"阿里巴巴的创始人马云提供了一千万购物优惠券\",\n",
    "        {\"entities\": [(0, 4, \"WEBSITE\")]},\n",
    "    ),\n",
    "    # 以及更多类似的数据。。。\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "texts = [data[0] for data in TRAINING_DATA]\n",
    "for doc in nlp.pipe(texts):\n",
    "    print([(token.text, token.idx) for token in doc])\n",
    "\n",
    "for text, entities in TRAINING_DATA:\n",
    "    print('-'*50) \n",
    "    doc = nlp(text) \n",
    "    for start, end, label in entities[\"entities\"]:\n",
    "        print(start, end, label) \n",
    "        print(doc.char_span(start, end, label=label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-22T23:59:19.198076Z",
     "start_time": "2021-08-22T23:59:17.827632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('如何预定 iPhone X', {'entities': [(5, 13, 'GADGET')]})\n",
      "('iPhone X就要来了', {'entities': [(0, 8, 'GADGET')]})\n",
      "('为买一个iPhone X花上万块钱值得吗？', {'entities': [(4, 12, 'GADGET')]})\n",
      "('iPhone 8的评测出来了', {'entities': [(0, 8, 'GADGET')]})\n",
      "('iPhone 11 vs iPhone 8：有哪些升级？', {'entities': [(0, 9, 'GADGET'), (13, 21, 'GADGET')]})\n",
      "('我急需一部新手机，给点建议吧！', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def get_train_data():\n",
    "    TEXTS = ['如何预定 iPhone X', \n",
    "             'iPhone X就要来了', \n",
    "             '为买一个iPhone X花上万块钱值得吗？', \n",
    "             'iPhone 8的评测出来了', \n",
    "             'iPhone 11 vs iPhone 8：有哪些升级？', \n",
    "             '我急需一部新手机，给点建议吧！']\n",
    "\n",
    "    nlp = spacy.load(\"zh_core_web_sm\")\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # 两个词符，其小写形式匹配到\"iphone\"和\"x\"上\n",
    "    pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "    # 词符的小写形式匹配到\"iphone\"和一个数字上\n",
    "    pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "    TRAIN_DATA = []\n",
    "\n",
    "    # 把模板加入到matcher中然后检查结果\n",
    "    matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "    for doc in nlp.pipe(TEXTS):\n",
    "        # 在doc上做匹配，创建一个匹配结果span的列表\n",
    "        spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "        # 获取(start character, end character, label)这样的匹配结果元组\n",
    "        entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "        # 将匹配结果的格式变为(doc.text, entities)元组\n",
    "        training_example = (doc.text, {\"entities\": entities})\n",
    "        # 把这些例子加入到训练数据中\n",
    "        TRAIN_DATA.append(training_example)\n",
    "\n",
    "    print(*TRAIN_DATA, sep=\"\\n\")    \n",
    "    return TRAIN_DATA\n",
    "\n",
    "TRAIN_DATA = get_train_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T00:04:33.294028Z",
     "start_time": "2021-08-23T00:04:33.099291Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 1300.42it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "db = DocBin() # create a DocBin object\n",
    "\n",
    "for text, annot in tqdm(TRAIN_DATA): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is not None:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"./train.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T07:06:54.946254Z",
     "start_time": "2021-08-24T07:06:54.940943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.choice(a=5, size=3, replace=True, p=[0.2, 0.1, 0.3, 0.4, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "154px",
    "width": "230px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
