{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba==0.39\n",
      "  Downloading https://files.pythonhosted.org/packages/71/46/c6f9179f73b818d5827202ad1c4a94e371a29473b7f043b736b4dab6b8cd/jieba-0.39.zip (7.3MB)\n",
      "\u001b[K    100% |################################| 7.3MB 186kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Running setup.py bdist_wheel for jieba ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c9/c7/63/a9ec0322ccc7c365fd51e475942a82395807186e94f0522243\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.39\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting keras==2.1.6\n",
      "  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
      "\u001b[K    100% |################################| 348kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py (from keras==2.1.6)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/0c/1c5dfa85e05052aa5f50969d87c67a2128dc39a6f8ce459a503717e56bd0/h5py-2.8.0-cp27-cp27mu-manylinux1_x86_64.whl (2.7MB)\n",
      "\u001b[K    100% |################################| 2.7MB 481kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.9.1 (from keras==2.1.6)\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/16/1134977cc35d2f72dbe80efa75a8e989ac21289f8e7e2c9005444cd17cd5/numpy-1.15.1-cp27-cp27mu-manylinux1_x86_64.whl (13.8MB)\n",
      "\u001b[K    100% |################################| 13.8MB 86kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting scipy>=0.14 (from keras==2.1.6)\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/f3/de9c1bd16311982711209edaa8c6caa962db30ebb6a8cc6f1dcd2d3ef616/scipy-1.1.0-cp27-cp27mu-manylinux1_x86_64.whl (30.8MB)\n",
      "\u001b[K    100% |################################| 30.8MB 45kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from keras==2.1.6)\n",
      "Collecting pyyaml (from keras==2.1.6)\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/a3/1d13970c3f36777c583f136c136f804d70f500168edc1edea6daa7200769/PyYAML-3.13.tar.gz (270kB)\n",
      "\u001b[K    100% |################################| 276kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
      "  Running setup.py bdist_wheel for pyyaml ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ad/da/0c/74eb680767247273e2cf2723482cb9c924fe70af57c334513f\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: numpy, h5py, scipy, pyyaml, keras\n",
      "  Found existing installation: numpy 1.14.0\n",
      "    Uninstalling numpy-1.14.0:\n",
      "      Successfully uninstalled numpy-1.14.0\n",
      "  Found existing installation: h5py 2.7.1\n",
      "    Uninstalling h5py-2.7.1:\n",
      "      Successfully uninstalled h5py-2.7.1\n",
      "  Found existing installation: scipy 1.0.0\n",
      "    Uninstalling scipy-1.0.0:\n",
      "      Successfully uninstalled scipy-1.0.0\n",
      "Successfully installed h5py-2.8.0 keras-2.1.6 numpy-1.15.1 pyyaml-3.13 scipy-1.1.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sklearn==0.19.1\n",
      "\u001b[31m  Could not find a version that satisfies the requirement sklearn==0.19.1 (from versions: 0.0)\u001b[0m\n",
      "\u001b[31mNo matching distribution found for sklearn==0.19.1\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting gensim==3.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/57/dc00a059b1b739c71dd25355541ebe141ce1ba31917671c826c5fcdfd145/gensim-3.4.0-cp27-cp27mu-manylinux1_x86_64.whl (22.6MB)\n",
      "\u001b[K    100% |################################| 22.6MB 61kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: scipy>=0.18.1 in /usr/local/lib/python2.7/dist-packages (from gensim==3.4.0)\n",
      "Requirement already up-to-date: numpy>=1.11.3 in /usr/local/lib/python2.7/dist-packages (from gensim==3.4.0)\n",
      "Collecting smart-open>=1.2.1 (from gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
      "Requirement already up-to-date: six>=1.5.0 in /usr/local/lib/python2.7/dist-packages (from gensim==3.4.0)\n",
      "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |################################| 1.4MB 1.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Collecting requests (from smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
      "\u001b[K    100% |################################| 92kB 5.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3 (from smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/38/aa/c100449bc8550c52b3f14e3b3831c2b5db6a4b7b752b7a11e76599508fc4/boto3-1.9.8-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |################################| 133kB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/df/f7/04fee6ac349e915b82171f8e23cee63644d83663b34c539f7a09aed18f9e/certifi-2018.8.24-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K    100% |################################| 153kB 5.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests->smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |################################| 143kB 4.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.24,>=1.21.1 (from requests->smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |################################| 143kB 3.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.8,>=2.5 (from requests->smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K    100% |################################| 61kB 6.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |################################| 61kB 5.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting botocore<1.13.0,>=1.12.8 (from boto3->smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/2d/a6fff67b40094667c806af91ec8b360a694df73afe2f8846115cbf47124d/botocore-1.12.8-py2.py3-none-any.whl (4.7MB)\n",
      "\u001b[K    100% |################################| 4.7MB 217kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from s3transfer<0.2.0,>=0.1.10->boto3->smart-open>=1.2.1->gensim==3.4.0)\n",
      "Collecting python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" (from botocore<1.13.0,>=1.12.8->boto3->smart-open>=1.2.1->gensim==3.4.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/f5/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825/python_dateutil-2.7.3-py2.py3-none-any.whl (211kB)\n",
      "\u001b[K    100% |################################| 215kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils>=0.10 (from botocore<1.13.0,>=1.12.8->boto3->smart-open>=1.2.1->gensim==3.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://files.pythonhosted.org/packages/50/09/c53398e0005b11f7ffb27b7aa720c617aba53be4fb4f4f3f06b9b5c60f28/docutils-0.14-py2-none-any.whl (543kB)\n",
      "\u001b[K    100% |################################| 552kB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: boto, bz2file, certifi, chardet, urllib3, idna, requests, jmespath, python-dateutil, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "  Found existing installation: certifi 2018.1.18\n",
      "    Uninstalling certifi-2018.1.18:\n",
      "      Successfully uninstalled certifi-2018.1.18\n",
      "  Found existing installation: python-dateutil 2.6.1\n",
      "    Uninstalling python-dateutil-2.6.1:\n",
      "      Successfully uninstalled python-dateutil-2.6.1\n",
      "Successfully installed boto-2.49.0 boto3-1.9.8 botocore-1.12.8 bz2file-0.98 certifi-2018.8.24 chardet-3.0.4 docutils-0.14 gensim-3.4.0 idna-2.7 jmespath-0.9.3 python-dateutil-2.7.3 requests-2.19.1 s3transfer-0.1.13 smart-open-1.7.1 urllib3-1.23\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pytorch==0.4.0\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pytorch==0.4.0 (from versions: 0.1.2)\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pytorch==0.4.0\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade jieba==0.39 --proxy http://web-proxy.rose.hp.com:8080  \n",
    "!pip install --upgrade keras==2.1.6 --proxy http://web-proxy.rose.hp.com:8080  \n",
    "!pip install --upgrade sklearn==0.19.1 --proxy http://web-proxy.rose.hp.com:8080   \n",
    "!pip install --upgrade gensim==3.4.0 --proxy http://web-proxy.rose.hp.com:8080  \n",
    "!pip install --upgrade pytorch==0.4.0 --proxy http://web-proxy.rose.hp.com:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.6\n",
      "0.22.0\n",
      "1.15.1\n",
      "0.19.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.286 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import sklearn\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers as L\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(keras.__version__)\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(sklearn.__version__)\n",
    "\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n",
      "ANSI_X3.4-1968\n",
      "(None, None)\n",
      "None\n",
      "UTF-8\n"
     ]
    }
   ],
   "source": [
    "import locale, sys\n",
    "print(sys.getdefaultencoding())    #系统默认编码\n",
    "print(sys.getfilesystemencoding()) #文件系统编码\n",
    "print(locale.getdefaultlocale())   #系统当前编码\n",
    "print(sys.stdin.encoding)          #终端输入编码\n",
    "print(sys.stdout.encoding)         #终端输出编码\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 加载数据和分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102477\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process(inpath, outpath):\n",
    "    with open(inpath, 'r') as fin, open(outpath, 'w') as fout:\n",
    "        for line in fin:\n",
    "            lineno, sen1, sen2 = line.strip().split('\\t')\n",
    "            words1= [ w for w in jieba.cut(sen1) if w.strip() ]\n",
    "            words2= [ w for w in jieba.cut(sen2) if w.strip() ]\n",
    "            union = words1 + words2\n",
    "            same_num = 0\n",
    "            for w in union:\n",
    "                if w in words1 and w in words2:\n",
    "                    same_num += 1\n",
    "            if same_num * 2 >= len(union):\n",
    "                fout.write(lineno + '\\t1\\n')\n",
    "            else:\n",
    "                fout.write(lineno + '\\t0\\n')\n",
    "\n",
    "\n",
    "def load_data(filepath):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        i = 0 \n",
    "        for line in f:\n",
    "            lineno, sent1, sent2, tag = line.strip().split('\\t')\n",
    "            data.append((lineno, sent1, sent2, tag))\n",
    "    return data      \n",
    "\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    return [w for w in jieba.cut(sentence) if w.strip() ]\n",
    "\n",
    "# def split_sentence(sentence):\n",
    "#     return [w for w in sentence.decode('utf-8') if w.strip() ]\n",
    "\n",
    "\n",
    "def segmentation(data, split=split_sentence, userwords_filepath='words.txt', stupwords_filepath='stopwords.txt'):\n",
    "    jieba.load_userdict(userwords_filepath) \n",
    "    new_data = []\n",
    "    for  (lineno, sen1, sen2, tag) in data:\n",
    "        words1 = split(sen1)\n",
    "        words2 = split(sen2)\n",
    "        new_data.append((lineno, words1, words2, tag))\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# 创建停用词list  \n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]  \n",
    "    return stopwords  \n",
    "\n",
    "\n",
    "# 对句子进行分词  \n",
    "def remove_stopwords(data, ):  \n",
    "    sentence_seged = jieba.cut(sentence.strip())  \n",
    "    stopwords = stopwordslist('./test/stopwords.txt')  # 这里加载停用词的路径  \n",
    "    outstr = ''  \n",
    "    for word in sentence_seged:  \n",
    "        if word not in stopwords:  \n",
    "            if word != '\\t':  \n",
    "                outstr += word  \n",
    "                outstr += \" \"  \n",
    "    return outstr  \n",
    "\n",
    "\n",
    "def print_unicode(text):\n",
    "    print(repr(text).decode('unicode_escape'))\n",
    "    \n",
    "def print_utf8(text):\n",
    "    print(repr(text).decode('string_escape'))    \n",
    "\n",
    "\n",
    "data_raw = load_data('./data/atec_nlp_sim_train.csv') + load_data('./data/atec_nlp_sim_train_add.csv')\n",
    "print(len(data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('怎么更改花呗手机号码', '我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号')\n",
      "u'\\u600e'\n",
      "u'\\u4e48'\n",
      "u'\\u66f4'\n",
      "u'\\u6539'\n",
      "u'\\u82b1'\n",
      "u'\\u5457'\n",
      "u'\\u624b'\n",
      "u'\\u673a'\n",
      "u'\\u53f7'\n",
      "u'\\u7801'\n",
      "('也开不了花呗，就这样了？完事了', '真的嘛？就是花呗付款')\n",
      "u'\\u4e5f'\n",
      "u'\\u5f00'\n",
      "u'\\u4e0d'\n",
      "u'\\u4e86'\n",
      "u'\\u82b1'\n",
      "u'\\u5457'\n",
      "u'\\uff0c'\n",
      "u'\\u5c31'\n",
      "u'\\u8fd9'\n",
      "u'\\u6837'\n",
      "u'\\u4e86'\n",
      "u'\\uff1f'\n",
      "u'\\u5b8c'\n",
      "u'\\u4e8b'\n",
      "u'\\u4e86'\n"
     ]
    }
   ],
   "source": [
    "for  (lineno, sen1, sen2, tag) in data_raw[0:2]:\n",
    "    print_utf8((sen1, sen2))\n",
    "    for word in sen1.decode('utf-8'):\n",
    "        print_utf8(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = segmentation(data_raw)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ï»¿1', [u'怎么', u'更改', u'花呗', u'手机号码'], [u'我', u'的', u'花呗', u'是', u'以前', u'的', u'手机号码', u'，', u'怎么', u'更', u'改成', u'现在', u'的', u'支付宝', u'的', u'号码', u'手机号'], '1')\n",
      "('4', '如何得知关闭借呗', '想永久关闭借呗', '0')\n",
      "[u'如何', u'得知', u'关闭', u'借呗']\n"
     ]
    }
   ],
   "source": [
    "#去除字符乱码\n",
    "print_unicode(data[0])   \n",
    "data[0]=('1', data[0][1], data[0][2], data[0][3])\n",
    "\n",
    "print_utf8(data_raw[3])\n",
    "print_unicode(data[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13420, 102477)\n",
      "[(u'花呗', 149045), (u'我', 61737), (u'借呗', 61340), (u'的', 60190), (u'*', 50301), (u'了', 47471), (u'，', 46908), (u'吗', 42196), (u'还', 35112), (u'怎么', 33715), (u'还款', 31820), (u'可以', 28454), (u'蚂蚁', 28414), (u'用', 26203), (u'为什么', 25282), (u'额度', 21894), (u'是', 17433), (u'分期', 17117), (u'开通', 17104), (u'不能', 12605), (u'钱', 12563), (u'能', 12168), (u'没有', 12015), (u'有', 11703), (u'不', 11692), (u'什么', 11164), (u'使用', 10375), (u'在', 9932), (u'支付宝', 9045), (u'不了', 8260), (u'付款', 7615), (u'支付', 7530), (u'月', 7407), (u'到', 7167), (u'多少', 7021), (u'逾期', 6793), (u'显示', 6453), (u'提前', 6266), (u'会', 6247), (u'现在', 6225), (u'没', 5603), (u'银行卡', 5451), (u'退款', 5396), (u'后', 5376), (u'余额', 5344), (u'时候', 5280), (u'收款', 5151), (u'自动', 5109), (u'申请', 5082), (u'关闭', 4805), (u'这个', 4795), (u'要', 4740), (u'和', 4634), (u'借', 4609), (u'如何', 4541), (u'么', 4417), (u'已经', 4378), (u'怎么办', 4354), (u'一个', 4122), (u'都', 4110), (u'账单', 3917), (u'买', 3851), (u'淘宝', 3801), (u'影响', 3739), (u'临时', 3681), (u'想', 3673), (u'账号', 3611), (u'上', 3600), (u'还清', 3596), (u'就', 3560), (u'还是', 3501), (u'号', 3435), (u'利息', 3280), (u'绑定', 3188), (u'信用卡', 2974), (u'里', 2971), (u'支持', 2890), (u'把', 2882), (u'但是', 2873), (u'信用', 2855), (u'手续费', 2853), (u'给', 2733), (u'取消', 2726), (u'商家', 2665), (u'扣款', 2640), (u'扣', 2617), (u'收钱', 2589), (u'码', 2556), (u'日', 2499), (u'恢复', 2499), (u'东西', 2488), (u'分', 2475), (u'完', 2461), (u'退', 2430), (u'需要', 2404), (u'怎样', 2401), (u'个', 2390), (u'手机', 2371), (u'最低', 2370), (u'是不是', 2354)]\n",
      "('three-occur word: ', 847)\n",
      "[u'星期一', u'谈', u'知', u'应为', u'配置', u'不知去向', u'餐', u'修好', u'七月', u'点事']\n",
      "('two-occur word: ', 1690)\n",
      "[u'有发', u'美团网', u'平安', u'为主', u'收嘛', u'用电', u'首笔', u'两万块', u'纸', u'应付']\n",
      "('one-occur word: ', 6421)\n",
      "[u'归整', u'付会收', u'填完', u'门票', u'过日子', u'记不清', u'脸才行', u'边网', u'选泽', u'电网']\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter()\n",
    "for (_, sent1, sent2, _) in data:   \n",
    "    word_counts.update(sent1)\n",
    "    word_counts.update(sent2)\n",
    "print(len(word_counts), len(data)) \n",
    "print_unicode(word_counts.most_common(100))\n",
    "\n",
    "low_freq_words = [word for word, count in word_counts.items() if count==3] \n",
    "print('three-occur word: ',len(low_freq_words))  #8960  total:13419\n",
    "print_unicode(low_freq_words[0:10])\n",
    "\n",
    "low_freq_words = [word for word, count in word_counts.items() if count==2] \n",
    "print('two-occur word: ',len(low_freq_words))  #1690  total:13419\n",
    "print_unicode(low_freq_words[0:10])\n",
    "\n",
    "low_freq_words = [word for word, count in word_counts.items() if count==1] \n",
    "print('one-occur word: ', len(low_freq_words))  #6423  total:13419\n",
    "print_unicode(low_freq_words[0:10])\n",
    "\n",
    "one_freq_words = low_freq_words\n",
    "\n",
    "#分词的一些问题：  ('我用', 2051)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面结果可以看出，低频词特别多，其中只出现一次的词高达6423，几乎占了所有词汇的一半以上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据分成train, validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102477, 102477)\n",
      "(71733, 71733)\n",
      "(30744, 30744)\n",
      "([u'我', u'的', u'来', u'分期', u'评估', u'和', u'支付宝', u'评估', u'两者', u'是', u'一样', u'的', u'评估', u'么'], [u'我用', u'支付宝', u'买', u'东西', u'也', u'蛮', u'多', u'的'])\n"
     ]
    }
   ],
   "source": [
    "X_all = [(sent1, sent2)  for (_, sent1, sent2, _) in data] \n",
    "y_all = np.array([tag  for (_, _, _, tag) in data]).astype(int)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "print(len(X_all), len(y_all))\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_val), len(y_val))\n",
    "print_unicode(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max length =', 90)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa795f3dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sents_all = [sent for (sent, _) in X_all] + [sent for (_, sent) in X_all]    \n",
    "MAX_LENGTH = max(map(len, sents_all))\n",
    "print(\"max length =\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.xticks(np.linspace(0,48,13,endpoint=True))\n",
    "plt.hist(list(map(len,sents_all)),bins=25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图可以看出，大部分的句子长度在8个长度左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one-occur word: ', 5492)\n",
      "[u'只得', u'有发', u'填完', u'过日子', u'二立', u'选泽', u'银子', u'吸', u'号通', u'禁言']\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter()\n",
    "for (sent1, sent2) in X_train:   \n",
    "    word_counts.update(sent1)\n",
    "    word_counts.update(sent2)\n",
    "    \n",
    "low_freq_words = [word for word, count in word_counts.items() if count==1] \n",
    "print('one-occur word: ', len(low_freq_words))  #6423  total:13419\n",
    "print_unicode(low_freq_words[0:10])\n",
    "\n",
    "one_freq_words = low_freq_words   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102477,)\n",
      "[[    0 83792]\n",
      " [    1 18685]]\n"
     ]
    }
   ],
   "source": [
    "print(y_all.shape)\n",
    "y_bin = np.bincount(y_all)\n",
    "ii = np.nonzero(y_bin)[0]\n",
    "print(np.vstack((ii,y_bin[ii])).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的结果来看数据的倾斜非常厉害。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 矩阵化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "PAD = \"#PAD#\"\n",
    "UNK = \"#UNK#\"\n",
    "START = \"#START#\"\n",
    "END = \"#END#\"\n",
    "\n",
    "\n",
    "def generate_vocabulary(X_train, min_occur_count=5):    \n",
    "    words = [ word for (sent, _)  in X_train for word in sent  ] + [ word for (_, sent)  in X_train for word in sent  ] \n",
    "    vocab = Counter(words)\n",
    "    vocab = [PAD, UNK, START, END] + [word for word, count in vocab.items() if count>=min_occur_count ] \n",
    "    return {token: index for index, token in enumerate(sorted(vocab))}\n",
    "\n",
    "def tokens_to_indices(X, vocab):    \n",
    "    matrix_1 = [[vocab.get(word, vocab[UNK]) for word in sent] for (sent, _)  in X] \n",
    "    matrix_2 = [[vocab.get(word, vocab[UNK]) for word in sent] for (_, sent)  in X]     \n",
    "    return matrix_1, matrix_2\n",
    "\n",
    "def to_matrix(X_indexed, pad=0, max_len=None, dtype='int32'):\n",
    "    if max_len  is None:\n",
    "        max_len = max(map(len, X_indexed[0]) + map(len, X_indexed[1]))\n",
    "    else:\n",
    "        max_len = min(max_len, max(map(len, X_indexed[0]) + map(len, X_indexed[1])))    \n",
    "        \n",
    "    max_len = max_len or max(map(len, X_indexed[0]) + map(len, X_indexed[1]))\n",
    "    matrix1 = np.empty([len(X_indexed[0]),max_len],dtype)\n",
    "    matrix2 = np.empty([len(X_indexed[1]),max_len],dtype)\n",
    "    matrix1.fill(pad)\n",
    "    matrix2.fill(pad)\n",
    "    \n",
    "    for i in range(len(X_indexed[0])):\n",
    "        line_ix = X_indexed[0][i][:max_len]\n",
    "        matrix1[i,:len(line_ix)] = line_ix\n",
    "        \n",
    "    for i in range(len(X_indexed[1])):\n",
    "        line_ix = X_indexed[1][i][:max_len]\n",
    "        matrix2[i,:len(line_ix)] = line_ix    \n",
    "    return matrix1, matrix2    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = generate_vocabulary(X_train, min_occur_count=1)\n",
    "vocab_inverse = {idx: w for w, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11372\n",
      "[u'有变', u'付会收', u'星期一', u'有发', u'出来']\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print_unicode(list(vocab)[0:5])\n",
    "# print(vocab[PAD], vocab[UNK], vocab[START], vocab[END], vocab[u\"花呗\"], vocab[u\"借呗\"], vocab[u\"开通\"], vocab[u\"支付宝\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index化。\n",
    "X_train_indexed = tokens_to_indices(X_train, vocab)\n",
    "X_val_indexed = tokens_to_indices(X_val, vocab)\n",
    "X_sample_indexed = tokens_to_indices(X_train[0:3], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "[u'我', u'的', u'来', u'分期', u'评估', u'和', u'支付宝', u'评估', u'两者', u'是', u'一样', u'的', u'评估', u'么']\n",
      "[u'我用', u'支付宝', u'买', u'东西', u'也', u'蛮', u'多', u'的']\n",
      "[u'我', u'的', u'来', u'分期', u'评估', u'和', u'支付宝', u'评估', u'两者', u'是', u'一样', u'的', u'评估', u'么']\n",
      "[u'我用', u'支付宝', u'买', u'东西', u'也', u'蛮', u'多', u'的']\n",
      "-----------------------------------------\n",
      "[u'身份证', u'过期', u'借呗', u'是不是', u'额度', u'没', u'办法', u'用']\n",
      "[u'我', u'的', u'身份证', u'过期', u'了', u'为什么', u'不能', u'办', u'蚂蚁', u'借呗']\n",
      "[u'身份证', u'过期', u'借呗', u'是不是', u'额度', u'没', u'办法', u'用']\n",
      "[u'我', u'的', u'身份证', u'过期', u'了', u'为什么', u'不能', u'办', u'蚂蚁', u'借呗']\n",
      "-----------------------------------------\n",
      "[u'借呗', u'提前', u'还款', u'后', u'没', u'额度', u'什么', u'原因']\n",
      "[u'借呗', u'提前', u'还款', u'没有', u'额度', u'了', u'，', u'都', u'好几个', u'月', u'都', u'出不来', u'了']\n",
      "[u'借呗', u'提前', u'还款', u'后', u'没', u'额度', u'什么', u'原因']\n",
      "[u'借呗', u'提前', u'还款', u'没有', u'额度', u'了', u'，', u'都', u'好几个', u'月', u'都', u'出不来', u'了']\n",
      "=========================================\n",
      "[u'只得', u'有发', u'填完', u'过日子', u'二立', u'选泽', u'银子', u'吸', u'号通', u'禁言']\n",
      "=========================================\n",
      "-----------------------------------------\n",
      "[5585, 8309, 7237, 2647, 9633, 3717, 6410, 9633, 950, 6854, 409, 8309, 9633, 1073]\n",
      "[5701, 6410, 1135, 887, 1123, 9370, 4097, 8309]\n",
      "[5585, 8309, 7237, 2647, 9633, 3717, 6410, 9633, 950, 6854, 409, 8309]\n",
      "[5701, 6410, 1135, 887, 1123, 9370, 4097, 8309, 3, 3, 3, 3]\n",
      "-----------------------------------------\n",
      "[10079, 10204, 1946, 6855, 11208, 7568, 2884, 8114]\n",
      "[5585, 8309, 10079, 10204, 1187, 1035, 812, 2876, 9362, 1946]\n",
      "[10079, 10204, 1946, 6855, 11208, 7568, 2884, 8114, 3, 3, 3, 3]\n",
      "[5585, 8309, 10079, 10204, 1187, 1035, 812, 2876, 9362, 1946, 3, 3]\n",
      "-----------------------------------------\n",
      "[1946, 6302, 10301, 3592, 7568, 11208, 1417, 3217]\n",
      "[1946, 6302, 10301, 7644, 11208, 1187, 11359, 10699, 4349, 6970, 10699, 2560, 1187]\n",
      "[1946, 6302, 10301, 3592, 7568, 11208, 1417, 3217, 3, 3, 3, 3]\n",
      "[1946, 6302, 10301, 7644, 11208, 1187, 11359, 10699, 4349, 6970, 10699, 2560]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(X_sample_indexed[0])):\n",
    "    print('-----------------------------------------')\n",
    "    sent1 = [vocab_inverse[index] for index in X_sample_indexed[0][i]]\n",
    "    sent2 = [vocab_inverse[index] for index in X_sample_indexed[1][i]]\n",
    "    print_unicode(sent1)\n",
    "    print_unicode(sent2)\n",
    "    print_unicode(X_train[i][0])\n",
    "    print_unicode(X_train[i][1])\n",
    "  \n",
    "    \n",
    "#检查一下，是否一些低频词被替换成#UNK#。\n",
    "def filter_by_word(X, word):\n",
    "    indices = [i for i,(sent1, sent2) in enumerate(X) if word in sent1 or word in sent2]\n",
    "    return indices\n",
    "\n",
    "def show_words(X_indexed, indice):\n",
    "    sent1 = [ vocab_inverse[i] for i in X_indexed[0][indice]] \n",
    "    sent2 = [ vocab_inverse[i] for i in X_indexed[1][indice]] \n",
    "    print_unicode((sent1, sent2))\n",
    "\n",
    "print('=========================================')  \n",
    "print_unicode(one_freq_words[0:10])  # [u'只得', u'有发', u'填完', u'过日子', u'二立', u'选泽', u'银子', u'吸', u'号通', u'禁言']\n",
    "# indice = filter_by_word(X_train, u'过日子')[0]\n",
    "# print_unicode(X_train[indice])\n",
    "# show_words(X_train_indexed, indice)\n",
    "# print(X_train_indexed[0][indice], X_train_indexed[1][indice])\n",
    "\n",
    "print('=========================================') \n",
    "\n",
    "\n",
    "X_train_seq = to_matrix(X_train_indexed, max_len=12, pad=vocab[PAD])\n",
    "X_val_seq = to_matrix(X_val_indexed, max_len=12, pad=vocab[PAD])\n",
    "X_sample_seq = to_matrix(X_sample_indexed, max_len=12, pad=vocab[PAD])\n",
    "\n",
    "\n",
    "for i in range(len(X_sample_indexed[0])):\n",
    "    print('-----------------------------------------')\n",
    "    sent1 = [index for index in X_sample_indexed[0][i]]\n",
    "    sent2 = [index for index in X_sample_indexed[1][i]]\n",
    "    sent11 = [index for index in X_sample_seq[0][i]]\n",
    "    sent21 = [index for index in X_sample_seq[1][i]]    \n",
    "    print_unicode(sent1)\n",
    "    print_unicode(sent2)\n",
    "    print_unicode(sent11)\n",
    "    print_unicode(sent21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 BOW(bag-of-words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u6211 \\u7684 \\u6765 \\u5206\\u671f \\u8bc4\\u4f30 \\u548c \\u652f\\u4ed8\\u5b9d \\u8bc4\\u4f30 \\u4e24\\u8005 \\u662f \\u4e00\\u6837 \\u7684 \\u8bc4\\u4f30 \\u4e48', u'\\u8eab\\u4efd\\u8bc1 \\u8fc7\\u671f \\u501f\\u5457 \\u662f\\u4e0d\\u662f \\u989d\\u5ea6 \\u6ca1 \\u529e\\u6cd5 \\u7528', u'\\u501f\\u5457 \\u63d0\\u524d \\u8fd8\\u6b3e \\u540e \\u6ca1 \\u989d\\u5ea6 \\u4ec0\\u4e48 \\u539f\\u56e0', u'\\u82b1\\u5457 \\u90fd \\u662f \\u53ef\\u4ee5 \\u7528', u'\\u82b1\\u5457 \\u7528 \\u4e0d\\u4e86', u'\\u6211 \\u5df2\\u7ecf \\u8fd8 \\u5b8c \\u82b1\\u5457 \\u4e86 \\uff0c \\u90a3 \\u706b\\u8f66\\u7968 \\u9000\\u6b3e \\u53bb \\u54ea\\u91cc \\u4e86', u'\\u501f\\u5457 \\u6700 \\u665a \\u591a\\u957f\\u65f6\\u95f4 \\u8fd8', u'\\u5f00\\u901a \\u4e86 \\u4fe1\\u7528\\u5361 \\u6536\\u6b3e \\u600e\\u4e48 \\u8fd8\\u662f \\u4e0d\\u80fd \\u7528', u'\\u6211 \\u7684 \\u82b1\\u5457 \\u65e0\\u6cd5 \\u626b\\u7801 \\u652f\\u4ed8 \\uff0c \\u6240\\u6709 \\u7684 \\u5e97\\u94fa \\u90fd \\u65e0\\u6cd5 \\u652f\\u4ed8', u'\\u6211 \\u82b1\\u5457 \\u6ca1\\u6709 \\u4e34\\u65f6 \\u989d\\u5ea6 \\u538b']\n"
     ]
    }
   ],
   "source": [
    "def tfidf_vectorize(X_train):\n",
    "    corpus = [\" \".join(sent) for (sent, _) in X_train] + [\" \".join(sent) for (_, sent) in X_train]\n",
    "    print(corpus[0:10])\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    vectorizer.fit(corpus) \n",
    "    return vectorizer\n",
    "\n",
    "def tfidf_to_matrix(X, vectorizer):\n",
    "    corpus_list1 = [\" \".join(sent) for (sent, _) in X]\n",
    "    corpus_list2 = [\" \".join(sent) for (_, sent) in X]\n",
    "    matrix_1 = vectorizer.transform(corpus_list1)\n",
    "    matrix_2 = vectorizer.transform(corpus_list2)\n",
    "    return matrix_1, matrix_2\n",
    "\n",
    "vectorizer= tfidf_vectorize(X_train)  \n",
    "X_train_tfidf = tfidf_to_matrix(X_train, vectorizer)\n",
    "X_val_tfidf = tfidf_to_matrix(X_val, vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u'我', u'的', u'来', u'分期', u'评估', u'和', u'支付宝', u'评估', u'两者', u'是', u'一样', u'的', u'评估', u'么'], [u'我用', u'支付宝', u'买', u'东西', u'也', u'蛮', u'多', u'的'])\n",
      "([u'我', u'的', u'来', u'分期', u'评估', u'和', u'支付宝', u'评估', u'两者', u'是', u'一样', u'的', u'评估', u'么'], [u'我用', u'支付宝', u'买', u'东西', u'也', u'蛮', u'多', u'的'])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output: \n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_object(filename):        \n",
    "    with open(filename, 'rb') as input:\n",
    "        obj = pickle.load(input)        \n",
    "    return obj\n",
    "        \n",
    "save_object(X_train, 'X_train.pkl')\n",
    "\n",
    "X_train_load  = load_object('X_train.pkl')\n",
    "print_unicode(X_train[0])\n",
    "print_unicode(X_train_load[0])\n",
    "\n",
    "# file_X_train = open('X_train.obj', 'w')\n",
    "# pickle.dump(X_train, file_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((71733, 11297), (71733, 11297))\n",
      "((30744, 11297), (30744, 11297))\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0].shape, X_train_tfidf[1].shape)\n",
    "print(X_val_tfidf[0].shape, X_val_tfidf[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------row 0------------------------\n",
      "sent1: 我的来分期评估和支付宝评估两者是一样的评估么 \n",
      "sent2: 我用支付宝买东西也蛮多的 \n",
      "y = 0\n",
      "tfidf1: 一样=0.22444348, 两者=0.42605797, 么=0.17509544, 分期=0.12893940, 和=0.17351256, 我=0.08470678, 支付宝=0.15173236, 是=0.12693644, 来=0.24532790, 的=0.17120391, 评估=0.74491743\n",
      "tfidf2: 东西=0.31149912, 也=0.31587926, 买=0.28584534, 多=0.33230920, 我用=0.32171005, 支付宝=0.23985517, 的=0.13531768, 蛮=0.65704863\n",
      "------------------------row 400------------------------\n",
      "sent1: 花呗有没有像信用卡那样最后还款日后三天还都不算逾期，也没费用 \n",
      "sent2: 我花呗逾期了，现在工资还没发，能不能宽限几天 \n",
      "y = 0\n",
      "tfidf1: 三天=0.31012129, 不算=0.32751496, 也=0.20227695, 信用卡=0.19136015, 像=0.33376436, 日后=0.39104723, 最后=0.26988303, 有没有=0.24449551, 没=0.16910407, 花呗=0.04964531, 费用=0.24412517, 还=0.10588056, 还款=0.10698801, 逾期=0.16214070, 那样=0.38718825, 都=0.18048036\n",
      "tfidf2: 不能=0.17940157, 了=0.12281289, 几天=0.31638647, 宽限=0.46113720, 工资=0.50057764, 我=0.11085712, 没发=0.48137780, 现在=0.21368438, 能=0.18146917, 花呗=0.06418411, 还=0.13688804, 逾期=0.20962416\n",
      "------------------------row 800------------------------\n",
      "sent1: 蚂蚁借呗借的钱都第二天了怎么还不到 \n",
      "sent2: 借呗怎么不能提现了 \n",
      "y = 0\n",
      "tfidf1: 不到=0.39807179, 了=0.17160442, 借=0.31950524, 借呗=0.14644700, 怎么=0.18605908, 的=0.15653578, 第二天=0.61847769, 蚂蚁=0.19765248, 还=0.19127138, 都=0.32603463, 钱=0.25490106\n",
      "tfidf2: 不能=0.43019295, 了=0.29449710, 借呗=0.25132345, 怎么=0.31930330, 提现=0.75039150\n",
      "------------------------row 1200------------------------\n",
      "sent1: 我现在需要咨询花呗转移 \n",
      "sent2: 现在企业支付宝无法使用花呗 \n",
      "y = 0\n",
      "tfidf1: 咨询=0.59290004, 我=0.16725050, 现在=0.32238632, 花呗=0.09683478, 转移=0.59590853, 需要=0.38997427\n",
      "tfidf2: 企业=0.68357614, 使用=0.30661307, 支付宝=0.31986699, 无法=0.45522536, 现在=0.34420609, 花呗=0.10338876\n",
      "------------------------row 1600------------------------\n",
      "sent1: 我花呗什么时候提额的 \n",
      "sent2: 花呗怎么一直不提高额度 \n",
      "y = 0\n",
      "tfidf1: 什么=0.42302499, 我=0.25249093, 提额=0.64519304, 时候=0.50452668, 的=0.25515924, 花呗=0.14618733\n",
      "tfidf2: 一直=0.56815388, 不=0.35572641, 怎么=0.25803269, 提高=0.61518058, 花呗=0.12437581, 额度=0.30027078\n",
      "------------------------row 2000------------------------\n",
      "sent1: 我这个月明明已经还了最低还款，为什么还要扣钱 \n",
      "sent2: 花呗还款了为什么还叫我还款 \n",
      "y = 0\n",
      "tfidf1: 为什么=0.18805603, 了=0.15733951, 已经=0.29404526, 我=0.14202258, 扣钱=0.39282168, 明明=0.47310696, 最低=0.33071419, 月=0.26927628, 还=0.17537162, 还款=0.17720591, 还要=0.35511260, 这个=0.29148485\n",
      "tfidf2: 为什么=0.28006851, 了=0.23432293, 叫=0.67831663, 我=0.21151171, 花呗=0.12246116, 还=0.26117784, 还款=0.52781923\n",
      "------------------------row 2400------------------------\n",
      "sent1: 蚂蚁借呗逾期后会自动扣款吗 \n",
      "sent2: 蚂蚁借呗是自动扣钱吗 \n",
      "y = 0\n",
      "tfidf1: 会=0.38950395, 借呗=0.19206705, 后=0.40357645, 吗=0.22438529, 扣款=0.46532349, 自动=0.40851179, 蚂蚁=0.25922368, 逾期=0.38414770\n",
      "tfidf2: 借呗=0.22483962, 吗=0.26267235, 扣钱=0.65777677, 是=0.35637617, 自动=0.47821652, 蚂蚁=0.30345525\n",
      "------------------------row 2800------------------------\n",
      "sent1: 为什么我的花呗帮定不了银行卡 \n",
      "sent2: 为什么我银行卡已经绑定了花呗然后回到花呗首页还是显示要绑定银行卡 \n",
      "y = 0\n",
      "tfidf1: 不了=0.32369173, 为什么=0.23771501, 帮定=0.79667104, 我=0.17952575, 的=0.18142297, 花呗=0.10394191, 银行卡=0.35788655\n",
      "tfidf2: 为什么=0.13554055, 了=0.11340175, 回到=0.38929960, 已经=0.21193182, 我=0.10236215, 显示=0.19527293, 然后=0.27969751, 绑定=0.45985070, 花呗=0.11853138, 要=0.20920554, 还是=0.22180652, 银行卡=0.40812013, 首页=0.40052442\n",
      "------------------------row 3200------------------------\n",
      "sent1: 去超市使用花呗最多可以付多钱 \n",
      "sent2: 花呗可以到超市有吗 \n",
      "y = 0\n",
      "tfidf1: 付多=0.64454660, 使用=0.21270732, 去=0.31736483, 可以=0.15824193, 多=0.30743583, 最=0.36471614, 花呗=0.07172410, 超市=0.37325304, 钱=0.20385655\n",
      "tfidf2: 到=0.43496364, 可以=0.29660328, 吗=0.25646576, 有=0.38668723, 花呗=0.13443720, 超市=0.69961280\n",
      "------------------------row 3600------------------------\n",
      "sent1: ***积分怎么借不了，借呗 \n",
      "sent2: 借呗以授权为什么借不了 \n",
      "y = 0\n",
      "tfidf1: 不了=0.44035576, 借=0.50376582, 借呗=0.23090386, 怎么=0.29336046, 积分=0.64259618\n",
      "tfidf2: 不了=0.33387173, 为什么=0.24519107, 以=0.61784089, 借=0.38194837, 借呗=0.17506815, 授权=0.51975979\n",
      "------------------------row 4000------------------------\n",
      "sent1: 南充可以用花呗加油吗 \n",
      "sent2: 花呗可以滴滴打车支付吗 \n",
      "y = 0\n",
      "tfidf1: 加油=0.52849745, 南充=0.77653499, 可以=0.19721090, 吗=0.17052355, 用=0.20426305, 花呗=0.08938702\n",
      "tfidf2: 可以=0.24931174, 吗=0.21557390, 打车=0.62829303, 支付=0.36460500, 滴滴=0.59237923, 花呗=0.11300203\n",
      "------------------------row 4400------------------------\n",
      "sent1: 使用借呗会影响以后贷款买房吗 \n",
      "sent2: 借呗会影响证信吗 \n",
      "y = 0\n",
      "tfidf1: 买房=0.59187931, 以后=0.39838412, 会=0.28844986, 使用=0.25832133, 借呗=0.14223659, 吗=0.16617009, 影响=0.32067361, 贷款=0.43630314\n",
      "tfidf2: 会=0.32684586, 借呗=0.16116992, 吗=0.18828924, 影响=0.36335896, 证信=0.83648877\n",
      "------------------------row 4800------------------------\n",
      "sent1: 查询蚂蚁花呗分多少期 \n",
      "sent2: 花呗分期分了***期 \n",
      "y = 1\n",
      "tfidf1: 分=0.46625468, 多少=0.37555531, 期=0.50791075, 查询=0.55192025, 花呗=0.11611903, 蚂蚁=0.25591375\n",
      "tfidf2: 了=0.27961573, 分=0.58676604, 分期=0.38419235, 期=0.63918882, 花呗=0.14613195\n",
      "------------------------row 5200------------------------\n",
      "sent1: 我本期还款最低还款花呗算逾期吗 \n",
      "sent2: 花呗可以每个月还最低还款嘛 \n",
      "y = 0\n",
      "tfidf1: 吗=0.17456366, 我=0.15804471, 最低=0.36802337, 本期=0.63103857, 算=0.39707087, 花呗=0.09150481, 还款=0.39439442, 逾期=0.29885306\n",
      "tfidf2: 可以=0.24378863, 嘛=0.47266730, 最低=0.44441472, 月=0.36185428, 每个=0.51405841, 花呗=0.11049865, 还=0.23566491, 还款=0.23812983\n",
      "------------------------row 5600------------------------\n",
      "sent1: 花呗里的钱在淘宝上怎么用 \n",
      "sent2: 我在淘宝买东西怎么使用花呗 \n",
      "y = 1\n",
      "tfidf1: 上=0.43762865, 在=0.34902515, 怎么=0.24207979, 淘宝=0.42941399, 用=0.26664601, 的=0.20366730, 花呗=0.11668626, 里=0.45549543, 钱=0.33164948\n",
      "tfidf2: 东西=0.48262002, 买=0.44287343, 使用=0.35622043, 在=0.35928449, 怎么=0.24919556, 我=0.20746151, 淘宝=0.44203631, 花呗=0.12011617\n"
     ]
    }
   ],
   "source": [
    "#查看tfidf后的结果\n",
    "def show_sentence(X, y, X_tfidf, words, items=range(10), y_pred=None, a_pred=None):\n",
    "    for i in items:\n",
    "        print('------------------------row %d------------------------' % (i))\n",
    "        print(\"sent1: {} \\nsent2: {} \".format(''.join(X[i][0]), ''.join(X[i][1])) )\n",
    "        if y_pred is None:\n",
    "            print(\"y = {}\".format(y[i]))\n",
    "        else:\n",
    "            print(\"y = {}, y_pred = {}, a_pred= {}\".format(y[i], y_pred[i], a_pred[i]))\n",
    "        sent1_tfidf, sent2_tfidf = [], []\n",
    "        for j in range(len(words)):\n",
    "            if X_tfidf[0][i,j] >= 1e-10: sent1_tfidf.append(words[j] + \"=\" + '{:.8f}'.format(X_tfidf[0][i,j]))\n",
    "            if X_tfidf[1][i,j] >= 1e-10: sent2_tfidf.append(words[j] + \"=\" + '{:.8f}'.format(X_tfidf[1][i,j]))\n",
    "        print('tfidf1: {}'.format(', '.join(sent1_tfidf)))\n",
    "        print('tfidf2: {}'.format(', '.join(sent2_tfidf)))\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "show_sentence(X_train, y_train,  X_train_tfidf, words, items=np.arange(0, 6000, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_indices(y_index, batch_size):\n",
    "    indices = np.random.permutation(np.arange(len(y_index)))\n",
    "    return list(y_index[indices[0:batch_size]] )\n",
    "\n",
    "#尝试随机的得到label比例是1：1的batch数据， 不知道这样是覅偶可以有做帮助数据倾斜的问题。\n",
    "#看来这招还挺有用，val f1很快从0.25提高到0.36\n",
    "#random_sent 无论是true, false, 对结果影响不是很大。\n",
    "def generate_batches_tfidf(X, y, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True):  \n",
    "    while True:\n",
    "        y_index_1 = np.array([i for i,label in enumerate(y) if label==1])\n",
    "        y_index_0 = np.array([i for i,label in enumerate(y) if label==0])\n",
    "\n",
    "        y_index_1_batch_size = int(batch_size*positive_ratio)\n",
    "        y_index_0_batch_size = batch_size - y_index_1_batch_size\n",
    "        \n",
    "        for start in range(0,len(y)-1,batch_size):\n",
    "            index = np.random.permutation(np.arange(2)) if random_sent else np.arange(2)\n",
    "            batch_indices = get_indices(y_index_1, y_index_1_batch_size) + get_indices(y_index_0, y_index_0_batch_size)\n",
    "            yield [X[index[0]][batch_indices].toarray(), X[index[1]][batch_indices].toarray()], y[batch_indices]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是自己写的一个求f1_score, precison, recall, accuracy的类，应该可以用sklearn里面的相同功能替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X):\n",
    "        A = self.model.predict(X)\n",
    "        return self.predict_(A)\n",
    "\n",
    "    def predict_(self, A):\n",
    "        return A\n",
    "    \n",
    "\n",
    "class ClassificationPredictor(Predictor):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        Predictor.__init__(self, model)\n",
    "\n",
    "    def predict_(self, A):\n",
    "        if A.shape[1]== 1:\n",
    "            return np.int32(A > 0.5).flatten()\n",
    "        else: \n",
    "            return  np.argmax(A, axis=-1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return self.accuracy_(y_pred, y)\n",
    "\n",
    "    def accuracy_(self, y_pred, y):\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "    def evaluate(self, X, y, title=\"\"):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = self.accuracy_(y_pred, y)\n",
    "\n",
    "        metrics = []\n",
    "\n",
    "        tp = np.sum((y_pred==1) * (y==1))*1.0\n",
    "        fn = np.sum((y_pred==0) * (y==1))*1.0\n",
    "        fp = np.sum((y_pred==1) * (y==0))*1.0\n",
    "        tn = np.sum((y_pred==0) * (y==0))*1.0\n",
    "\n",
    "        recall = tp/(tp+fn) if tp>0 else 0\n",
    "        precision = tp/(tp+fp) if tp>0 else 0\n",
    "        specificity = tn/(tn+fp) if tn>0 else 0\n",
    "        f1 = 2*recall*precision/(recall+precision) if recall+precision>0 else 0\n",
    "\n",
    "        metrics.append([accuracy, recall, precision, specificity, f1, tp, fn, fp, tn])\n",
    "\n",
    "        metrics = pd.DataFrame(metrics, index=[title],\n",
    "                               columns=['accuracy', 'recall', 'precision', 'specificity', 'f1',  'tp', 'fn', 'fp', 'tn'])\n",
    "        return metrics\n",
    "\n",
    "    def print_metrics(self, train_X, train_y, dev_X=None, dev_y=None, test_X=None, test_y=None):\n",
    "        metrics = []\n",
    "        if train_X is not None:\n",
    "            metrics_ = self.evaluate(train_X, train_y)\n",
    "            metrics_.index=[\"train\"]\n",
    "            metrics.append(metrics_)\n",
    "        if dev_X is not None:\n",
    "            metrics_ = self.evaluate(dev_X, dev_y)\n",
    "            metrics_.index = [\"dev\"]\n",
    "            metrics.append(metrics_)\n",
    "        if test_X is not None:\n",
    "            metrics_ = self.evaluate(test_X, test_y)\n",
    "            metrics_.index = [\"test\"]\n",
    "            metrics.append(metrics_)\n",
    "        print(pd.concat(metrics))\n",
    "\n",
    "    def print_accuracy(self, X, y, title=\"train\"):\n",
    "        print(\"{} accuracy: {}\".format(title, self.accuracy(X, y)))\n",
    "\n",
    "    def print_accuracy_train_test(self, train_X, train_y, dev_X=None, dev_y=None, test_X=None, test_y=None):\n",
    "        if train_X is not None:\n",
    "            self.print_accuracy(train_X, train_y, \"train\")\n",
    "        if dev_X is not None:\n",
    "            self.print_accuracy(dev_X, dev_y, \"dev\")\n",
    "        if test_X is not None:\n",
    "            self.print_accuracy(test_X, test_y, \"test\")\n",
    "            \n",
    "def accuracy1(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precision1 = precision(y_true, y_pred)\n",
    "    recall1 = recall(y_true, y_pred)\n",
    "    return 2*((precision1*recall1)/(precision1+recall1+K.epsilon()))\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 BOW + NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "\n",
    "    input_shape = X_train_tfidf[0].shape[1]\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.Dense(512, input_shape=(input_shape,)))\n",
    "    model_sent1.add(L.Activation('relu')) \n",
    "    model_sent1.add(L.Dropout(0.1))\n",
    "    model_sent1.add(L.Dense(128))\n",
    "    model_sent1.add(L.Activation('relu')) \n",
    "    model_sent1.add(L.Dropout(0.1))\n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.Dense(512, input_shape=(input_shape,)))\n",
    "    model_sent2.add(L.Activation('relu')) \n",
    "    model_sent2.add(L.Dropout(0.1))\n",
    "    model_sent2.add(L.Dense(128))\n",
    "    model_sent2.add(L.Activation('relu')) \n",
    "    model_sent2.add(L.Dropout(0.1))\n",
    "\n",
    "    input1 = L.Input(shape=(input_shape, ))\n",
    "    input2 = L.Input(shape=(input_shape, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    X = L.concatenate([encoder1, encoder2])\n",
    "    X = L.Dense(64)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dropout(0.1)(X)\n",
    "    X = L.Dense(32)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dropout(0.1)(X)\n",
    "    predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_accuracy(model):\n",
    "    #predict tag probabilities of shape [batch,time,n_tags]\n",
    "    predicted_tag_probabilities = model.predict(X_val_tfidf, verbose=1)\n",
    "\n",
    "    #compute accurary excluding padding\n",
    "    numerator = np.sum(predicted_tags == y_val)\n",
    "    denominator = len(y_val)\n",
    "    return float(numerator)/denominator\n",
    "\n",
    "\n",
    "class EvaluateAccuracy(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        sys.stdout.flush()\n",
    "        print(\"\\nMeasuring validation accuracy...\")\n",
    "        acc = compute_test_accuracy(self.model)\n",
    "        print(\"\\nValidation accuracy: %.5f\\n\" % acc)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "(71733, 11297)\n",
      "((71733, 11297), (71733, 11297))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 11297)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 11297)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          5850240     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          5850240     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           2080        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,719,041\n",
      "Trainable params: 11,719,041\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 71733 samples, validate on 30744 samples\n",
      "Epoch 1/5\n",
      "71733/71733 [==============================] - 23s 322us/step - loss: 0.4734 - acc: 0.8164 - recall: 2.7452e-04 - precision: 1.4870e-04 - f1: 1.9291e-04 - val_loss: 0.4585 - val_acc: 0.8201 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_f1: 0.0000e+00\n",
      "Epoch 2/5\n",
      "71733/71733 [==============================] - 23s 318us/step - loss: 0.4635 - acc: 0.8168 - recall: 0.0047 - precision: 0.0473 - f1: 0.0084 - val_loss: 0.4554 - val_acc: 0.8204 - val_recall: 0.0083 - val_precision: 0.0926 - val_f1: 0.0152\n",
      "Epoch 3/5\n",
      "71733/71733 [==============================] - 23s 318us/step - loss: 0.4560 - acc: 0.8189 - recall: 0.0404 - precision: 0.3121 - f1: 0.0694 - val_loss: 0.4545 - val_acc: 0.8204 - val_recall: 0.0229 - val_precision: 0.2047 - val_f1: 0.0402\n",
      "Epoch 4/5\n",
      "71733/71733 [==============================] - 23s 318us/step - loss: 0.4456 - acc: 0.8232 - recall: 0.0907 - precision: 0.4940 - f1: 0.1467 - val_loss: 0.4610 - val_acc: 0.8189 - val_recall: 0.0477 - val_precision: 0.3298 - val_f1: 0.0804\n",
      "Epoch 5/5\n",
      "71733/71733 [==============================] - 23s 318us/step - loss: 0.4299 - acc: 0.8302 - recall: 0.1538 - precision: 0.6167 - f1: 0.2333 - val_loss: 0.4631 - val_acc: 0.8141 - val_recall: 0.0911 - val_precision: 0.3768 - val_f1: 0.1394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5f4d5cbd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train_tfidf[1].shape)\n",
    "print(X_train_tfidf[0].shape, X_train_tfidf[1].shape)\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit([X_train_tfidf[0], X_train_tfidf[1]], y_train, validation_data=[[X_val_tfidf[0], X_val_tfidf[1]], y_val], epochs=5, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_train_tfidf[0], X_train_tfidf[1]], y_train, validation_data=[[X_val_tfidf[0], X_val_tfidf[1]], y_val], epochs=10, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "(71733, 11294)\n",
      "((71733, 11294), (71733, 11294))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 11294)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 11294)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          5848704     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          5848704     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 64)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           2080        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            33          dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 11,715,969\n",
      "Trainable params: 11,715,969\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "GPU sync failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-89bf3cb6b0f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m ) \n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2478\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2482\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 193\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: GPU sync failed"
     ]
    }
   ],
   "source": [
    "            \n",
    "print(y_train.shape)\n",
    "print(X_train_tfidf[1].shape)\n",
    "print(X_train_tfidf[0].shape, X_train_tfidf[1].shape)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_tfidf(X_train_tfidf, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_tfidf[0], X_val_tfidf[1]], y_val]\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1120/1120 [==============================] - 22s 20ms/step - loss: 0.4407 - acc: 0.8093 - recall: 0.8240 - precision: 0.8040 - f1: 0.8116 - val_loss: 0.6436 - val_acc: 0.6932 - val_recall: 0.5363 - val_precision: 0.3014 - val_f1: 0.3781\n",
      "Epoch 2/10\n",
      "1120/1120 [==============================] - 22s 20ms/step - loss: 0.4161 - acc: 0.8238 - recall: 0.8426 - precision: 0.8155 - f1: 0.8269 - val_loss: 0.6470 - val_acc: 0.6917 - val_recall: 0.5442 - val_precision: 0.3021 - val_f1: 0.3806\n",
      "Epoch 3/10\n",
      "1120/1120 [==============================] - 22s 20ms/step - loss: 0.3943 - acc: 0.8363 - recall: 0.8573 - precision: 0.8261 - f1: 0.8395 - val_loss: 0.6596 - val_acc: 0.6910 - val_recall: 0.5423 - val_precision: 0.3001 - val_f1: 0.3790\n",
      "Epoch 4/10\n",
      "1120/1120 [==============================] - 22s 20ms/step - loss: 0.3716 - acc: 0.8468 - recall: 0.8663 - precision: 0.8368 - f1: 0.8495 - val_loss: 0.6147 - val_acc: 0.7089 - val_recall: 0.5126 - val_precision: 0.3115 - val_f1: 0.3795\n",
      "Epoch 5/10\n",
      "1120/1120 [==============================] - 22s 20ms/step - loss: 0.3545 - acc: 0.8551 - recall: 0.8794 - precision: 0.8417 - f1: 0.8585 - val_loss: 0.6969 - val_acc: 0.6844 - val_recall: 0.5326 - val_precision: 0.2930 - val_f1: 0.3705\n",
      "Epoch 6/10\n",
      "1120/1120 [==============================] - 22s 20ms/step - loss: 0.3374 - acc: 0.8641 - recall: 0.8866 - precision: 0.8514 - f1: 0.8671 - val_loss: 0.6749 - val_acc: 0.7107 - val_recall: 0.4856 - val_precision: 0.3067 - val_f1: 0.3681\n",
      "Epoch 7/10\n",
      "1120/1120 [==============================] - 22s 19ms/step - loss: 0.3256 - acc: 0.8692 - recall: 0.8924 - precision: 0.8557 - f1: 0.8722 - val_loss: 0.7060 - val_acc: 0.7129 - val_recall: 0.4970 - val_precision: 0.3123 - val_f1: 0.3752\n",
      "Epoch 8/10\n",
      "1120/1120 [==============================] - 22s 20ms/step - loss: 0.3185 - acc: 0.8730 - recall: 0.8949 - precision: 0.8602 - f1: 0.8757 - val_loss: 0.7049 - val_acc: 0.7141 - val_recall: 0.4756 - val_precision: 0.3085 - val_f1: 0.3658\n",
      "Epoch 9/10\n",
      "1120/1120 [==============================] - 22s 20ms/step - loss: 0.3068 - acc: 0.8784 - recall: 0.9011 - precision: 0.8648 - f1: 0.8812 - val_loss: 0.6919 - val_acc: 0.6967 - val_recall: 0.5066 - val_precision: 0.2981 - val_f1: 0.3674\n",
      "Epoch 10/10\n",
      "1120/1120 [==============================] - 22s 20ms/step - loss: 0.2969 - acc: 0.8825 - recall: 0.9042 - precision: 0.8692 - f1: 0.8849 - val_loss: 0.7233 - val_acc: 0.7040 - val_recall: 0.4898 - val_precision: 0.3018 - val_f1: 0.3651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f980877fa50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_batches_tfidf(X_train_tfidf, y_train), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=10,\n",
    "    validation_data=[[X_val_tfidf[0], X_val_tfidf[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accuracy    recall  precision  specificity        f1     tp      fn     fp       tn\n",
      "val  0.814858  0.074141   0.417515     0.977314  0.125921  410.0  5120.0  572.0  24642.0\n",
      "       accuracy    recall  precision  specificity        f1      tp       fn     fp       tn\n",
      "train  0.839683  0.159863   0.824383     0.992352  0.267796  2103.0  11052.0  448.0  58130.0\n",
      "     accuracy    recall  precision  specificity        f1     tp      fn     fp       tn\n",
      "val  0.815249  0.074141    0.42268      0.97779  0.126154  410.0  5120.0  560.0  24654.0\n",
      "       accuracy   recall  precision  specificity        f1     tp       fn      fp       tn\n",
      "train  0.810868  0.07404   0.412712     0.976339  0.125556  974.0  12181.0  1386.0  57192.0\n"
     ]
    }
   ],
   "source": [
    "# # Test score...\n",
    "# y_val_pred = model.predict([X_val_tfidf[0], X_val_tfidf[1]])\n",
    "# # val_class = np.int(val_predictions>=0.5)\n",
    "# print(val_predictions.shape)\n",
    "\n",
    "predictor = ClassificationPredictor(model)\n",
    "print(predictor.evaluate([X_val_tfidf[0], X_val_tfidf[1]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_tfidf[0], X_train_tfidf[1]], y_train, title='train'))\n",
    "\n",
    "print(predictor.evaluate([X_val_tfidf[1], X_val_tfidf[0]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_tfidf[1], X_train_tfidf[0]], y_train, title='train'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------row 0------------------------\n",
      "sent1: 我的花呗还款到底是怎么回事 \n",
      "sent2: 花呗还款***是什么意思 \n",
      "y = 0, y_pred = 1, a_pred= [0.6430028]\n",
      "tfidf1: 到底=0.78888405, 怎么回事=0.54158861, 花呗=0.12220759, 还款=0.26345593\n",
      "tfidf2: 什么=0.50664006, 意思=0.75519500, 花呗=0.17502105, 还款=0.37731152\n",
      "------------------------row 2200------------------------\n",
      "sent1: 我不差花呗钱怎么会扣钱了 \n",
      "sent2: 花呗都扣钱啦 \n",
      "y = 0, y_pred = 0, a_pred= [0.48342127]\n",
      "tfidf1: 不差=0.84771171, 怎么=0.20751163, 扣钱=0.47783476, 花呗=0.09998865\n",
      "tfidf2: 扣钱=0.97880019, 花呗=0.20481747\n",
      "------------------------row 4400------------------------\n",
      "sent1: 我还清了花呗还扣我的钱 \n",
      "sent2: 我换了花呗为什么有扣我的钱 \n",
      "y = 0, y_pred = 0, a_pred= [0.32526815]\n",
      "tfidf1: 花呗=0.25909687, 还清=0.96585134\n",
      "tfidf2: 为什么=0.35635795, 我换=0.92127440, 花呗=0.15576418\n",
      "------------------------row 6600------------------------\n",
      "sent1: 花呗分期！这个月分期了下个月还能分期么 \n",
      "sent2: 分期后的花呗每个月 \n",
      "y = 0, y_pred = 0, a_pred= [0.14375648]\n",
      "tfidf1: 下个月=0.45875606, 分期=0.80510743, 花呗=0.10204133, 这个=0.36184592\n",
      "tfidf2: 分期=0.48360992, 每个=0.85575051, 花呗=0.18388179\n",
      "------------------------row 8800------------------------\n",
      "sent1: 支付宝花呗不在理赔范围内 \n",
      "sent2: 花呗在保障范围内吗 \n",
      "y = 0, y_pred = 1, a_pred= [0.6766684]\n",
      "tfidf1: 支付宝=0.27902648, 理赔=0.71039558, 花呗=0.09015636, 范围=0.63980792\n",
      "tfidf2: 保障=0.71493494, 花呗=0.09756030, 范围=0.69235108\n",
      "------------------------row 11000------------------------\n",
      "sent1: 蚂蚁借呗关联账户是什么意思 \n",
      "sent2: 蚂蚁借呗是怎么回事儿 \n",
      "y = 0, y_pred = 0, a_pred= [0.16207394]\n",
      "tfidf1: 什么=0.31160905, 借呗=0.17584182, 关联=0.63018044, 意思=0.46448281, 蚂蚁=0.23732527, 账户=0.45031600\n",
      "tfidf2: 借呗=0.19791029, 回事儿=0.90898867, 怎么=0.25144254, 蚂蚁=0.26711002\n",
      "------------------------row 13200------------------------\n",
      "sent1: 借呗不能还款 \n",
      "sent2: 借呗在哪里还款 \n",
      "y = 0, y_pred = 1, a_pred= [0.78098005]\n",
      "tfidf1: 不能=0.71874517, 借呗=0.41989883, 还款=0.55415734\n",
      "tfidf2: 借呗=0.33355683, 哪里=0.83364049, 还款=0.44020833\n",
      "------------------------row 15400------------------------\n",
      "sent1: 我的花呗怎么还款 \n",
      "sent2: 我就是，我花呗我都进不去，我怎么换 \n",
      "y = 0, y_pred = 0, a_pred= [0.18867384]\n",
      "tfidf1: 怎么=0.65777917, 花呗=0.31694826, 还款=0.68327913\n",
      "tfidf2: 就是=0.58304792, 怎么=0.25593439, 花呗=0.12332096, 进不去=0.76114693\n",
      "------------------------row 17600------------------------\n",
      "sent1: 已经实名认证为什么开通不了借呗 \n",
      "sent2: 为什么借呗会没有额度了 \n",
      "y = 0, y_pred = 0, a_pred= [0.10799263]\n",
      "tfidf1: 不了=0.31655189, 为什么=0.23247160, 借呗=0.16598637, 实名=0.58030020, 已经=0.36349365, 开通=0.26451836, 认证=0.52853993\n",
      "tfidf2: 为什么=0.48931217, 借呗=0.34937235, 没有=0.60967796, 额度=0.51653204\n",
      "------------------------row 19800------------------------\n",
      "sent1: 蚂蚁借呗的额度多少 \n",
      "sent2: 蚂蚁借呗上期借款是多少 \n",
      "y = 0, y_pred = 0, a_pred= [0.28960776]\n",
      "tfidf1: 借呗=0.33463221, 多少=0.66278067, 蚂蚁=0.45163704, 额度=0.49473938\n",
      "tfidf2: 上期=0.80789982, 借呗=0.16135681, 借款=0.41436661, 多少=0.31958721, 蚂蚁=0.21777555\n",
      "------------------------row 22000------------------------\n",
      "sent1: 借呗借钱不能分十二期吗 \n",
      "sent2: 借呗不分期还款 \n",
      "y = 0, y_pred = 0, a_pred= [0.16280569]\n",
      "tfidf1: 不能=0.34476829, 借呗=0.20141742, 借钱=0.57616132, 十二=0.71316477\n",
      "tfidf2: 借呗=0.43298871, 分期=0.69712666, 还款=0.57143258\n",
      "------------------------row 24200------------------------\n",
      "sent1: 我的花呗逾期了 \n",
      "sent2: 我的蚂蚁花呗信用情况如何，有逾期的情况吗 \n",
      "y = 0, y_pred = 0, a_pred= [0.35676426]\n",
      "tfidf1: 花呗=0.29267612, 逾期=0.95621163\n",
      "tfidf2: 信用=0.31493178, 如何=0.28677361, 情况=0.84304369, 花呗=0.08076519, 蚂蚁=0.17806041, 逾期=0.26387056\n",
      "------------------------row 26400------------------------\n",
      "sent1: 店家店里显示支持花呗可是支付的时候显示店家不支持 \n",
      "sent2: 什么样的店家支持花呗 \n",
      "y = 0, y_pred = 0, a_pred= [0.27976418]\n",
      "tfidf1: 可是=0.25017925, 店家=0.67914385, 店里=0.35032531, 支付=0.16978765, 支持=0.41063434, 时候=0.18161184, 显示=0.34676754, 花呗=0.05260376\n",
      "tfidf2: 什么样=0.65357301, 店家=0.64205987, 支持=0.38821206, 花呗=0.09946277\n",
      "------------------------row 28600------------------------\n",
      "sent1: 如何兑换花呗分期免息 \n",
      "sent2: 如何兑换蚂蚁花呗免息卷 \n",
      "y = 1, y_pred = 1, a_pred= [0.7048001]\n",
      "tfidf1: 免息=0.56348070, 兑换=0.61632931, 分期=0.31935596, 如何=0.43115509, 花呗=0.12142792\n",
      "tfidf2: 免息=0.57222252, 兑换=0.62589102, 如何=0.43784402, 花呗=0.12331175, 蚂蚁=0.27186144\n"
     ]
    }
   ],
   "source": [
    "show_sentence(X_val, y_val,  X_val_tfidf, words, items=np.arange(0, len(X_val), 2200), \n",
    "              y_pred=predictor.predict([X_val_tfidf[0], X_val_tfidf[1]]), a_pred=model.predict([X_val_tfidf[0], X_val_tfidf[1]])  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "     accuracy    recall  precision  specificity        f1      tp      fn      fp       tn\n",
      "val  0.700755  0.532369   0.308014     0.737685  0.390244  2944.0  2586.0  6614.0  18600.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train  0.788326  0.774534   0.454724     0.791423  0.573027  10189.0  2966.0  12218.0  46360.0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "loaded_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "\n",
    "predictor = ClassificationPredictor(model)\n",
    "print(predictor.evaluate([X_val_tfidf[0], X_val_tfidf[1]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_tfidf[0], X_train_tfidf[1]], y_train, title='train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Word Embedding + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(y_index, batch_size):\n",
    "    indices = np.random.permutation(np.arange(len(y_index)))\n",
    "    return list(y_index[indices[0:batch_size]] )\n",
    "\n",
    "#尝试随机的得到label比例是1：1的batch数据， 不知道这样是覅偶可以有做帮助数据倾斜的问题。\n",
    "#看来这招还挺有用，val f1很快从0.25提高到0.36\n",
    "def generate_batches_seq(X, y, batch_size=BATCH_SIZE, positive_ratio=0.5, random_sent=True):  \n",
    "    while True:\n",
    "        y_index_1 = np.array([i for i,label in enumerate(y) if label==1])\n",
    "        y_index_0 = np.array([i for i,label in enumerate(y) if label==0])\n",
    "\n",
    "        y_index_1_batch_size = int(batch_size*positive_ratio)\n",
    "        y_index_0_batch_size = batch_size - y_index_1_batch_size\n",
    "    \n",
    "        \n",
    "        for start in range(0,len(y)-1,batch_size):\n",
    "            index = np.random.permutation(np.arange(2)) if random_sent else np.arange(2)\n",
    "            batch_indices = get_indices(y_index_1, y_index_1_batch_size) + get_indices(y_index_0, y_index_0_batch_size)\n",
    "            yield [X[index[0]][batch_indices], X[index[1]][batch_indices]], y[batch_indices]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent1.add(L.SimpleRNN(128,return_sequences=False))\n",
    "#     model_sent1.add(L.Dense(64))\n",
    "#     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent2.add(L.SimpleRNN(128,return_sequences=False))\n",
    "#     model_sent2.add(L.Dense(64))\n",
    "#     model_sent2.add(L.Activation('elu')) \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    X = L.concatenate([encoder1, encoder2])\n",
    "    X = L.Dense(64)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dense(32)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "\n",
    "    predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model_gru():\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent1.add(L.CuDNNGRU(128,return_sequences=False))\n",
    "#     model_sent1.add(L.Dense(64))\n",
    "#     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent2.add(L.CuDNNGRU(128,return_sequences=False))\n",
    "#     model_sent2.add(L.Dense(64))\n",
    "#     model_sent2.add(L.Activation('elu')) \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    X = L.concatenate([encoder1, encoder2])\n",
    "    X = L.Dense(64)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dense(32)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "\n",
    "    predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_model_lstm():\n",
    "    K.clear_session()\n",
    "\n",
    "    model_sent1 = Sequential()\n",
    "    model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent1.add(L.Bidirectional(L.CuDNNLSTM(128,return_sequences=False)))\n",
    "#     model_sent1.add(L.Dense(64))\n",
    "#     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "    model_sent2 = Sequential()\n",
    "    model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "    model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "    model_sent2.add(L.Bidirectional(L.CuDNNLSTM(128,return_sequences=False)))\n",
    "#     model_sent2.add(L.Dense(64))\n",
    "#     model_sent2.add(L.Activation('elu')) \n",
    "\n",
    "    input1 = L.Input(shape=(None, ))\n",
    "    input2 = L.Input(shape=(None, ))\n",
    "\n",
    "    encoder1 = model_sent1(input1)\n",
    "    encoder2 = model_sent2(input2)\n",
    "\n",
    "    X = L.concatenate([encoder1, encoder2])\n",
    "    X = L.Dense(128)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "    X = L.Dense(32)(X)\n",
    "    X = L.Activation('elu')(X)\n",
    "\n",
    "    predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "11369\n",
      "(71733, 12)\n",
      "((71733, 12), (71733, 12))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          1488128     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          1488128     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,994,817\n",
      "Trainable params: 2,994,817\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(64, 128), b.shape=(128, 128), m=64, n=128, k=128\n\t [[Node: sequential_1/simple_rnn_1/while/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](sequential_1/simple_rnn_1/while/Switch_2:1, sequential_1/simple_rnn_1/while/MatMul_1/Enter)]]\n\t [[Node: metrics/recall/Mean/_129 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1433_metrics/recall/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'sequential_1/simple_rnn_1/while/MatMul_1', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-46-1a5fcc20a3f8>\", line 7, in <module>\n    model = get_model()\n  File \"<ipython-input-44-9a60c5f85397>\", line 21, in get_model\n    encoder1 = model_sent1(input1)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 579, in call\n    return self.model.call(inputs, mask)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 2085, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 2235, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py\", line 1030, in call\n    initial_state=initial_state)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py\", line 609, in call\n    input_length=timesteps)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2771, in rnn\n    swap_memory=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2934, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2720, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2662, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2757, in _step\n    tuple(constants))\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py\", line 600, in step\n    return self.cell.call(inputs, states, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py\", line 873, in call\n    output = h + K.dot(prev_output, self.recurrent_kernel)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 1076, in dot\n    out = tf.matmul(x, y)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 2022, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2516, in _mat_mul\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(64, 128), b.shape=(128, 128), m=64, n=128, k=128\n\t [[Node: sequential_1/simple_rnn_1/while/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](sequential_1/simple_rnn_1/while/Switch_2:1, sequential_1/simple_rnn_1/while/MatMul_1/Enter)]]\n\t [[Node: metrics/recall/Mean/_129 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1433_metrics/recall/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-1a5fcc20a3f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m ) \n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(64, 128), b.shape=(128, 128), m=64, n=128, k=128\n\t [[Node: sequential_1/simple_rnn_1/while/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](sequential_1/simple_rnn_1/while/Switch_2:1, sequential_1/simple_rnn_1/while/MatMul_1/Enter)]]\n\t [[Node: metrics/recall/Mean/_129 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1433_metrics/recall/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'sequential_1/simple_rnn_1/while/MatMul_1', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-46-1a5fcc20a3f8>\", line 7, in <module>\n    model = get_model()\n  File \"<ipython-input-44-9a60c5f85397>\", line 21, in get_model\n    encoder1 = model_sent1(input1)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 579, in call\n    return self.model.call(inputs, mask)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 2085, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py\", line 2235, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py\", line 1030, in call\n    initial_state=initial_state)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py\", line 609, in call\n    input_length=timesteps)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2771, in rnn\n    swap_memory=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2934, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2720, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2662, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 2757, in _step\n    tuple(constants))\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py\", line 600, in step\n    return self.cell.call(inputs, states, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.py\", line 873, in call\n    output = h + K.dot(prev_output, self.recurrent_kernel)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 1076, in dot\n    out = tf.matmul(x, y)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 2022, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 2516, in _mat_mul\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(64, 128), b.shape=(128, 128), m=64, n=128, k=128\n\t [[Node: sequential_1/simple_rnn_1/while/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](sequential_1/simple_rnn_1/while/Switch_2:1, sequential_1/simple_rnn_1/while/MatMul_1/Enter)]]\n\t [[Node: metrics/recall/Mean/_129 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1433_metrics/recall/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "print(y_train.shape)\n",
    "print(len(vocab))\n",
    "print(X_train_seq[1].shape)\n",
    "print(X_train_seq[0].shape, X_train_seq[1].shape)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.4780 - acc: 0.7754 - recall: 0.8049 - precision: 0.7632 - f1: 0.7812 - val_loss: 0.6511 - val_acc: 0.6626 - val_recall: 0.6209 - val_precision: 0.2939 - val_f1: 0.3919\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.4568 - acc: 0.7905 - recall: 0.8203 - precision: 0.7775 - f1: 0.7962 - val_loss: 0.5936 - val_acc: 0.7032 - val_recall: 0.5502 - val_precision: 0.3151 - val_f1: 0.3926\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.4328 - acc: 0.8037 - recall: 0.8356 - precision: 0.7888 - f1: 0.8096 - val_loss: 0.6338 - val_acc: 0.6836 - val_recall: 0.5967 - val_precision: 0.3057 - val_f1: 0.3968\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.4047 - acc: 0.8202 - recall: 0.8521 - precision: 0.8045 - f1: 0.8256 - val_loss: 0.6200 - val_acc: 0.7122 - val_recall: 0.5394 - val_precision: 0.3206 - val_f1: 0.3935\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 13s 11ms/step - loss: 0.3858 - acc: 0.8316 - recall: 0.8651 - precision: 0.8144 - f1: 0.8371 - val_loss: 0.6286 - val_acc: 0.7086 - val_recall: 0.5355 - val_precision: 0.3159 - val_f1: 0.3896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f953a2df7d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "11372\n",
      "(71733, 12)\n",
      "((71733, 12), (71733, 12))\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 128)          1554688     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 128)          1554688     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           16448       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           2080        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            33          activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,127,937\n",
      "Trainable params: 3,127,937\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "1120/1120 [==============================] - 12s 11ms/step - loss: 0.6632 - acc: 0.6031 - recall: 0.5579 - precision: 0.6183 - f1: 0.5769 - val_loss: 0.6590 - val_acc: 0.6594 - val_recall: 0.5417 - val_precision: 0.2734 - val_f1: 0.3559\n",
      "Epoch 2/5\n",
      "1120/1120 [==============================] - 11s 10ms/step - loss: 0.6433 - acc: 0.6313 - recall: 0.5717 - precision: 0.6532 - f1: 0.6054 - val_loss: 0.6074 - val_acc: 0.6949 - val_recall: 0.5062 - val_precision: 0.2969 - val_f1: 0.3660\n",
      "Epoch 3/5\n",
      "1120/1120 [==============================] - 11s 10ms/step - loss: 0.6270 - acc: 0.6519 - recall: 0.6271 - precision: 0.6627 - f1: 0.6412 - val_loss: 0.6323 - val_acc: 0.6568 - val_recall: 0.5837 - val_precision: 0.2808 - val_f1: 0.3721\n",
      "Epoch 4/5\n",
      "1120/1120 [==============================] - 11s 10ms/step - loss: 0.6078 - acc: 0.6699 - recall: 0.6626 - precision: 0.6754 - f1: 0.6657 - val_loss: 0.6161 - val_acc: 0.6658 - val_recall: 0.5765 - val_precision: 0.2858 - val_f1: 0.3749\n",
      "Epoch 5/5\n",
      "1120/1120 [==============================] - 11s 10ms/step - loss: 0.5914 - acc: 0.6879 - recall: 0.6934 - precision: 0.6892 - f1: 0.6882 - val_loss: 0.6112 - val_acc: 0.6660 - val_recall: 0.5958 - val_precision: 0.2905 - val_f1: 0.3831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f953005f750>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            \n",
    "print(y_train.shape)\n",
    "print(len(vocab))\n",
    "print(X_train_seq[1].shape)\n",
    "print(X_train_seq[0].shape, X_train_seq[1].shape)\n",
    "\n",
    "model = get_model_gru()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_lstm():\n",
    "#     K.clear_session()\n",
    "\n",
    "#     model_sent1 = Sequential()\n",
    "#     model_sent1.add(L.InputLayer([None], dtype='int32'))\n",
    "#     model_sent1.add(L.Embedding(len(vocab), 128))\n",
    "#     model_sent1.add(L.CuDNNLSTM(128,return_sequences=False))\n",
    "# #     model_sent1.add(L.Bidirectional(L.CuDNNLSTM(128,return_sequences=False)))\n",
    "# #     model_sent1.add(L.Dense(64))\n",
    "# #     model_sent1.add(L.Activation('elu')) \n",
    "\n",
    "#     model_sent2 = Sequential()\n",
    "#     model_sent2.add(L.InputLayer([None], dtype='int32'))\n",
    "#     model_sent2.add(L.Embedding(len(vocab), 128))\n",
    "#     model_sent2.add(L.CuDNNLSTM(128,return_sequences=False))\n",
    "# #     model_sent2.add(L.Bidirectional(L.CuDNNLSTM(128,return_sequences=False)))\n",
    "# #     model_sent2.add(L.Dense(64))\n",
    "# #     model_sent2.add(L.Activation('elu')) \n",
    "\n",
    "#     input1 = L.Input(shape=(None, ))\n",
    "#     input2 = L.Input(shape=(None, ))\n",
    "\n",
    "#     encoder1 = model_sent1(input1)\n",
    "#     encoder2 = model_sent2(input2)\n",
    "\n",
    "#     X = L.concatenate([encoder1, encoder2])\n",
    "    \n",
    "    \n",
    "#     X = L.Conv2D(filters=16, kernel_size=5, padding=\"same\")(X)\n",
    "#     X = L.Flatten()(X)\n",
    "#     X = L.Dense(128)(X)\n",
    "#     X = L.Activation('elu')(X)\n",
    "#     X = L.Dense(32)(X)\n",
    "#     X = L.Activation('elu')(X)\n",
    "\n",
    "#     predictions = L.Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "#     model = Model(inputs=[input1, input2], outputs=predictions)\n",
    "#     model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1])\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71733,)\n",
      "11371\n",
      "(71733, 12)\n",
      "((71733, 12), (71733, 12))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-400-4e145a77b106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-399-461f5f655762>\u001b[0m in \u001b[0;36mget_model_lstm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=2"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(len(vocab))\n",
    "print(X_train_seq[1].shape)\n",
    "print(X_train_seq[0].shape, X_train_seq[1].shape)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = get_model_lstm()\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    generate_batches_seq(X_train_seq, y_train, positive_ratio=0.5, random_sent=True), \n",
    "    steps_per_epoch=len(y_train) // BATCH_SIZE, \n",
    "    epochs=5,\n",
    "    validation_data=[[X_val_seq[0], X_val_seq[1]], y_val]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accuracy    recall  precision  specificity        f1      tp      fn      fp       tn\n",
      "val  0.708561  0.535624   0.316656      0.74649  0.398011  2962.0  2568.0  6392.0  18822.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train  0.820334  0.863246   0.505948     0.810697  0.637978  11356.0  1799.0  11089.0  47489.0\n",
      "     accuracy    recall  precision  specificity        f1      tp      fn      fp       tn\n",
      "val  0.704528  0.530922   0.311479     0.742603  0.392618  2936.0  2594.0  6490.0  18724.0\n",
      "       accuracy    recall  precision  specificity        f1       tp      fn       fp       tn\n",
      "train  0.821784  0.861193   0.508323     0.812933  0.639298  11329.0  1826.0  10958.0  47620.0\n"
     ]
    }
   ],
   "source": [
    "predictor = ClassificationPredictor(model)\n",
    "print(predictor.evaluate([X_val_seq[0], X_val_seq[1]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_seq[0], X_train_seq[1]], y_train, title='train'))\n",
    "\n",
    "print(predictor.evaluate([X_val_seq[1], X_val_seq[0]], y_val, title='val'))\n",
    "print(predictor.evaluate([X_train_seq[1], X_train_seq[0]], y_train, title='train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思路\n",
    "- 分词效果分析\n",
    "- 错误分类分析\n",
    "    - 同音词分析\n",
    "- Data Augmentation   \n",
    "    - 去除一个或若干个词。\n",
    "- 外部训练好的word embedding向量。 \n",
    "- 把所有汉字变成拼音，这样可以有晓解决谐音字的问题。  07-07\n",
    "\n",
    "## 总结\n",
    "\n",
    "- BOW\n",
    "    - fit： 经过15 epochs，f1在0.25左右  \n",
    "    val_loss: 0.5505 - val_acc: 0.7726 - val_recall: 0.2186 - val_precision: 0.3074 - val_f1: 0.2459\n",
    "    - fit_generator + generate_batches_tfidf：\n",
    "        - 由于数据倾斜比较厉害，所以在获得batch的时候，正例:反例=1:1，这种情况下性能快速提高。 f1提高到0.37。  下面是5 epochs 的结果。如果再增加epochs，则过拟合更加严重。     \n",
    "    \n",
    "            val_loss: 0.6349 - val_acc: 0.6664 - val_recall: 0.5774 - val_precision: 0.2874 - val_f1: 0.3761      \n",
    "      \n",
    "        - 另外一个体会是，以前会用某一个阈值（一般0.5，如果数据）来计算y_pred, 但现在发现可能用generate_batches_tfidf里，设定positive的比例，更加简单，效果也更好。尝试了positive_ratio=[0.6,0.5.0.4,0.3,0.2], 发现当positive_ratio=[0.6,0.5,0.4]的时候,性能接近。当positive_ratio=0.3，f1马上下降到0.3，当positive_ratio=0.2，性能非常差。这也进一步证明数据倾斜的问题。    \n",
    "    \n",
    "        - 随机化两个句子的顺序。\n",
    "        \n",
    "            当调用 generate_batches_tfidf(X_train_tfidf, y_train, positive_ratio=0.5, random_sent=False) 产生traiing数据， 结果如下：  \n",
    "\n",
    "![ali_nlp_random_sent_false.ipynb](../../../image/ali_nlp_random_sent_false.png)\n",
    "\n",
    "            而调用 generate_batches_tfidf(X_train_tfidf, y_train, positive_ratio=0.5, random_sent=True) 产生traiing数据， 结果如下\n",
    "![ali_nlp_random_sent_true.ipynb](../../../image/ali_nlp_random_sent_true.png)\n",
    "\n",
    "            比较这两个结果，可以很明显发现，random_sent=True能够提高val f1_大概0.02左右，而且能够非常明显降低过拟合。\n",
    "            \n",
    "- Word Embedding    \n",
    "    - simple RNN\n",
    "    - lstm \n",
    "    - gru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Word Embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
