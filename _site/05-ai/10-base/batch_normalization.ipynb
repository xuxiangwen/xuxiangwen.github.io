{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一直就想对batch_normalization的细节（尤其是反向传播的推导）了解清楚，正好这次有时间。看了一下的及pain文章，发现反向推导的公式基于论文中的严格的链式推导，的确是非常不错的。只是心里还是想要用自己的方法来推导一遍。  \n",
    "\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167)  \n",
    "[深度学习中 Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762)  \n",
    "[Batch Normalization梯度反向传播推导](https://blog.csdn.net/yuechuen/article/details/71502503)  \n",
    "[What does the gradient flowing through batch normalization looks like ?](http://cthorey.github.io/backpropagation/)  \n",
    "[Understanding the backward pass through Batch Normalization Layer](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)\n",
    "\n",
    "\n",
    "![batch_normalization_fp](../../image/batch_normalization_fp.jpg) | ![batch_normalization_bp](../../image/batch_normalization_bp.jpg) \n",
    "::|::\n",
    "Forward propagation | Backward propagation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "    X, X_norm, mu, var, gamma, beta = cache\n",
    "\n",
    "    N, D = X.shape\n",
    "\n",
    "    X_mu = X - mu\n",
    "    std_inv = 1. / np.sqrt(var + 1e-8)\n",
    "\n",
    "    dX_norm = dout * gamma\n",
    "    dvar = np.sum(dX_norm * X_mu, axis=0) * -.5 * std_inv**3\n",
    "    dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
    "\n",
    "    dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmu / N)\n",
    "    dgamma = np.sum(dout * X_norm, axis=0)\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "\n",
    "    return dX, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这个代码好像有问题，计算dx的时候缺少gamma啊。下面是我写的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(Layer):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.epsilon = epsilon       \n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.t = 0\n",
    "        \n",
    "        self.initialized = False\n",
    "\n",
    "    def initial_params(self, var, mu):\n",
    "        self.gamma = var\n",
    "        self.beta = mu            \n",
    "        self.initialized = True\n",
    "        \n",
    "    def compute(self, input):\n",
    "        mu = np.mean(input, axis = 0)\n",
    "        var = np.mean(np.power(input - mu, 2), axis = 0)     \n",
    "        x = (input - mu)/np.sqrt(var + self.epsilon)\n",
    "        return mu, var, x\n",
    "\n",
    "    def forward(self,input):\n",
    "        mu, var, x = self.compute(input)\n",
    "        \n",
    "        if not self.initialized:\n",
    "            self.initial_params(var, mu)          \n",
    "            return input\n",
    "        else:         \n",
    "            return self.gamma * x + beta\n",
    "            \n",
    "    def backward(self,input,grad_output):    \n",
    "        mu, var, x = self.compute(input)\n",
    "                \n",
    "        if not self.initialized:\n",
    "            print(\"self.initialized\")\n",
    "            self.initial_params(var, mu)     \n",
    "        \n",
    "        grad_gamma = np.sum(grad_output*x, axis=0)\n",
    "        grad_beta = np.sum(grad_output, axis=0)\n",
    "        \n",
    "        grad_input = self.gamma/np.sqrt(var + self.epsilon)*(grad_output - np.mean(grad_output, axis=0)) - \\\n",
    "            self.gamma *(input - mu) * np.mean(grad_output * (input - mu), axis = 0)/np.power(var + self.epsilon, 1.5)\n",
    "        \n",
    "        self.gamma = self.gamma - grad_gamma\n",
    "        self.beta = self.beta - grad_beta  \n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么时候使用batch normalization\n",
    "\n",
    "在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
